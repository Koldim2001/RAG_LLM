{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG примеры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 17:51:03,884 - nodes.VectorDBNode - INFO - Connected to Milvus at localhost:19530 successfully!\n",
      "2025-02-19 17:51:03,884 - nodes.DataParsingNode - INFO - Парсинг https://www.eurochem.ru/...\n",
      "2025-02-19 17:51:04,106 - nodes.DataParsingNode - INFO - Парсинг https://www.eurochem.ru/global-operations/...\n",
      "2025-02-19 17:51:04,271 - nodes.DataParsingNode - INFO - Парсинг https://www.eurochem.ru/about-us/komplaens/...\n",
      "2025-02-19 17:51:04,410 - nodes.DataParsingNode - INFO - Парсинг https://www.eurochem.ru/proteh-lab/...\n",
      "2025-02-19 17:51:04,581 - nodes.DataParsingNode - INFO - Парсинг https://digtp.com/...\n",
      "2025-02-19 17:51:04,689 - nodes.DataParsingNode - INFO - Парсинг https://digtp.com/projects/machine-learning-platforma...\n",
      "2025-02-19 17:51:04,784 - nodes.DataParsingNode - INFO - Парсинг https://digtp.com/projects/rekomendatelnye-modeli...\n",
      "2025-02-19 17:51:04,915 - nodes.DataParsingNode - INFO - Парсинг https://digtp.com/projects/mobilnoe-prilozenie-mineralogiia...\n",
      "2025-02-19 17:51:05,012 - nodes.DataParsingNode - INFO - Парсинг https://digtp.com/contacts...\n",
      "2025-02-19 17:51:05,117 - nodes.DataParsingNode - INFO - Парсинг https://www.eurochem-career.com/news/iskusstvennyi-intellekt-v-ximii-gpt-assistenty-v-evroxime...\n",
      "2025-02-19 17:51:05,355 - nodes.DataParsingNode - INFO - Парсинг https://otus.ru/instructors/10517...\n",
      "2025-02-19 17:51:09,265 - nodes.DataParsingNode - INFO - Парсинг https://ru.wikipedia.org/wiki/ЕвроХим...\n",
      "2025-02-19 17:51:09,537 - nodes.DataParsingNode - INFO - Парсинг https://www.eurochem.ru/usolskij-kalijnyj-kombinat/...\n",
      "2025-02-19 17:51:09,683 - nodes.DataParsingNode - INFO - Парсинг https://uralmines.ru/evrohim-usolskij-kalijnyj-kombinat/...\n",
      "2025-02-19 17:51:10,523 - nodes.DataParsingNode - INFO - Парсинг https://docs.ultralytics.com/tasks/segment...\n",
      "2025-02-19 17:51:11,292 - nodes.DataParsingNode - INFO - Парсинг https://docs.ultralytics.com/tasks/detect...\n",
      "2025-02-19 17:51:11,724 - nodes.DataParsingNode - INFO - Парсинг https://docs.ultralytics.com/tasks...\n",
      "2025-02-19 17:51:12,023 - nodes.DataParsingNode - INFO - Парсинг https://docs.ultralytics.com/modes/...\n",
      "2025-02-19 17:51:12,383 - nodes.DataParsingNode - INFO - Парсинг https://docs.ultralytics.com/solutions...\n",
      "2025-02-19 17:51:12,952 - nodes.DataParsingNode - INFO - Парсинг https://github.com/Koldim2001...\n",
      "2025-02-19 17:51:13,619 - nodes.DataParsingNode - INFO - Парсинг https://github.com/Koldim2001/YOLO-Patch-Based-Inference...\n",
      "2025-02-19 17:51:14,421 - nodes.DataParsingNode - INFO - Парсинг https://github.com/Koldim2001/TrafficAnalyzer...\n",
      "2025-02-19 17:51:15,300 - nodes.DataParsingNode - INFO - Парсинг https://github.com/Koldim2001/COCO_to_YOLOv8...\n",
      "2025-02-19 17:51:16,050 - nodes.EmbedderNode - INFO - Обработка батча 1, размер батча: 156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Чанки сохранены в файл results/chunks_output.txt\n",
      "Итоговый текст сохранен в файл results/output_parsing.txt\n",
      "DataElement(\n",
      "  Сollection db name: data\n",
      "  URLs: https://www.eurochem.ru/, https://www.eurochem.ru/global-operations/, https://www.eurochem.ru/about-us/komplaens/, https://www.eurochem.ru/proteh-lab/, https://digtp.com/, https://digtp.com/projects/machine-learning-platforma, https://digtp.com/projects/rekomendatelnye-modeli, https://digtp.com/projects/mobilnoe-prilozenie-mineralogiia, https://digtp.com/contacts, https://www.eurochem-career.com/news/iskusstvennyi-intellekt-v-ximii-gpt-assistenty-v-evroxime, https://otus.ru/instructors/10517, https://ru.wikipedia.org/wiki/ЕвроХим, https://www.eurochem.ru/usolskij-kalijnyj-kombinat/, https://uralmines.ru/evrohim-usolskij-kalijnyj-kombinat/, https://docs.ultralytics.com/tasks/segment, https://docs.ultralytics.com/tasks/detect, https://docs.ultralytics.com/tasks, https://docs.ultralytics.com/modes/, https://docs.ultralytics.com/solutions, https://github.com/Koldim2001, https://github.com/Koldim2001/YOLO-Patch-Based-Inference, https://github.com/Koldim2001/TrafficAnalyzer, https://github.com/Koldim2001/COCO_to_YOLOv8\n",
      "  URL Data: 23 entries\n",
      "  Chunks: 156 chunks\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 17:51:17,465 - nodes.VectorDBNode - INFO - Collection 'data' deleted successfully.\n",
      "2025-02-19 17:51:17,542 - nodes.VectorDBNode - INFO - Collection 'data' created successfully.\n",
      "2025-02-19 17:51:18,061 - nodes.VectorDBNode - INFO - Index created on field 'embedding' for collection 'data'.\n",
      "2025-02-19 17:51:18,716 - nodes.VectorDBNode - INFO - Collection 'data' loaded successfully.\n",
      "2025-02-19 17:51:18,859 - nodes.VectorDBNode - INFO - Inserted 156 records into collection 'data'.\n"
     ]
    }
   ],
   "source": [
    "from services.MakeDatasetRAG import MakeDatasetRAG\n",
    "\n",
    "mk = MakeDatasetRAG()\n",
    "\n",
    "url_list = [\n",
    "    \"https://www.eurochem.ru/\",\n",
    "    \"https://www.eurochem.ru/global-operations/\",\n",
    "    \"https://www.eurochem.ru/about-us/komplaens/\",\n",
    "    \"https://www.eurochem.ru/proteh-lab/\",\n",
    "    \"https://digtp.com/\",\n",
    "    \"https://digtp.com/projects/machine-learning-platforma\",\n",
    "    \"https://digtp.com/projects/rekomendatelnye-modeli\",\n",
    "    \"https://digtp.com/projects/mobilnoe-prilozenie-mineralogiia\",\n",
    "    \"https://digtp.com/contacts\",\n",
    "    \"https://www.eurochem-career.com/news/iskusstvennyi-intellekt-v-ximii-gpt-assistenty-v-evroxime\",\n",
    "    \"https://otus.ru/instructors/10517\",\n",
    "    \"https://ru.wikipedia.org/wiki/ЕвроХим\",\n",
    "    \"https://www.eurochem.ru/usolskij-kalijnyj-kombinat/\",\n",
    "    \"https://uralmines.ru/evrohim-usolskij-kalijnyj-kombinat/\",\n",
    "    \"https://docs.ultralytics.com/tasks/segment\",\n",
    "    \"https://docs.ultralytics.com/tasks/detect\",\n",
    "    \"https://docs.ultralytics.com/tasks\",\n",
    "    \"https://docs.ultralytics.com/modes/\",\n",
    "    \"https://docs.ultralytics.com/solutions\",\n",
    "    \"https://github.com/Koldim2001\",\n",
    "    \"https://github.com/Koldim2001/YOLO-Patch-Based-Inference\",\n",
    "    \"https://github.com/Koldim2001/TrafficAnalyzer\",\n",
    "    \"https://github.com/Koldim2001/COCO_to_YOLOv8\"\n",
    "]\n",
    "\n",
    "collection_db_name = \"data\"\n",
    "\n",
    "mk.process(url_list, collection_db_name, show_data_info=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 17:51:20,888 - nodes.VectorDBNode - INFO - First 5 records in collection 'data':\n",
      "2025-02-19 17:51:20,890 - nodes.VectorDBNode - INFO - ID: 456119300212324401, Text: [Источник: АО «Минерально-химическая компания Евро..., Length: 1023, Timestamp: 1739976678\n",
      "2025-02-19 17:51:20,890 - nodes.VectorDBNode - INFO - ID: 456119300212324402, Text: [Источник: Наши активы - добыча, производство, про..., Length: 1229, Timestamp: 1739976678\n",
      "2025-02-19 17:51:20,891 - nodes.VectorDBNode - INFO - ID: 456119300212324403, Text: [Источник: Наши активы - добыча, производство, про..., Length: 1226, Timestamp: 1739976678\n",
      "2025-02-19 17:51:20,891 - nodes.VectorDBNode - INFO - ID: 456119300212324404, Text: [Источник: Наши активы - добыча, производство, про..., Length: 678, Timestamp: 1739976678\n",
      "2025-02-19 17:51:20,891 - nodes.VectorDBNode - INFO - ID: 456119300212324405, Text: [Источник: Комплаенс]\n",
      "Свяжитесь с нами\n",
      "Удобрения и..., Length: 1204, Timestamp: 1739976678\n"
     ]
    }
   ],
   "source": [
    "mk.vector_db_node.display_first_n_records(collection_db_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mk.vector_db_node.get_total_records(collection_db_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 17:59:24,082 - nodes.VectorDBNode - INFO - Connected to Milvus at localhost:19530 successfully!\n"
     ]
    }
   ],
   "source": [
    "from services.AskLLM import AskLLM\n",
    "ask = AskLLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "query= \"что такое YOLO\"\n",
    "message_number=0\n",
    "collection_db_name=\"data\"\n",
    "previous_messages=[]\n",
    "show_data_info=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 17:59:24,572 - nodes.EmbedderNode - INFO - Обработка батча 1, размер батча: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результат векторного поиска:\n",
      "Chunk 1: [Источник: Detect - Ultralytics YOLO Docs]\n",
      "yolo detect val model = yolo11n.pt # val official model yolo detect val model = path/to/best.pt # val custom model\n",
      "Use a trained YOLO11n model to run predict..., \n",
      "metadata={'source': 'chunk_0', 'distance': 0.853311, 'chunk_length': 1263}\n",
      "\n",
      "Chunk 2: [Источник: Segment - Ultralytics YOLO Docs]\n",
      "from ultralytics import YOLO # Load a model model = YOLO ( \"yolo11n-seg.yaml\" ) # build a new model from YAML model = YOLO ( \"yolo11n-seg.pt\" ) # load a pre..., \n",
      "metadata={'source': 'chunk_1', 'distance': 0.84686, 'chunk_length': 1259}\n",
      "\n",
      "Chunk 3: [Источник: Detect - Ultralytics YOLO Docs]\n",
      "# Build a new model from YAML and start training from scratch yolo detect train data = coco8.yaml model = yolo11n.yaml epochs = 100 imgsz = 640 # Start train..., \n",
      "metadata={'source': 'chunk_2', 'distance': 0.841616, 'chunk_length': 1234}\n",
      "\n",
      "Chunk 4: [Источник: Segment - Ultralytics YOLO Docs]\n",
      "from ultralytics import YOLO # Load a model model = YOLO ( \"yolo11n-seg.pt\" ) # load an official model model = YOLO ( \"path/to/best.pt\" ) # load a custom mo..., \n",
      "metadata={'source': 'chunk_3', 'distance': 0.841316, 'chunk_length': 823}\n",
      "\n",
      "Chunk 5: [Источник: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      "classes_names: These are the human-readable names corresponding to ..., \n",
      "metadata={'source': 'chunk_4', 'distance': 0.839344, 'chunk_length': 1362}\n",
      "\n",
      "Chunk 6: [Источник: Detect - Ultralytics YOLO Docs]\n",
      "YOLO11 Detect models are the default YOLO11 models, i.e.\n",
      "and are pretrained on\n",
      "YOLO11 pretrained Detect models are shown here. Detect, Segment and Pose model..., \n",
      "metadata={'source': 'chunk_5', 'distance': 0.8357, 'chunk_length': 1128}\n",
      "\n",
      "Chunk 7: [Источник: GitHub - Koldim2001/COCO_to_YOLOv8: Converting COCO annotation (CVAT) to annotation for YOLO-seg (instance segmentation) and YOLO-obb (oriented bounding box detection)]\n",
      "--coco_dataset TEXT ..., \n",
      "metadata={'source': 'chunk_6', 'distance': 0.834664, 'chunk_length': 1154}\n",
      "\n",
      "Chunk 8: [Источник: Segment - Ultralytics YOLO Docs]\n",
      "from ultralytics import YOLO # Load a model model = YOLO ( \"yolo11n-seg.pt\" ) # load an official model model = YOLO ( \"path/to/best.pt\" ) # load a custom mo..., \n",
      "metadata={'source': 'chunk_7', 'distance': 0.83358, 'chunk_length': 1237}\n",
      "\n",
      "Chunk 9: [Источник: GitHub - Koldim2001/COCO_to_YOLOv8: Converting COCO annotation (CVAT) to annotation for YOLO-seg (instance segmentation) and YOLO-obb (oriented bounding box detection)]\n",
      "Ключевое применение ..., \n",
      "metadata={'source': 'chunk_8', 'distance': 0.832139, 'chunk_length': 1415}\n",
      "\n",
      "Chunk 10: [Источник: GitHub - Koldim2001/COCO_to_YOLOv8: Converting COCO annotation (CVAT) to annotation for YOLO-seg (instance segmentation) and YOLO-obb (oriented bounding box detection)]\n",
      "How to run the code:..., \n",
      "metadata={'source': 'chunk_9', 'distance': 0.824105, 'chunk_length': 1407}\n",
      "\n",
      "Chunk 11: [Источник: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      "IoU threshold for non-maximum suppression YOLO of single  crop.\n",
      "Lis..., \n",
      "metadata={'source': 'chunk_10', 'distance': 0.823422, 'chunk_length': 1292}\n",
      "\n",
      "Chunk 12: [Источник: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      "The type of analysis to perform. Can be \"resolution_based\" for Reso..., \n",
      "metadata={'source': 'chunk_11', 'distance': 0.822136, 'chunk_length': 1082}\n",
      "\n",
      "Chunk 13: [Источник: Ultralytics YOLO11 Modes - Ultralytics YOLO Docs]\n",
      "# Export a YOLO model to ONNX format from the command line yolo export model = yolo11n.pt format = onnx\n",
      "Detailed steps for each export form..., \n",
      "metadata={'source': 'chunk_12', 'distance': 0.820331, 'chunk_length': 1168}\n",
      "\n",
      "Chunk 14: [Источник: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      "GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for ..., \n",
      "metadata={'source': 'chunk_13', 'distance': 0.819961, 'chunk_length': 1209}\n",
      "\n",
      "Chunk 15: [Источник: Ultralytics Solutions: Harness YOLO11 to Solve Real-World Problems - Ultralytics YOLO Docs]\n",
      "🚀: Estimate object speed using YOLO11 and object tracking techniques, crucial for applications li..., \n",
      "metadata={'source': 'chunk_14', 'distance': 0.819552, 'chunk_length': 1255}\n",
      "\n",
      "Chunk 16: [Источник: Detect - Ultralytics YOLO Docs]\n",
      "from ultralytics import YOLO # Load a pretrained model model = YOLO ( \"yolo11n.pt\" ) # Train the model on your custom dataset model . train ( data = \"my_cust..., \n",
      "metadata={'source': 'chunk_15', 'distance': 0.818875, 'chunk_length': 1260}\n",
      "\n",
      "Chunk 17: [Источник: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      "Whether to show bounding boxes.\n",
      "Whether to show class labels.\n",
      "Wheth..., \n",
      "metadata={'source': 'chunk_16', 'distance': 0.818564, 'chunk_length': 1197}\n",
      "\n",
      "Chunk 18: [Источник: Detect - Ultralytics YOLO Docs]\n",
      "from ultralytics import YOLO # Load a model model = YOLO ( \"yolo11n.pt\" ) # load an official model model = YOLO ( \"path/to/best.pt\" ) # load a custom trained..., \n",
      "metadata={'source': 'chunk_17', 'distance': 0.818067, 'chunk_length': 1109}\n",
      "\n",
      "Chunk 19: [Источник: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      "from patched_yolo_infer import visualize_results # Assuming result ..., \n",
      "metadata={'source': 'chunk_18', 'distance': 0.817376, 'chunk_length': 1326}\n",
      "\n",
      "Chunk 20: [Источник: GitHub - Koldim2001/COCO_to_YOLOv8: Converting COCO annotation (CVAT) to annotation for YOLO-seg (instance segmentation) and YOLO-obb (oriented bounding box detection)]\n",
      "GitHub - Koldim2001/..., \n",
      "metadata={'source': 'chunk_19', 'distance': 0.815978, 'chunk_length': 1418}\n",
      "\n",
      "Chunk 21: [Источник: Ultralytics YOLO11 Modes - Ultralytics YOLO Docs]\n",
      "mAP (mean Average Precision)\n",
      ": This evaluates the accuracy of object detection.\n",
      "IOU (Intersection over Union)\n",
      ": Measures the overlap betwee..., \n",
      "metadata={'source': 'chunk_20', 'distance': 0.815706, 'chunk_length': 1238}\n",
      "\n",
      "Chunk 22: [Источник: Koldim2001 (Dmitry Kolesnikov) · GitHub]\n",
      "Koldim2001 (Dmitry Kolesnikov) · GitHub\n",
      "Block or report Koldim2001\n",
      "Learn more about\n",
      "Add an optional note:\n",
      "Please don't include any personal informat..., \n",
      "metadata={'source': 'chunk_21', 'distance': 0.814972, 'chunk_length': 1277}\n",
      "\n",
      "Chunk 23: [Источник: ЦТиП: Machine learning-платформа]\n",
      "свидетельство № АО-20211230-3269820362-3 от 10.01.2022, \n",
      "metadata={'source': 'chunk_22', 'distance': 0.814469, 'chunk_length': 99}\n",
      "\n",
      "Chunk 24: [Источник: Ultralytics Solutions: Harness YOLO11 to Solve Real-World Problems - Ultralytics YOLO Docs]\n",
      "yolo solutions count show = True # for object counting yolo solutions source = \"path/to/video/fil..., \n",
      "metadata={'source': 'chunk_23', 'distance': 0.814027, 'chunk_length': 1305}\n",
      "\n",
      "Chunk 25: [Источник: Computer Vision Tasks supported by Ultralytics YOLO11 - Ultralytics YOLO Docs]\n",
      "Train the YOLO11 model using the detection task.\n",
      "Use the model to make predictions by feeding in new images or..., \n",
      "metadata={'source': 'chunk_24', 'distance': 0.813338, 'chunk_length': 1312}\n",
      "\n",
      "Chunk 26: [Источник: Segment - Ultralytics YOLO Docs]\n",
      "The output of an instance segmentation model is a set of masks or contours that outline each object in the image, along with class labels and confidence sco..., \n",
      "metadata={'source': 'chunk_25', 'distance': 0.809926, 'chunk_length': 1110}\n",
      "\n",
      "Chunk 27: [Источник: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      "boxes: These bounding boxes are represented as a list of lists, whe..., \n",
      "metadata={'source': 'chunk_26', 'distance': 0.809622, 'chunk_length': 1283}\n",
      "\n",
      "Chunk 28: [Источник: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      "Note: If CUDA support is available, it's recommended to pre-install..., \n",
      "metadata={'source': 'chunk_27', 'distance': 0.809479, 'chunk_length': 1350}\n",
      "\n",
      "Chunk 29: [Источник: Цифровые Технологии и Платформы]\n",
      "Узнать о наших вакансиях\n",
      "для промышленности и прочих сфер.\n",
      "Политики и документы\n",
      "ООО «Цифровые технологии и платформы»\n",
      "является аккредитованной ИТ-компанией,..., \n",
      "metadata={'source': 'chunk_28', 'distance': 0.808979, 'chunk_length': 255}\n",
      "\n",
      "Chunk 30: [Источник: Ultralytics YOLO11 Modes - Ultralytics YOLO Docs]\n",
      "mode: Fine-tune your model on custom or preloaded datasets.\n",
      "mode: A post-training checkpoint to validate model performance.\n",
      "mode: Unleash t..., \n",
      "metadata={'source': 'chunk_29', 'distance': 0.80709, 'chunk_length': 1183}\n",
      "\n",
      "\n",
      "=======================\n",
      "\n",
      "Результат после реранка:\n",
      "Chunk 1: [Источник: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      "The type of analysis to perform. Can be \"resolution_based\" for Reso..., \n",
      "metadata={'source': 'chunk_11', 'distance': 0.822136, 'chunk_length': 1082, 'score_rerank': 0.6131208}\n",
      "\n",
      "Chunk 2: [Источник: Detect - Ultralytics YOLO Docs]\n",
      "from ultralytics import YOLO # Load a pretrained model model = YOLO ( \"yolo11n.pt\" ) # Train the model on your custom dataset model . train ( data = \"my_cust..., \n",
      "metadata={'source': 'chunk_15', 'distance': 0.818875, 'chunk_length': 1260, 'score_rerank': 0.5110913}\n",
      "\n",
      "Chunk 3: [Источник: Segment - Ultralytics YOLO Docs]\n",
      "The output of an instance segmentation model is a set of masks or contours that outline each object in the image, along with class labels and confidence sco..., \n",
      "metadata={'source': 'chunk_25', 'distance': 0.809926, 'chunk_length': 1110, 'score_rerank': 0.49988285}\n",
      "\n",
      "Chunk 4: [Источник: Computer Vision Tasks supported by Ultralytics YOLO11 - Ultralytics YOLO Docs]\n",
      "Train the YOLO11 model using the detection task.\n",
      "Use the model to make predictions by feeding in new images or..., \n",
      "metadata={'source': 'chunk_24', 'distance': 0.813338, 'chunk_length': 1312, 'score_rerank': 0.48727688}\n",
      "\n",
      "Chunk 5: [Источник: Ultralytics Solutions: Harness YOLO11 to Solve Real-World Problems - Ultralytics YOLO Docs]\n",
      "🚀: Estimate object speed using YOLO11 and object tracking techniques, crucial for applications li..., \n",
      "metadata={'source': 'chunk_14', 'distance': 0.819552, 'chunk_length': 1255, 'score_rerank': 0.44658518}\n",
      "\n",
      "\n",
      "=======================\n",
      "\n",
      "Итоговый промпт, подаваемый на вход модели:\n",
      "\n",
      "system: Вы полезный помощник, который отвечает на вопросы на русском языке. Ваши ответы должны быть четкими, информативными и полезными.\n",
      "human: Учитывай информацию из этих отрывков текста если считаешь нужным:\n",
      "1. [Источник: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      "The type of analysis to perform. Can be \"resolution_based\" for Resolution-Based Analysis or \"network_based\" for Neural Network-Based Analysis.\n",
      "Pre-initialized model object for \"network_based\" mode. If not provided, the default YOLO11m model will be used.\n",
      "A list of class indices to consider for object detection in \"network_based\" mode. If None, all classes will be considered.\n",
      "The confidence threshold for detection in \"network_based\" mode.\n",
      "Example of using:\n",
      "import cv2 from ultralytics import YOLO from patched_yolo_infer import auto_calculate_crop_values # Load the image img_path = \"test_image.jpg\" img = cv2 . imread ( img_path ) # Calculate the optimal crop size and overlap for an image shape_x , shape_y , overlap_x , overlap_y = auto_calculate_crop_values ( image = img , mode = \"network_based\" , model = YOLO ( \"yolo11m.pt\" )\n",
      "An example of working with\n",
      "is presented in Google Colab notebook -\n",
      "Implementing Patching at Different Resolutions\n",
      "2. [Источник: Detect - Ultralytics YOLO Docs]\n",
      "from ultralytics import YOLO # Load a pretrained model model = YOLO ( \"yolo11n.pt\" ) # Train the model on your custom dataset model . train ( data = \"my_custom_dataset.yaml\" , epochs = 100 , imgsz = 640 )\n",
      "yolo detect train data = my_custom_dataset.yaml model = yolo11n.pt epochs = 100 imgsz = 640\n",
      "For detailed configuration options, visit the\n",
      "What pretrained models are available in YOLO11?\n",
      "Ultralytics YOLO11 offers various pretrained models for object detection, segmentation, and pose estimation. These models are pretrained on the COCO dataset or ImageNet for classification tasks. Here are some of the available models:\n",
      "For a detailed list and performance metrics, refer to the\n",
      "How can I validate the accuracy of my trained YOLO model?\n",
      "To validate the accuracy of your trained YOLO11 model, you can use the\n",
      "method in Python or the\n",
      "yolo detect val\n",
      "command in CLI. This will provide metrics like mAP50-95, mAP50, and more.\n",
      "from ultralytics import YOLO # Load the model model = YOLO ( \"path/to/best.pt\" ) # Validate the model metrics = model . val () print ( metrics . box . map ) # mAP50-95\n",
      "yolo detect val model = path/to/best.pt\n",
      "For more validation details, visit the\n",
      "What formats can I export a YOLO11 model to?\n",
      "3. [Источник: Segment - Ultralytics YOLO Docs]\n",
      "The output of an instance segmentation model is a set of masks or contours that outline each object in the image, along with class labels and confidence scores for each object. Instance segmentation is useful when you need to know not only where objects are in an image, but also what their exact shape is.\n",
      "Run Segmentation with Pre-Trained Ultralytics YOLO Model in Python.\n",
      "YOLO11 Segment models use the\n",
      "and are pretrained on\n",
      "YOLO11 pretrained Segment models are shown here. Detect, Segment and Pose models are pretrained on the\n",
      "dataset, while Classify models are pretrained on the\n",
      "download automatically from the latest Ultralytics\n",
      "on first use.\n",
      "65.9 ± 1.1\n",
      "1.8 ± 0.0\n",
      "117.6 ± 4.9\n",
      "2.9 ± 0.0\n",
      "281.6 ± 1.2\n",
      "6.3 ± 0.1\n",
      "344.2 ± 3.2\n",
      "7.8 ± 0.2\n",
      "664.5 ± 3.2\n",
      "15.8 ± 0.7\n",
      "values are for single-model single-scale on\n",
      "yolo val segment data=coco.yaml device=0\n",
      "averaged over COCO val images using an\n",
      "Amazon EC2 P4d\n",
      "yolo val segment data=coco.yaml batch=1 device=0|cpu\n",
      "Train YOLO11n-seg on the COCO8-seg dataset for 100\n",
      "at image size 640. For a full list of available arguments see the\n",
      "4. [Источник: Computer Vision Tasks supported by Ultralytics YOLO11 - Ultralytics YOLO Docs]\n",
      "Train the YOLO11 model using the detection task.\n",
      "Use the model to make predictions by feeding in new images or video frames.\n",
      "from ultralytics import YOLO # Load a pre-trained YOLO model (adjust model type as needed) model = YOLO ( \"yolo11n.pt\" ) # n, s, m, l, x versions available # Perform object detection on an image results = model . predict ( source = \"image.jpg\" ) # Can also use video, directory, URL, etc. # Display the results results [ 0 ] . show () # Show the first image results\n",
      "# Run YOLO detection from the command line yolo detect model = yolo11n.pt source = \"image.jpg\" # Adjust model and source as needed\n",
      "For more detailed instructions, check out our\n",
      "What are the benefits of using YOLO11 for segmentation tasks?\n",
      "Using YOLO11 for segmentation tasks provides several advantages:\n",
      "The segmentation task leverages a variant of the U-Net architecture to achieve precise segmentation.\n",
      "YOLO11 is optimized for real-time applications, offering quick processing even for high-resolution images.\n",
      "It is ideal for medical imaging, autonomous driving, and other applications requiring detailed image segmentation.\n",
      "Learn more about the benefits and use cases of YOLO11 for segmentation in the\n",
      "image segmentation section\n",
      "5. [Источник: Ultralytics Solutions: Harness YOLO11 to Solve Real-World Problems - Ultralytics YOLO Docs]\n",
      "🚀: Estimate object speed using YOLO11 and object tracking techniques, crucial for applications like autonomous vehicles and traffic monitoring.\n",
      "🚀: Calculate distances between objects using\n",
      "centroids in YOLO11, essential for spatial analysis.\n",
      "🚀: Implement efficient queue management systems to minimize wait times and improve productivity using YOLO11.\n",
      "🚀: Organize and direct vehicle flow in parking areas with YOLO11, optimizing space utilization and user experience.\n",
      "📊: Conduct comprehensive data analysis to discover patterns and make informed decisions, leveraging YOLO11 for descriptive, predictive, and prescriptive analytics.\n",
      "Live Inference with Streamlit\n",
      "🚀: Leverage the power of YOLO11 for real-time\n",
      "directly through your web browser with a user-friendly Streamlit interface.\n",
      "Track Objects in Zone\n",
      "🎯 NEW: Learn how to track objects within specific zones of video frames using YOLO11 for precise and efficient monitoring.\n",
      "yolo SOLUTIONS SOLUTION_NAME ARGS\n",
      "is a required keyword.\n",
      "(optional) is one of:\n",
      "['count', 'heatmap', 'queue', 'speed', 'workout', 'analytics', 'trackzone']\n",
      "(optional) are custom\n",
      "pairs, such as\n",
      ", to override default settings.\n",
      "\n",
      "Ответь на вопрос: что такое YOLO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 17:59:40,256 - httpx - INFO - HTTP Request: POST http://localhost:8071/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "res = ask.process(query, message_number, collection_db_name, previous_messages, show_data_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO (You Only Look Once) — это серия моделей для задач компьютерного зрения, таких как объектное распознавание, сегментация объектов, позирование объектов и классификация. YOLO11 — это конкретная версия модели YOLO, предложенная компанией Ultralytics. Вот некоторые ключевые особенности YOLO11:\n",
      "\n",
      "1. **Объектное распознавание**: YOLO11 может использоваться для распознавания объектов на изображениях и видео. Для этого можно использовать предобученные модели или обучать модель на своих данных.\n",
      "\n",
      "2. **Сегментация объектов**: YOLO11 также поддерживает сегментацию объектов, что позволяет определить не только местоположение объектов, но и их точные границы.\n",
      "\n",
      "3. **Позирование объектов**: YOLO11 может использоваться для определения позы объектов, что полезно в задачах, связанных с человеческим движением и позированием.\n",
      "\n",
      "4. **Классификация**: YOLO11 также поддерживает классификацию изображений.\n",
      "\n",
      "5. **Преимущества**:\n",
      "   - **Оптимизация для реального времени**: YOLO11 оптимизирован для быстрого обработки изображений, что делает его подходящим для задач реального времени.\n",
      "   - **Универсальность**: YOLO11 может использоваться для различных задач компьютерного зрения, включая объектное распознавание, сегментацию и классификацию.\n",
      "   - **Простота использования**: YOLO11 легко интегрируется в различные приложения, включая веб-интерфейсы и реальные системы.\n",
      "\n",
      "6. **Инструменты и библиотеки**:\n",
      "   - **GitHub**: YOLO11 доступен в виде библиотеки на GitHub, которая предоставляет инструменты для анализа изображений.\n",
      "   - **Ultralytics YOLO Docs**: Документация от Ultralytics содержит подробные инструкции по использованию YOLO11, включая примеры кода и настройки.\n",
      "\n",
      "7. **Примеры использования**:\n",
      "   - **Объектное распознавание**: Использование YOLO11 для распознавания объектов на изображениях.\n",
      "   - **Сегментация**: Использование YOLO11 для сегментации объектов на изображениях.\n",
      "   - **Реальное время**: Использование YOLO11 для реального времени анализа изображений и видео.\n",
      "\n",
      "8. **Валидация и тестирование**: YOLO11 можно валидировать и тестировать с помощью встроенных инструментов, таких как `yolo detect val`.\n",
      "\n",
      "9. **Экспорт моделей**: YOLO11 модели можно экспортировать в различные форматы для использования в других приложениях.\n",
      "\n",
      "Инструменты и библиотеки, такие как `ultralytics`, предоставляют удобный интерфейс для работы с YOLO11, включая загрузку предобученных моделей, обучение на новых данных и валидацию результатов.\n"
     ]
    }
   ],
   "source": [
    "print(res.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа с историей чата"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='что такое YOLO', additional_kwargs={}, response_metadata={}), AIMessage(content='YOLO (You Only Look Once) — это серия моделей для задач компьютерного зрения, таких как объектное распознавание, сегментация объектов, позирование объектов и классификация. YOLO11 — это конкретная версия модели YOLO, предложенная компанией Ultralytics. Вот некоторые ключевые особенности YOLO11:\\n\\n1. **Объектное распознавание**: YOLO11 может использоваться для распознавания объектов на изображениях и видео. Для этого можно использовать предобученные модели или обучать модель на своих данных.\\n\\n2. **Сегментация объектов**: YOLO11 также поддерживает сегментацию объектов, что позволяет определить не только местоположение объектов, но и их точные границы.\\n\\n3. **Позирование объектов**: YOLO11 может использоваться для определения позы объектов, что полезно в задачах, связанных с человеческим движением и позированием.\\n\\n4. **Классификация**: YOLO11 также поддерживает классификацию изображений.\\n\\n5. **Преимущества**:\\n   - **Оптимизация для реального времени**: YOLO11 оптимизирован для быстрого обработки изображений, что делает его подходящим для задач реального времени.\\n   - **Универсальность**: YOLO11 может использоваться для различных задач компьютерного зрения, включая объектное распознавание, сегментацию и классификацию.\\n   - **Простота использования**: YOLO11 легко интегрируется в различные приложения, включая веб-интерфейсы и реальные системы.\\n\\n6. **Инструменты и библиотеки**:\\n   - **GitHub**: YOLO11 доступен в виде библиотеки на GitHub, которая предоставляет инструменты для анализа изображений.\\n   - **Ultralytics YOLO Docs**: Документация от Ultralytics содержит подробные инструкции по использованию YOLO11, включая примеры кода и настройки.\\n\\n7. **Примеры использования**:\\n   - **Объектное распознавание**: Использование YOLO11 для распознавания объектов на изображениях.\\n   - **Сегментация**: Использование YOLO11 для сегментации объектов на изображениях.\\n   - **Реальное время**: Использование YOLO11 для реального времени анализа изображений и видео.\\n\\n8. **Валидация и тестирование**: YOLO11 можно валидировать и тестировать с помощью встроенных инструментов, таких как `yolo detect val`.\\n\\n9. **Экспорт моделей**: YOLO11 модели можно экспортировать в различные форматы для использования в других приложениях.\\n\\nИнструменты и библиотеки, такие как `ultralytics`, предоставляют удобный интерфейс для работы с YOLO11, включая загрузку предобученных моделей, обучение на новых данных и валидацию результатов.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "previous_messages = ask.get_new_history(res)\n",
    "print(previous_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query= \"кратко надо в 1 предложение\"\n",
    "message_number=1\n",
    "collection_db_name=\"data\"\n",
    "show_data_info=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 18:02:00,805 - httpx - INFO - HTTP Request: POST http://localhost:8071/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-19 18:02:00,806 - nodes.LLMNode - INFO - Измененный текст сообщения для получения эмеддингов: Нужно кратко описать YOLO (You Only Look Once) в одном предложении.\n",
      "2025-02-19 18:02:00,810 - nodes.EmbedderNode - INFO - Обработка батча 1, размер батча: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результат векторного поиска:\n",
      "Chunk 1: [Источник: Koldim2001 (Dmitry Kolesnikov) · GitHub]\n",
      "You can’t perform that action at this time., \n",
      "metadata={'source': 'chunk_0', 'distance': 0.827942, 'chunk_length': 95}\n",
      "\n",
      "Chunk 2: [Источник: Detect - Ultralytics YOLO Docs]\n",
      "yolo detect val model = yolo11n.pt # val official model yolo detect val model = path/to/best.pt # val custom model\n",
      "Use a trained YOLO11n model to run predict..., \n",
      "metadata={'source': 'chunk_1', 'distance': 0.824334, 'chunk_length': 1263}\n",
      "\n",
      "Chunk 3: [Источник: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      "classes_names: These are the human-readable names corresponding to ..., \n",
      "metadata={'source': 'chunk_2', 'distance': 0.819741, 'chunk_length': 1362}\n",
      "\n",
      "Chunk 4: [Источник: Segment - Ultralytics YOLO Docs]\n",
      "from ultralytics import YOLO # Load a model model = YOLO ( \"yolo11n-seg.pt\" ) # load an official model model = YOLO ( \"path/to/best.pt\" ) # load a custom mo..., \n",
      "metadata={'source': 'chunk_3', 'distance': 0.817718, 'chunk_length': 823}\n",
      "\n",
      "Chunk 5: [Источник: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      "from patched_yolo_infer import visualize_results # Assuming result ..., \n",
      "metadata={'source': 'chunk_4', 'distance': 0.813763, 'chunk_length': 1326}\n",
      "\n",
      "Chunk 6: [Источник: Segment - Ultralytics YOLO Docs]\n",
      "from ultralytics import YOLO # Load a model model = YOLO ( \"yolo11n-seg.yaml\" ) # build a new model from YAML model = YOLO ( \"yolo11n-seg.pt\" ) # load a pre..., \n",
      "metadata={'source': 'chunk_5', 'distance': 0.813704, 'chunk_length': 1259}\n",
      "\n",
      "Chunk 7: [Источник: Detect - Ultralytics YOLO Docs]\n",
      "# Build a new model from YAML and start training from scratch yolo detect train data = coco8.yaml model = yolo11n.yaml epochs = 100 imgsz = 640 # Start train..., \n",
      "metadata={'source': 'chunk_6', 'distance': 0.813239, 'chunk_length': 1234}\n",
      "\n",
      "Chunk 8: [Источник: GitHub - Koldim2001/COCO_to_YOLOv8: Converting COCO annotation (CVAT) to annotation for YOLO-seg (instance segmentation) and YOLO-obb (oriented bounding box detection)]\n",
      "--coco_dataset TEXT ..., \n",
      "metadata={'source': 'chunk_7', 'distance': 0.81006, 'chunk_length': 1154}\n",
      "\n",
      "Chunk 9: [Источник: Koldim2001 (Dmitry Kolesnikov) · GitHub]\n",
      "Koldim2001 (Dmitry Kolesnikov) · GitHub\n",
      "Block or report Koldim2001\n",
      "Learn more about\n",
      "Add an optional note:\n",
      "Please don't include any personal informat..., \n",
      "metadata={'source': 'chunk_8', 'distance': 0.810041, 'chunk_length': 1277}\n",
      "\n",
      "Chunk 10: [Источник: GitHub - Koldim2001/COCO_to_YOLOv8: Converting COCO annotation (CVAT) to annotation for YOLO-seg (instance segmentation) and YOLO-obb (oriented bounding box detection)]\n",
      "Ключевое применение ..., \n",
      "metadata={'source': 'chunk_9', 'distance': 0.809664, 'chunk_length': 1415}\n",
      "\n",
      "Chunk 11: [Источник: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      "IoU threshold for non-maximum suppression YOLO of single  crop.\n",
      "Lis..., \n",
      "metadata={'source': 'chunk_10', 'distance': 0.809629, 'chunk_length': 1292}\n",
      "\n",
      "Chunk 12: [Источник: Detect - Ultralytics YOLO Docs]\n",
      "YOLO11 Detect models are the default YOLO11 models, i.e.\n",
      "and are pretrained on\n",
      "YOLO11 pretrained Detect models are shown here. Detect, Segment and Pose model..., \n",
      "metadata={'source': 'chunk_11', 'distance': 0.808179, 'chunk_length': 1128}\n",
      "\n",
      "Chunk 13: [Источник: GitHub - Koldim2001/COCO_to_YOLOv8: Converting COCO annotation (CVAT) to annotation for YOLO-seg (instance segmentation) and YOLO-obb (oriented bounding box detection)]\n",
      "How to run the code:..., \n",
      "metadata={'source': 'chunk_12', 'distance': 0.806214, 'chunk_length': 1407}\n",
      "\n",
      "Chunk 14: [Источник: Segment - Ultralytics YOLO Docs]\n",
      "from ultralytics import YOLO # Load a model model = YOLO ( \"yolo11n-seg.pt\" ) # load an official model model = YOLO ( \"path/to/best.pt\" ) # load a custom mo..., \n",
      "metadata={'source': 'chunk_13', 'distance': 0.805313, 'chunk_length': 1237}\n",
      "\n",
      "Chunk 15: [Источник: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      "The type of analysis to perform. Can be \"resolution_based\" for Reso..., \n",
      "metadata={'source': 'chunk_14', 'distance': 0.805282, 'chunk_length': 1082}\n",
      "\n",
      "Chunk 16: [Источник: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      "Whether to show bounding boxes.\n",
      "Whether to show class labels.\n",
      "Wheth..., \n",
      "metadata={'source': 'chunk_15', 'distance': 0.804939, 'chunk_length': 1197}\n",
      "\n",
      "Chunk 17: [Источник: Ultralytics YOLO11 Modes - Ultralytics YOLO Docs]\n",
      "# Export a YOLO model to ONNX format from the command line yolo export model = yolo11n.pt format = onnx\n",
      "Detailed steps for each export form..., \n",
      "metadata={'source': 'chunk_16', 'distance': 0.803485, 'chunk_length': 1168}\n",
      "\n",
      "Chunk 18: [Источник: Ultralytics Solutions: Harness YOLO11 to Solve Real-World Problems - Ultralytics YOLO Docs]\n",
      "🚀: Estimate object speed using YOLO11 and object tracking techniques, crucial for applications li..., \n",
      "metadata={'source': 'chunk_17', 'distance': 0.803329, 'chunk_length': 1255}\n",
      "\n",
      "Chunk 19: [Источник: GitHub - Koldim2001/COCO_to_YOLOv8: Converting COCO annotation (CVAT) to annotation for YOLO-seg (instance segmentation) and YOLO-obb (oriented bounding box detection)]\n",
      "GitHub - Koldim2001/..., \n",
      "metadata={'source': 'chunk_18', 'distance': 0.802009, 'chunk_length': 1418}\n",
      "\n",
      "Chunk 20: [Источник: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      "GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for ..., \n",
      "metadata={'source': 'chunk_19', 'distance': 0.80161, 'chunk_length': 1209}\n",
      "\n",
      "Chunk 21: [Источник: Ultralytics Solutions: Harness YOLO11 to Solve Real-World Problems - Ultralytics YOLO Docs]\n",
      "yolo solutions count show = True # for object counting yolo solutions source = \"path/to/video/fil..., \n",
      "metadata={'source': 'chunk_20', 'distance': 0.800239, 'chunk_length': 1305}\n",
      "\n",
      "Chunk 22: [Источник: ЦТиП: Machine learning-платформа]\n",
      "свидетельство № АО-20211230-3269820362-3 от 10.01.2022, \n",
      "metadata={'source': 'chunk_21', 'distance': 0.798681, 'chunk_length': 99}\n",
      "\n",
      "Chunk 23: [Источник: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      "There is an opportunity to produce cropping into patches at differe..., \n",
      "metadata={'source': 'chunk_22', 'distance': 0.798068, 'chunk_length': 983}\n",
      "\n",
      "Chunk 24: [Источник: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      "boxes: These bounding boxes are represented as a list of lists, whe..., \n",
      "metadata={'source': 'chunk_23', 'distance': 0.794719, 'chunk_length': 1283}\n",
      "\n",
      "Chunk 25: [Источник: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      ": To detect more objects within a single crop, increase the\n",
      "paramet..., \n",
      "metadata={'source': 'chunk_24', 'distance': 0.794482, 'chunk_length': 1312}\n",
      "\n",
      "Chunk 26: [Источник: Ultralytics YOLO11 Modes - Ultralytics YOLO Docs]\n",
      "mAP (mean Average Precision)\n",
      ": This evaluates the accuracy of object detection.\n",
      "IOU (Intersection over Union)\n",
      ": Measures the overlap betwee..., \n",
      "metadata={'source': 'chunk_25', 'distance': 0.79253, 'chunk_length': 1238}\n",
      "\n",
      "Chunk 27: [Источник: ПроТех Лаб]\n",
      "Укажите ту почту, которую вы часто проверяете, т.к. на нее будет приходить информация о проекте. Без спама, только важное, мы заботимся о вашем информационном поле\n",
      "Место работы,..., \n",
      "metadata={'source': 'chunk_26', 'distance': 0.792462, 'chunk_length': 921}\n",
      "\n",
      "Chunk 28: [Источник: Дмитрий Колесников | OTUS]\n",
      "Дмитрий Колесников | OTUS\n",
      "Аналитика и анализ\n",
      "IT без программирования\n",
      "Календарь запуска курсов\n",
      "Подписка на курсы\n",
      "Проверьте свои знания\n",
      "СМИ о нас\n",
      "Как выбрать курс\n",
      "В..., \n",
      "metadata={'source': 'chunk_27', 'distance': 0.792155, 'chunk_length': 1266}\n",
      "\n",
      "Chunk 29: [Источник: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      "Upon completion, you receive the result, from which you can extract..., \n",
      "metadata={'source': 'chunk_28', 'distance': 0.791859, 'chunk_length': 1351}\n",
      "\n",
      "Chunk 30: [Источник: Computer Vision Tasks supported by Ultralytics YOLO11 - Ultralytics YOLO Docs]\n",
      "Train the YOLO11 model using the detection task.\n",
      "Use the model to make predictions by feeding in new images or..., \n",
      "metadata={'source': 'chunk_29', 'distance': 0.791789, 'chunk_length': 1312}\n",
      "\n",
      "\n",
      "=======================\n",
      "\n",
      "Результат после реранка:\n",
      "Chunk 1: [Источник: Koldim2001 (Dmitry Kolesnikov) · GitHub]\n",
      "Koldim2001 (Dmitry Kolesnikov) · GitHub\n",
      "Block or report Koldim2001\n",
      "Learn more about\n",
      "Add an optional note:\n",
      "Please don't include any personal informat..., \n",
      "metadata={'source': 'chunk_8', 'distance': 0.810041, 'chunk_length': 1277, 'score_rerank': 0.20850106}\n",
      "\n",
      "Chunk 2: [Источник: Segment - Ultralytics YOLO Docs]\n",
      "from ultralytics import YOLO # Load a model model = YOLO ( \"yolo11n-seg.pt\" ) # load an official model model = YOLO ( \"path/to/best.pt\" ) # load a custom mo..., \n",
      "metadata={'source': 'chunk_13', 'distance': 0.805313, 'chunk_length': 1237, 'score_rerank': 0.040087357}\n",
      "\n",
      "Chunk 3: [Источник: Detect - Ultralytics YOLO Docs]\n",
      "YOLO11 Detect models are the default YOLO11 models, i.e.\n",
      "and are pretrained on\n",
      "YOLO11 pretrained Detect models are shown here. Detect, Segment and Pose model..., \n",
      "metadata={'source': 'chunk_11', 'distance': 0.808179, 'chunk_length': 1128, 'score_rerank': 0.04001227}\n",
      "\n",
      "Chunk 4: [Источник: Detect - Ultralytics YOLO Docs]\n",
      "# Build a new model from YAML and start training from scratch yolo detect train data = coco8.yaml model = yolo11n.yaml epochs = 100 imgsz = 640 # Start train..., \n",
      "metadata={'source': 'chunk_6', 'distance': 0.813239, 'chunk_length': 1234, 'score_rerank': 0.03963884}\n",
      "\n",
      "Chunk 5: [Источник: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      "classes_names: These are the human-readable names corresponding to ..., \n",
      "metadata={'source': 'chunk_2', 'distance': 0.819741, 'chunk_length': 1362, 'score_rerank': 0.038683496}\n",
      "\n",
      "\n",
      "=======================\n",
      "\n",
      "Итоговый промпт, подаваемый на вход модели:\n",
      "\n",
      "system: Вы полезный помощник, который отвечает на вопросы на русском языке. Ваши ответы должны быть четкими, информативными и полезными.\n",
      "human: что такое YOLO\n",
      "ai: YOLO (You Only Look Once) — это серия моделей для задач компьютерного зрения, таких как объектное распознавание, сегментация объектов, позирование объектов и классификация. YOLO11 — это конкретная версия модели YOLO, предложенная компанией Ultralytics. Вот некоторые ключевые особенности YOLO11:\n",
      "\n",
      "1. **Объектное распознавание**: YOLO11 может использоваться для распознавания объектов на изображениях и видео. Для этого можно использовать предобученные модели или обучать модель на своих данных.\n",
      "\n",
      "2. **Сегментация объектов**: YOLO11 также поддерживает сегментацию объектов, что позволяет определить не только местоположение объектов, но и их точные границы.\n",
      "\n",
      "3. **Позирование объектов**: YOLO11 может использоваться для определения позы объектов, что полезно в задачах, связанных с человеческим движением и позированием.\n",
      "\n",
      "4. **Классификация**: YOLO11 также поддерживает классификацию изображений.\n",
      "\n",
      "5. **Преимущества**:\n",
      "   - **Оптимизация для реального времени**: YOLO11 оптимизирован для быстрого обработки изображений, что делает его подходящим для задач реального времени.\n",
      "   - **Универсальность**: YOLO11 может использоваться для различных задач компьютерного зрения, включая объектное распознавание, сегментацию и классификацию.\n",
      "   - **Простота использования**: YOLO11 легко интегрируется в различные приложения, включая веб-интерфейсы и реальные системы.\n",
      "\n",
      "6. **Инструменты и библиотеки**:\n",
      "   - **GitHub**: YOLO11 доступен в виде библиотеки на GitHub, которая предоставляет инструменты для анализа изображений.\n",
      "   - **Ultralytics YOLO Docs**: Документация от Ultralytics содержит подробные инструкции по использованию YOLO11, включая примеры кода и настройки.\n",
      "\n",
      "7. **Примеры использования**:\n",
      "   - **Объектное распознавание**: Использование YOLO11 для распознавания объектов на изображениях.\n",
      "   - **Сегментация**: Использование YOLO11 для сегментации объектов на изображениях.\n",
      "   - **Реальное время**: Использование YOLO11 для реального времени анализа изображений и видео.\n",
      "\n",
      "8. **Валидация и тестирование**: YOLO11 можно валидировать и тестировать с помощью встроенных инструментов, таких как `yolo detect val`.\n",
      "\n",
      "9. **Экспорт моделей**: YOLO11 модели можно экспортировать в различные форматы для использования в других приложениях.\n",
      "\n",
      "Инструменты и библиотеки, такие как `ultralytics`, предоставляют удобный интерфейс для работы с YOLO11, включая загрузку предобученных моделей, обучение на новых данных и валидацию результатов.\n",
      "human: Учитывай информацию из этих отрывков текста если считаешь нужным:\n",
      "1. [Источник: Koldim2001 (Dmitry Kolesnikov) · GitHub]\n",
      "Koldim2001 (Dmitry Kolesnikov) · GitHub\n",
      "Block or report Koldim2001\n",
      "Learn more about\n",
      "Add an optional note:\n",
      "Please don't include any personal information such as legal names or email addresses. Maximum 100 characters, markdown supported. This note will be visible to only you.\n",
      "Contact GitHub support about this user’s behavior.\n",
      "Learn more about\n",
      "Hi there 👋 I'm Dmitry\n",
      "I am a Lead Computer Vision Engineer and Data Scientist, specializing in ML/DL model development.\n",
      "Connect with me:\n",
      "Languages and Tools:\n",
      "Python library for YOLO small object detection and instance segmentation\n",
      "Converting COCO annotation (CVAT) to annotation for YOLO-seg (instance segmentation) and YOLO-obb (oriented bounding box detection)\n",
      "Анализ трафика на круговом движении с использованием компьютерного зрения\n",
      "Веб-версия проекта по получение ВЭКГ на основе ЭКГ и СППР на основе векторных петель\n",
      "Репозиторий для обучения нейросетевых моделей по семантической сегментации + пример использования моделей на практике\n",
      "Обнаружение точек лица, рук и всего тела с использованием DL алгоритмов\n",
      "Something went wrong, please refresh the page to try again.\n",
      "If the problem persists, check the\n",
      "GitHub status page\n",
      "© 2025 GitHub, Inc.\n",
      "Do not share my personal information\n",
      "2. [Источник: Segment - Ultralytics YOLO Docs]\n",
      "from ultralytics import YOLO # Load a model model = YOLO ( \"yolo11n-seg.pt\" ) # load an official model model = YOLO ( \"path/to/best.pt\" ) # load a custom model # Predict with the model results = model ( \"https://ultralytics.com/images/bus.jpg\" ) # predict on an image # Access the results for result in results : xy = result . masks . xy # mask in polygon format xyn = result . masks . xyn # normalized masks = result . masks . data # mask in matrix format (num_objects x H x W)\n",
      "yolo segment predict model = yolo11n-seg.pt source = 'https://ultralytics.com/images/bus.jpg' # predict with official model yolo segment predict model = path/to/best.pt source = 'https://ultralytics.com/images/bus.jpg' # predict with custom model\n",
      "mode details in the\n",
      "Export a YOLO11n-seg model to a different format like ONNX, CoreML, etc.\n",
      "from ultralytics import YOLO # Load a model model = YOLO ( \"yolo11n-seg.pt\" ) # load an official model model = YOLO ( \"path/to/best.pt\" ) # load a custom trained model # Export the model model . export ( format = \"onnx\" )\n",
      "yolo export model = yolo11n-seg.pt format = onnx # export official model yolo export model = path/to/best.pt format = onnx # export custom trained model\n",
      "3. [Источник: Detect - Ultralytics YOLO Docs]\n",
      "YOLO11 Detect models are the default YOLO11 models, i.e.\n",
      "and are pretrained on\n",
      "YOLO11 pretrained Detect models are shown here. Detect, Segment and Pose models are pretrained on the\n",
      "dataset, while Classify models are pretrained on the\n",
      "download automatically from the latest Ultralytics\n",
      "on first use.\n",
      "56.1 ± 0.8\n",
      "1.5 ± 0.0\n",
      "90.0 ± 1.2\n",
      "2.5 ± 0.0\n",
      "183.2 ± 2.0\n",
      "4.7 ± 0.1\n",
      "238.6 ± 1.4\n",
      "6.2 ± 0.1\n",
      "462.8 ± 6.7\n",
      "11.3 ± 0.2\n",
      "values are for single-model single-scale on\n",
      "yolo val detect data=coco.yaml device=0\n",
      "averaged over COCO val images using an\n",
      "Amazon EC2 P4d\n",
      "yolo val detect data=coco.yaml batch=1 device=0|cpu\n",
      "Train YOLO11n on the COCO8 dataset for 100\n",
      "at image size 640. For a full list of available arguments see the\n",
      "from ultralytics import YOLO # Load a model model = YOLO ( \"yolo11n.yaml\" ) # build a new model from YAML model = YOLO ( \"yolo11n.pt\" ) # load a pretrained model (recommended for training) model = YOLO ( \"yolo11n.yaml\" ) . load ( \"yolo11n.pt\" ) # build from YAML and transfer weights # Train the model results = model . train ( data = \"coco8.yaml\" , epochs = 100 , imgsz = 640 )\n",
      "4. [Источник: Detect - Ultralytics YOLO Docs]\n",
      "# Build a new model from YAML and start training from scratch yolo detect train data = coco8.yaml model = yolo11n.yaml epochs = 100 imgsz = 640 # Start training from a pretrained *.pt model yolo detect train data = coco8.yaml model = yolo11n.pt epochs = 100 imgsz = 640 # Build a new model from YAML, transfer pretrained weights to it and start training yolo detect train data = coco8.yaml model = yolo11n.yaml pretrained = yolo11n.pt epochs = 100 imgsz = 640\n",
      "YOLO detection dataset format can be found in detail in the\n",
      ". To convert your existing dataset from other formats (like COCO etc.) to YOLO format, please use\n",
      "tool by Ultralytics.\n",
      "Validate trained YOLO11n model\n",
      "on the COCO8 dataset. No arguments are needed as the\n",
      "retains its training\n",
      "and arguments as model attributes.\n",
      "from ultralytics import YOLO # Load a model model = YOLO ( \"yolo11n.pt\" ) # load an official model model = YOLO ( \"path/to/best.pt\" ) # load a custom model # Validate the model metrics = model . val () # no arguments needed, dataset and settings remembered metrics . box . map # map50-95 metrics . box . map50 # map50 metrics . box . map75 # map75 metrics . box . maps # a list contains map50-95 of each category\n",
      "5. [Источник: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      "classes_names: These are the human-readable names corresponding to the class IDs. They provide semantic labels for the detected objects, making the results easier to interpret.\n",
      "import cv2 from patched_yolo_infer import MakeCropsDetectThem , CombineDetections # Load the image img_path = \"test_image.jpg\" img = cv2 . imread ( img_path ) element_crops = MakeCropsDetectThem ( image = img , model_path = \"yolo11m.pt\" , segment = False , shape_x = 640 , shape_y = 640 , overlap_x = 25 , overlap_y = 25 , conf = 0.5 , iou = 0.7 ,\n",
      ") result = CombineDetections ( element_crops , nms_threshold = 0.25 ) # Final Results: img = result . image confidences = result . filtered_confidences boxes = result . filtered_boxes polygons = result . filtered_polygons classes_ids = result . filtered_classes_id classes_names = result . filtered_classes_names\n",
      "Explanation of possible input arguments:\n",
      "Class implementing cropping and passing crops through a neural network for detection/segmentation:\n",
      "The input image in BGR format.\n",
      "Path to the YOLO model.\n",
      "Pre-initialized model object. If provided, the model will be used directly instead of loading from model_path.\n",
      "Size of the input image for inference YOLO.\n",
      "Confidence threshold for detections YOLO.\n",
      "\n",
      "Ответь на вопрос: кратко надо в 1 предложение\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 18:02:03,681 - httpx - INFO - HTTP Request: POST http://localhost:8071/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "res = ask.process(query, message_number, collection_db_name, previous_messages, show_data_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO11 (You Only Look Once) — это модель для объектного распознавания и сегментации, предложенная компанией Ultralytics, которая позволяет быстро и эффективно обрабатывать изображения и видео для определения и сегментации объектов.\n"
     ]
    }
   ],
   "source": [
    "print(res.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Нужно кратко описать YOLO (You Only Look Once) в одном предложении.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.upgraded_query"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MAIN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
