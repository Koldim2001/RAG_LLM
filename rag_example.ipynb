{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG –ø—Ä–∏–º–µ—Ä—ã"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 17:51:03,884 - nodes.VectorDBNode - INFO - Connected to Milvus at localhost:19530 successfully!\n",
      "2025-02-19 17:51:03,884 - nodes.DataParsingNode - INFO - –ü–∞—Ä—Å–∏–Ω–≥ https://www.eurochem.ru/...\n",
      "2025-02-19 17:51:04,106 - nodes.DataParsingNode - INFO - –ü–∞—Ä—Å–∏–Ω–≥ https://www.eurochem.ru/global-operations/...\n",
      "2025-02-19 17:51:04,271 - nodes.DataParsingNode - INFO - –ü–∞—Ä—Å–∏–Ω–≥ https://www.eurochem.ru/about-us/komplaens/...\n",
      "2025-02-19 17:51:04,410 - nodes.DataParsingNode - INFO - –ü–∞—Ä—Å–∏–Ω–≥ https://www.eurochem.ru/proteh-lab/...\n",
      "2025-02-19 17:51:04,581 - nodes.DataParsingNode - INFO - –ü–∞—Ä—Å–∏–Ω–≥ https://digtp.com/...\n",
      "2025-02-19 17:51:04,689 - nodes.DataParsingNode - INFO - –ü–∞—Ä—Å–∏–Ω–≥ https://digtp.com/projects/machine-learning-platforma...\n",
      "2025-02-19 17:51:04,784 - nodes.DataParsingNode - INFO - –ü–∞—Ä—Å–∏–Ω–≥ https://digtp.com/projects/rekomendatelnye-modeli...\n",
      "2025-02-19 17:51:04,915 - nodes.DataParsingNode - INFO - –ü–∞—Ä—Å–∏–Ω–≥ https://digtp.com/projects/mobilnoe-prilozenie-mineralogiia...\n",
      "2025-02-19 17:51:05,012 - nodes.DataParsingNode - INFO - –ü–∞—Ä—Å–∏–Ω–≥ https://digtp.com/contacts...\n",
      "2025-02-19 17:51:05,117 - nodes.DataParsingNode - INFO - –ü–∞—Ä—Å–∏–Ω–≥ https://www.eurochem-career.com/news/iskusstvennyi-intellekt-v-ximii-gpt-assistenty-v-evroxime...\n",
      "2025-02-19 17:51:05,355 - nodes.DataParsingNode - INFO - –ü–∞—Ä—Å–∏–Ω–≥ https://otus.ru/instructors/10517...\n",
      "2025-02-19 17:51:09,265 - nodes.DataParsingNode - INFO - –ü–∞—Ä—Å–∏–Ω–≥ https://ru.wikipedia.org/wiki/–ï–≤—Ä–æ–•–∏–º...\n",
      "2025-02-19 17:51:09,537 - nodes.DataParsingNode - INFO - –ü–∞—Ä—Å–∏–Ω–≥ https://www.eurochem.ru/usolskij-kalijnyj-kombinat/...\n",
      "2025-02-19 17:51:09,683 - nodes.DataParsingNode - INFO - –ü–∞—Ä—Å–∏–Ω–≥ https://uralmines.ru/evrohim-usolskij-kalijnyj-kombinat/...\n",
      "2025-02-19 17:51:10,523 - nodes.DataParsingNode - INFO - –ü–∞—Ä—Å–∏–Ω–≥ https://docs.ultralytics.com/tasks/segment...\n",
      "2025-02-19 17:51:11,292 - nodes.DataParsingNode - INFO - –ü–∞—Ä—Å–∏–Ω–≥ https://docs.ultralytics.com/tasks/detect...\n",
      "2025-02-19 17:51:11,724 - nodes.DataParsingNode - INFO - –ü–∞—Ä—Å–∏–Ω–≥ https://docs.ultralytics.com/tasks...\n",
      "2025-02-19 17:51:12,023 - nodes.DataParsingNode - INFO - –ü–∞—Ä—Å–∏–Ω–≥ https://docs.ultralytics.com/modes/...\n",
      "2025-02-19 17:51:12,383 - nodes.DataParsingNode - INFO - –ü–∞—Ä—Å–∏–Ω–≥ https://docs.ultralytics.com/solutions...\n",
      "2025-02-19 17:51:12,952 - nodes.DataParsingNode - INFO - –ü–∞—Ä—Å–∏–Ω–≥ https://github.com/Koldim2001...\n",
      "2025-02-19 17:51:13,619 - nodes.DataParsingNode - INFO - –ü–∞—Ä—Å–∏–Ω–≥ https://github.com/Koldim2001/YOLO-Patch-Based-Inference...\n",
      "2025-02-19 17:51:14,421 - nodes.DataParsingNode - INFO - –ü–∞—Ä—Å–∏–Ω–≥ https://github.com/Koldim2001/TrafficAnalyzer...\n",
      "2025-02-19 17:51:15,300 - nodes.DataParsingNode - INFO - –ü–∞—Ä—Å–∏–Ω–≥ https://github.com/Koldim2001/COCO_to_YOLOv8...\n",
      "2025-02-19 17:51:16,050 - nodes.EmbedderNode - INFO - –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ 1, —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: 156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ß–∞–Ω–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª results/chunks_output.txt\n",
      "–ò—Ç–æ–≥–æ–≤—ã–π —Ç–µ–∫—Å—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª results/output_parsing.txt\n",
      "DataElement(\n",
      "  –°ollection db name: data\n",
      "  URLs: https://www.eurochem.ru/, https://www.eurochem.ru/global-operations/, https://www.eurochem.ru/about-us/komplaens/, https://www.eurochem.ru/proteh-lab/, https://digtp.com/, https://digtp.com/projects/machine-learning-platforma, https://digtp.com/projects/rekomendatelnye-modeli, https://digtp.com/projects/mobilnoe-prilozenie-mineralogiia, https://digtp.com/contacts, https://www.eurochem-career.com/news/iskusstvennyi-intellekt-v-ximii-gpt-assistenty-v-evroxime, https://otus.ru/instructors/10517, https://ru.wikipedia.org/wiki/–ï–≤—Ä–æ–•–∏–º, https://www.eurochem.ru/usolskij-kalijnyj-kombinat/, https://uralmines.ru/evrohim-usolskij-kalijnyj-kombinat/, https://docs.ultralytics.com/tasks/segment, https://docs.ultralytics.com/tasks/detect, https://docs.ultralytics.com/tasks, https://docs.ultralytics.com/modes/, https://docs.ultralytics.com/solutions, https://github.com/Koldim2001, https://github.com/Koldim2001/YOLO-Patch-Based-Inference, https://github.com/Koldim2001/TrafficAnalyzer, https://github.com/Koldim2001/COCO_to_YOLOv8\n",
      "  URL Data: 23 entries\n",
      "  Chunks: 156 chunks\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 17:51:17,465 - nodes.VectorDBNode - INFO - Collection 'data' deleted successfully.\n",
      "2025-02-19 17:51:17,542 - nodes.VectorDBNode - INFO - Collection 'data' created successfully.\n",
      "2025-02-19 17:51:18,061 - nodes.VectorDBNode - INFO - Index created on field 'embedding' for collection 'data'.\n",
      "2025-02-19 17:51:18,716 - nodes.VectorDBNode - INFO - Collection 'data' loaded successfully.\n",
      "2025-02-19 17:51:18,859 - nodes.VectorDBNode - INFO - Inserted 156 records into collection 'data'.\n"
     ]
    }
   ],
   "source": [
    "from services.MakeDatasetRAG import MakeDatasetRAG\n",
    "\n",
    "mk = MakeDatasetRAG()\n",
    "\n",
    "url_list = [\n",
    "    \"https://www.eurochem.ru/\",\n",
    "    \"https://www.eurochem.ru/global-operations/\",\n",
    "    \"https://www.eurochem.ru/about-us/komplaens/\",\n",
    "    \"https://www.eurochem.ru/proteh-lab/\",\n",
    "    \"https://digtp.com/\",\n",
    "    \"https://digtp.com/projects/machine-learning-platforma\",\n",
    "    \"https://digtp.com/projects/rekomendatelnye-modeli\",\n",
    "    \"https://digtp.com/projects/mobilnoe-prilozenie-mineralogiia\",\n",
    "    \"https://digtp.com/contacts\",\n",
    "    \"https://www.eurochem-career.com/news/iskusstvennyi-intellekt-v-ximii-gpt-assistenty-v-evroxime\",\n",
    "    \"https://otus.ru/instructors/10517\",\n",
    "    \"https://ru.wikipedia.org/wiki/–ï–≤—Ä–æ–•–∏–º\",\n",
    "    \"https://www.eurochem.ru/usolskij-kalijnyj-kombinat/\",\n",
    "    \"https://uralmines.ru/evrohim-usolskij-kalijnyj-kombinat/\",\n",
    "    \"https://docs.ultralytics.com/tasks/segment\",\n",
    "    \"https://docs.ultralytics.com/tasks/detect\",\n",
    "    \"https://docs.ultralytics.com/tasks\",\n",
    "    \"https://docs.ultralytics.com/modes/\",\n",
    "    \"https://docs.ultralytics.com/solutions\",\n",
    "    \"https://github.com/Koldim2001\",\n",
    "    \"https://github.com/Koldim2001/YOLO-Patch-Based-Inference\",\n",
    "    \"https://github.com/Koldim2001/TrafficAnalyzer\",\n",
    "    \"https://github.com/Koldim2001/COCO_to_YOLOv8\"\n",
    "]\n",
    "\n",
    "collection_db_name = \"data\"\n",
    "\n",
    "mk.process(url_list, collection_db_name, show_data_info=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 17:51:20,888 - nodes.VectorDBNode - INFO - First 5 records in collection 'data':\n",
      "2025-02-19 17:51:20,890 - nodes.VectorDBNode - INFO - ID: 456119300212324401, Text: [–ò—Å—Ç–æ—á–Ω–∏–∫: –ê–û ¬´–ú–∏–Ω–µ—Ä–∞–ª—å–Ω–æ-—Ö–∏–º–∏—á–µ—Å–∫–∞—è –∫–æ–º–ø–∞–Ω–∏—è –ï–≤—Ä–æ..., Length: 1023, Timestamp: 1739976678\n",
      "2025-02-19 17:51:20,890 - nodes.VectorDBNode - INFO - ID: 456119300212324402, Text: [–ò—Å—Ç–æ—á–Ω–∏–∫: –ù–∞—à–∏ –∞–∫—Ç–∏–≤—ã - –¥–æ–±—ã—á–∞, –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ, –ø—Ä–æ..., Length: 1229, Timestamp: 1739976678\n",
      "2025-02-19 17:51:20,891 - nodes.VectorDBNode - INFO - ID: 456119300212324403, Text: [–ò—Å—Ç–æ—á–Ω–∏–∫: –ù–∞—à–∏ –∞–∫—Ç–∏–≤—ã - –¥–æ–±—ã—á–∞, –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ, –ø—Ä–æ..., Length: 1226, Timestamp: 1739976678\n",
      "2025-02-19 17:51:20,891 - nodes.VectorDBNode - INFO - ID: 456119300212324404, Text: [–ò—Å—Ç–æ—á–Ω–∏–∫: –ù–∞—à–∏ –∞–∫—Ç–∏–≤—ã - –¥–æ–±—ã—á–∞, –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ, –ø—Ä–æ..., Length: 678, Timestamp: 1739976678\n",
      "2025-02-19 17:51:20,891 - nodes.VectorDBNode - INFO - ID: 456119300212324405, Text: [–ò—Å—Ç–æ—á–Ω–∏–∫: –ö–æ–º–ø–ª–∞–µ–Ω—Å]\n",
      "–°–≤—è–∂–∏—Ç–µ—Å—å —Å –Ω–∞–º–∏\n",
      "–£–¥–æ–±—Ä–µ–Ω–∏—è –∏..., Length: 1204, Timestamp: 1739976678\n"
     ]
    }
   ],
   "source": [
    "mk.vector_db_node.display_first_n_records(collection_db_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mk.vector_db_node.get_total_records(collection_db_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 17:59:24,082 - nodes.VectorDBNode - INFO - Connected to Milvus at localhost:19530 successfully!\n"
     ]
    }
   ],
   "source": [
    "from services.AskLLM import AskLLM\n",
    "ask = AskLLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "query= \"—á—Ç–æ —Ç–∞–∫–æ–µ YOLO\"\n",
    "message_number=0\n",
    "collection_db_name=\"data\"\n",
    "previous_messages=[]\n",
    "show_data_info=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 18:03:28,093 - nodes.EmbedderNode - INFO - –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ 1, —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–†–µ–∑—É–ª—å—Ç–∞—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞:\n",
      "Chunk 1: [–ò—Å—Ç–æ—á–Ω–∏–∫: Detect - Ultralytics YOLO Docs]\n",
      "yolo detect val model = yolo11n.pt # val official model yolo detect val model = path/to/best.pt # val custom model\n",
      "Use a trained YOLO11n model to run predict..., \n",
      "metadata={'source': 'chunk_0', 'distance': 0.853311, 'chunk_length': 1263}\n",
      "\n",
      "Chunk 2: [–ò—Å—Ç–æ—á–Ω–∏–∫: Segment - Ultralytics YOLO Docs]\n",
      "from ultralytics import YOLO # Load a model model = YOLO ( \"yolo11n-seg.yaml\" ) # build a new model from YAML model = YOLO ( \"yolo11n-seg.pt\" ) # load a pre..., \n",
      "metadata={'source': 'chunk_1', 'distance': 0.84686, 'chunk_length': 1259}\n",
      "\n",
      "Chunk 3: [–ò—Å—Ç–æ—á–Ω–∏–∫: Detect - Ultralytics YOLO Docs]\n",
      "# Build a new model from YAML and start training from scratch yolo detect train data = coco8.yaml model = yolo11n.yaml epochs = 100 imgsz = 640 # Start train..., \n",
      "metadata={'source': 'chunk_2', 'distance': 0.841616, 'chunk_length': 1234}\n",
      "\n",
      "Chunk 4: [–ò—Å—Ç–æ—á–Ω–∏–∫: Segment - Ultralytics YOLO Docs]\n",
      "from ultralytics import YOLO # Load a model model = YOLO ( \"yolo11n-seg.pt\" ) # load an official model model = YOLO ( \"path/to/best.pt\" ) # load a custom mo..., \n",
      "metadata={'source': 'chunk_3', 'distance': 0.841316, 'chunk_length': 823}\n",
      "\n",
      "Chunk 5: [–ò—Å—Ç–æ—á–Ω–∏–∫: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      "classes_names: These are the human-readable names corresponding to ..., \n",
      "metadata={'source': 'chunk_4', 'distance': 0.839344, 'chunk_length': 1362}\n",
      "\n",
      "Chunk 6: [–ò—Å—Ç–æ—á–Ω–∏–∫: Detect - Ultralytics YOLO Docs]\n",
      "YOLO11 Detect models are the default YOLO11 models, i.e.\n",
      "and are pretrained on\n",
      "YOLO11 pretrained Detect models are shown here. Detect, Segment and Pose model..., \n",
      "metadata={'source': 'chunk_5', 'distance': 0.8357, 'chunk_length': 1128}\n",
      "\n",
      "Chunk 7: [–ò—Å—Ç–æ—á–Ω–∏–∫: GitHub - Koldim2001/COCO_to_YOLOv8: Converting COCO annotation (CVAT) to annotation for YOLO-seg (instance segmentation) and YOLO-obb (oriented bounding box detection)]\n",
      "--coco_dataset TEXT ..., \n",
      "metadata={'source': 'chunk_6', 'distance': 0.834664, 'chunk_length': 1154}\n",
      "\n",
      "Chunk 8: [–ò—Å—Ç–æ—á–Ω–∏–∫: Segment - Ultralytics YOLO Docs]\n",
      "from ultralytics import YOLO # Load a model model = YOLO ( \"yolo11n-seg.pt\" ) # load an official model model = YOLO ( \"path/to/best.pt\" ) # load a custom mo..., \n",
      "metadata={'source': 'chunk_7', 'distance': 0.83358, 'chunk_length': 1237}\n",
      "\n",
      "Chunk 9: [–ò—Å—Ç–æ—á–Ω–∏–∫: GitHub - Koldim2001/COCO_to_YOLOv8: Converting COCO annotation (CVAT) to annotation for YOLO-seg (instance segmentation) and YOLO-obb (oriented bounding box detection)]\n",
      "–ö–ª—é—á–µ–≤–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ ..., \n",
      "metadata={'source': 'chunk_8', 'distance': 0.832139, 'chunk_length': 1415}\n",
      "\n",
      "Chunk 10: [–ò—Å—Ç–æ—á–Ω–∏–∫: GitHub - Koldim2001/COCO_to_YOLOv8: Converting COCO annotation (CVAT) to annotation for YOLO-seg (instance segmentation) and YOLO-obb (oriented bounding box detection)]\n",
      "How to run the code:..., \n",
      "metadata={'source': 'chunk_9', 'distance': 0.824105, 'chunk_length': 1407}\n",
      "\n",
      "Chunk 11: [–ò—Å—Ç–æ—á–Ω–∏–∫: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      "IoU threshold for non-maximum suppression YOLO of single  crop.\n",
      "Lis..., \n",
      "metadata={'source': 'chunk_10', 'distance': 0.823422, 'chunk_length': 1292}\n",
      "\n",
      "Chunk 12: [–ò—Å—Ç–æ—á–Ω–∏–∫: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      "The type of analysis to perform. Can be \"resolution_based\" for Reso..., \n",
      "metadata={'source': 'chunk_11', 'distance': 0.822136, 'chunk_length': 1082}\n",
      "\n",
      "Chunk 13: [–ò—Å—Ç–æ—á–Ω–∏–∫: Ultralytics YOLO11 Modes - Ultralytics YOLO Docs]\n",
      "# Export a YOLO model to ONNX format from the command line yolo export model = yolo11n.pt format = onnx\n",
      "Detailed steps for each export form..., \n",
      "metadata={'source': 'chunk_12', 'distance': 0.820331, 'chunk_length': 1168}\n",
      "\n",
      "Chunk 14: [–ò—Å—Ç–æ—á–Ω–∏–∫: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      "GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for ..., \n",
      "metadata={'source': 'chunk_13', 'distance': 0.819961, 'chunk_length': 1209}\n",
      "\n",
      "Chunk 15: [–ò—Å—Ç–æ—á–Ω–∏–∫: Ultralytics Solutions: Harness YOLO11 to Solve Real-World Problems - Ultralytics YOLO Docs]\n",
      "üöÄ: Estimate object speed using YOLO11 and object tracking techniques, crucial for applications li..., \n",
      "metadata={'source': 'chunk_14', 'distance': 0.819552, 'chunk_length': 1255}\n",
      "\n",
      "Chunk 16: [–ò—Å—Ç–æ—á–Ω–∏–∫: Detect - Ultralytics YOLO Docs]\n",
      "from ultralytics import YOLO # Load a pretrained model model = YOLO ( \"yolo11n.pt\" ) # Train the model on your custom dataset model . train ( data = \"my_cust..., \n",
      "metadata={'source': 'chunk_15', 'distance': 0.818875, 'chunk_length': 1260}\n",
      "\n",
      "Chunk 17: [–ò—Å—Ç–æ—á–Ω–∏–∫: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      "Whether to show bounding boxes.\n",
      "Whether to show class labels.\n",
      "Wheth..., \n",
      "metadata={'source': 'chunk_16', 'distance': 0.818564, 'chunk_length': 1197}\n",
      "\n",
      "Chunk 18: [–ò—Å—Ç–æ—á–Ω–∏–∫: Detect - Ultralytics YOLO Docs]\n",
      "from ultralytics import YOLO # Load a model model = YOLO ( \"yolo11n.pt\" ) # load an official model model = YOLO ( \"path/to/best.pt\" ) # load a custom trained..., \n",
      "metadata={'source': 'chunk_17', 'distance': 0.818067, 'chunk_length': 1109}\n",
      "\n",
      "Chunk 19: [–ò—Å—Ç–æ—á–Ω–∏–∫: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      "from patched_yolo_infer import visualize_results # Assuming result ..., \n",
      "metadata={'source': 'chunk_18', 'distance': 0.817376, 'chunk_length': 1326}\n",
      "\n",
      "Chunk 20: [–ò—Å—Ç–æ—á–Ω–∏–∫: GitHub - Koldim2001/COCO_to_YOLOv8: Converting COCO annotation (CVAT) to annotation for YOLO-seg (instance segmentation) and YOLO-obb (oriented bounding box detection)]\n",
      "GitHub - Koldim2001/..., \n",
      "metadata={'source': 'chunk_19', 'distance': 0.815978, 'chunk_length': 1418}\n",
      "\n",
      "Chunk 21: [–ò—Å—Ç–æ—á–Ω–∏–∫: Ultralytics YOLO11 Modes - Ultralytics YOLO Docs]\n",
      "mAP (mean Average Precision)\n",
      ": This evaluates the accuracy of object detection.\n",
      "IOU (Intersection over Union)\n",
      ": Measures the overlap betwee..., \n",
      "metadata={'source': 'chunk_20', 'distance': 0.815706, 'chunk_length': 1238}\n",
      "\n",
      "Chunk 22: [–ò—Å—Ç–æ—á–Ω–∏–∫: Koldim2001 (Dmitry Kolesnikov) ¬∑ GitHub]\n",
      "Koldim2001 (Dmitry Kolesnikov) ¬∑ GitHub\n",
      "Block or report Koldim2001\n",
      "Learn more about\n",
      "Add an optional note:\n",
      "Please don't include any personal informat..., \n",
      "metadata={'source': 'chunk_21', 'distance': 0.814972, 'chunk_length': 1277}\n",
      "\n",
      "Chunk 23: [–ò—Å—Ç–æ—á–Ω–∏–∫: –¶–¢–∏–ü: Machine learning-–ø–ª–∞—Ç—Ñ–æ—Ä–º–∞]\n",
      "—Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤–æ ‚Ññ –ê–û-20211230-3269820362-3 –æ—Ç 10.01.2022, \n",
      "metadata={'source': 'chunk_22', 'distance': 0.814469, 'chunk_length': 99}\n",
      "\n",
      "Chunk 24: [–ò—Å—Ç–æ—á–Ω–∏–∫: Ultralytics Solutions: Harness YOLO11 to Solve Real-World Problems - Ultralytics YOLO Docs]\n",
      "yolo solutions count show = True # for object counting yolo solutions source = \"path/to/video/fil..., \n",
      "metadata={'source': 'chunk_23', 'distance': 0.814027, 'chunk_length': 1305}\n",
      "\n",
      "Chunk 25: [–ò—Å—Ç–æ—á–Ω–∏–∫: Computer Vision Tasks supported by Ultralytics YOLO11 - Ultralytics YOLO Docs]\n",
      "Train the YOLO11 model using the detection task.\n",
      "Use the model to make predictions by feeding in new images or..., \n",
      "metadata={'source': 'chunk_24', 'distance': 0.813338, 'chunk_length': 1312}\n",
      "\n",
      "Chunk 26: [–ò—Å—Ç–æ—á–Ω–∏–∫: Segment - Ultralytics YOLO Docs]\n",
      "The output of an instance segmentation model is a set of masks or contours that outline each object in the image, along with class labels and confidence sco..., \n",
      "metadata={'source': 'chunk_25', 'distance': 0.809926, 'chunk_length': 1110}\n",
      "\n",
      "Chunk 27: [–ò—Å—Ç–æ—á–Ω–∏–∫: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      "boxes: These bounding boxes are represented as a list of lists, whe..., \n",
      "metadata={'source': 'chunk_26', 'distance': 0.809622, 'chunk_length': 1283}\n",
      "\n",
      "Chunk 28: [–ò—Å—Ç–æ—á–Ω–∏–∫: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      "Note: If CUDA support is available, it's recommended to pre-install..., \n",
      "metadata={'source': 'chunk_27', 'distance': 0.809479, 'chunk_length': 1350}\n",
      "\n",
      "Chunk 29: [–ò—Å—Ç–æ—á–Ω–∏–∫: –¶–∏—Ñ—Ä–æ–≤—ã–µ –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏ –ü–ª–∞—Ç—Ñ–æ—Ä–º—ã]\n",
      "–£–∑–Ω–∞—Ç—å –æ¬†–Ω–∞—à–∏—Ö –≤–∞–∫–∞–Ω—Å–∏—è—Ö\n",
      "–¥–ª—è –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–æ—á–∏—Ö —Å—Ñ–µ—Ä.\n",
      "–ü–æ–ª–∏—Ç–∏–∫–∏ –∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã\n",
      "–û–û–û ¬´–¶–∏—Ñ—Ä–æ–≤—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã¬ª\n",
      "—è–≤–ª—è–µ—Ç—Å—è –∞–∫–∫—Ä–µ–¥–∏—Ç–æ–≤–∞–Ω–Ω–æ–π –ò–¢-–∫–æ–º–ø–∞–Ω–∏–µ–π,..., \n",
      "metadata={'source': 'chunk_28', 'distance': 0.808979, 'chunk_length': 255}\n",
      "\n",
      "Chunk 30: [–ò—Å—Ç–æ—á–Ω–∏–∫: Ultralytics YOLO11 Modes - Ultralytics YOLO Docs]\n",
      "mode: Fine-tune your model on custom or preloaded datasets.\n",
      "mode: A post-training checkpoint to validate model performance.\n",
      "mode: Unleash t..., \n",
      "metadata={'source': 'chunk_29', 'distance': 0.80709, 'chunk_length': 1183}\n",
      "\n",
      "\n",
      "=======================\n",
      "\n",
      "–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ—Å–ª–µ —Ä–µ—Ä–∞–Ω–∫–∞:\n",
      "Chunk 1: [–ò—Å—Ç–æ—á–Ω–∏–∫: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      "The type of analysis to perform. Can be \"resolution_based\" for Reso..., \n",
      "metadata={'source': 'chunk_11', 'distance': 0.822136, 'chunk_length': 1082, 'score_rerank': 0.6130049}\n",
      "\n",
      "Chunk 2: [–ò—Å—Ç–æ—á–Ω–∏–∫: Detect - Ultralytics YOLO Docs]\n",
      "from ultralytics import YOLO # Load a pretrained model model = YOLO ( \"yolo11n.pt\" ) # Train the model on your custom dataset model . train ( data = \"my_cust..., \n",
      "metadata={'source': 'chunk_15', 'distance': 0.818875, 'chunk_length': 1260, 'score_rerank': 0.5116022}\n",
      "\n",
      "Chunk 3: [–ò—Å—Ç–æ—á–Ω–∏–∫: Segment - Ultralytics YOLO Docs]\n",
      "The output of an instance segmentation model is a set of masks or contours that outline each object in the image, along with class labels and confidence sco..., \n",
      "metadata={'source': 'chunk_25', 'distance': 0.809926, 'chunk_length': 1110, 'score_rerank': 0.50069284}\n",
      "\n",
      "Chunk 4: [–ò—Å—Ç–æ—á–Ω–∏–∫: Computer Vision Tasks supported by Ultralytics YOLO11 - Ultralytics YOLO Docs]\n",
      "Train the YOLO11 model using the detection task.\n",
      "Use the model to make predictions by feeding in new images or..., \n",
      "metadata={'source': 'chunk_24', 'distance': 0.813338, 'chunk_length': 1312, 'score_rerank': 0.48733032}\n",
      "\n",
      "Chunk 5: [–ò—Å—Ç–æ—á–Ω–∏–∫: Ultralytics Solutions: Harness YOLO11 to Solve Real-World Problems - Ultralytics YOLO Docs]\n",
      "üöÄ: Estimate object speed using YOLO11 and object tracking techniques, crucial for applications li..., \n",
      "metadata={'source': 'chunk_14', 'distance': 0.819552, 'chunk_length': 1255, 'score_rerank': 0.44628355}\n",
      "\n",
      "\n",
      "=======================\n",
      "\n",
      "–ò—Ç–æ–≥–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç, –ø–æ–¥–∞–≤–∞–µ–º—ã–π –Ω–∞ –≤—Ö–æ–¥ –º–æ–¥–µ–ª–∏:\n",
      "\n",
      "system: –í—ã –ø–æ–ª–µ–∑–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫, –∫–æ—Ç–æ—Ä—ã–π –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –í–∞—à–∏ –æ—Ç–≤–µ—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —á–µ—Ç–∫–∏–º–∏, –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º–∏ –∏ –ø–æ–ª–µ–∑–Ω—ã–º–∏.\n",
      "human: –£—á–∏—Ç—ã–≤–∞–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —ç—Ç–∏—Ö –æ—Ç—Ä—ã–≤–∫–æ–≤ —Ç–µ–∫—Å—Ç–∞ –µ—Å–ª–∏ —Å—á–∏—Ç–∞–µ—à—å –Ω—É–∂–Ω—ã–º:\n",
      "1. [–ò—Å—Ç–æ—á–Ω–∏–∫: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      "The type of analysis to perform. Can be \"resolution_based\" for Resolution-Based Analysis or \"network_based\" for Neural Network-Based Analysis.\n",
      "Pre-initialized model object for \"network_based\" mode. If not provided, the default YOLO11m model will be used.\n",
      "A list of class indices to consider for object detection in \"network_based\" mode. If None, all classes will be considered.\n",
      "The confidence threshold for detection in \"network_based\" mode.\n",
      "Example of using:\n",
      "import cv2 from ultralytics import YOLO from patched_yolo_infer import auto_calculate_crop_values # Load the image img_path = \"test_image.jpg\" img = cv2 . imread ( img_path ) # Calculate the optimal crop size and overlap for an image shape_x , shape_y , overlap_x , overlap_y = auto_calculate_crop_values ( image = img , mode = \"network_based\" , model = YOLO ( \"yolo11m.pt\" )\n",
      "An example of working with\n",
      "is presented in Google Colab notebook -\n",
      "Implementing Patching at Different Resolutions\n",
      "2. [–ò—Å—Ç–æ—á–Ω–∏–∫: Detect - Ultralytics YOLO Docs]\n",
      "from ultralytics import YOLO # Load a pretrained model model = YOLO ( \"yolo11n.pt\" ) # Train the model on your custom dataset model . train ( data = \"my_custom_dataset.yaml\" , epochs = 100 , imgsz = 640 )\n",
      "yolo detect train data = my_custom_dataset.yaml model = yolo11n.pt epochs = 100 imgsz = 640\n",
      "For detailed configuration options, visit the\n",
      "What pretrained models are available in YOLO11?\n",
      "Ultralytics YOLO11 offers various pretrained models for object detection, segmentation, and pose estimation. These models are pretrained on the COCO dataset or ImageNet for classification tasks. Here are some of the available models:\n",
      "For a detailed list and performance metrics, refer to the\n",
      "How can I validate the accuracy of my trained YOLO model?\n",
      "To validate the accuracy of your trained YOLO11 model, you can use the\n",
      "method in Python or the\n",
      "yolo detect val\n",
      "command in CLI. This will provide metrics like mAP50-95, mAP50, and more.\n",
      "from ultralytics import YOLO # Load the model model = YOLO ( \"path/to/best.pt\" ) # Validate the model metrics = model . val () print ( metrics . box . map ) # mAP50-95\n",
      "yolo detect val model = path/to/best.pt\n",
      "For more validation details, visit the\n",
      "What formats can I export a YOLO11 model to?\n",
      "3. [–ò—Å—Ç–æ—á–Ω–∏–∫: Segment - Ultralytics YOLO Docs]\n",
      "The output of an instance segmentation model is a set of masks or contours that outline each object in the image, along with class labels and confidence scores for each object. Instance segmentation is useful when you need to know not only where objects are in an image, but also what their exact shape is.\n",
      "Run Segmentation with Pre-Trained Ultralytics YOLO Model in Python.\n",
      "YOLO11 Segment models use the\n",
      "and are pretrained on\n",
      "YOLO11 pretrained Segment models are shown here. Detect, Segment and Pose models are pretrained on the\n",
      "dataset, while Classify models are pretrained on the\n",
      "download automatically from the latest Ultralytics\n",
      "on first use.\n",
      "65.9 ¬± 1.1\n",
      "1.8 ¬± 0.0\n",
      "117.6 ¬± 4.9\n",
      "2.9 ¬± 0.0\n",
      "281.6 ¬± 1.2\n",
      "6.3 ¬± 0.1\n",
      "344.2 ¬± 3.2\n",
      "7.8 ¬± 0.2\n",
      "664.5 ¬± 3.2\n",
      "15.8 ¬± 0.7\n",
      "values are for single-model single-scale on\n",
      "yolo val segment data=coco.yaml device=0\n",
      "averaged over COCO val images using an\n",
      "Amazon EC2 P4d\n",
      "yolo val segment data=coco.yaml batch=1 device=0|cpu\n",
      "Train YOLO11n-seg on the COCO8-seg dataset for 100\n",
      "at image size 640. For a full list of available arguments see the\n",
      "4. [–ò—Å—Ç–æ—á–Ω–∏–∫: Computer Vision Tasks supported by Ultralytics YOLO11 - Ultralytics YOLO Docs]\n",
      "Train the YOLO11 model using the detection task.\n",
      "Use the model to make predictions by feeding in new images or video frames.\n",
      "from ultralytics import YOLO # Load a pre-trained YOLO model (adjust model type as needed) model = YOLO ( \"yolo11n.pt\" ) # n, s, m, l, x versions available # Perform object detection on an image results = model . predict ( source = \"image.jpg\" ) # Can also use video, directory, URL, etc. # Display the results results [ 0 ] . show () # Show the first image results\n",
      "# Run YOLO detection from the command line yolo detect model = yolo11n.pt source = \"image.jpg\" # Adjust model and source as needed\n",
      "For more detailed instructions, check out our\n",
      "What are the benefits of using YOLO11 for segmentation tasks?\n",
      "Using YOLO11 for segmentation tasks provides several advantages:\n",
      "The segmentation task leverages a variant of the U-Net architecture to achieve precise segmentation.\n",
      "YOLO11 is optimized for real-time applications, offering quick processing even for high-resolution images.\n",
      "It is ideal for medical imaging, autonomous driving, and other applications requiring detailed image segmentation.\n",
      "Learn more about the benefits and use cases of YOLO11 for segmentation in the\n",
      "image segmentation section\n",
      "5. [–ò—Å—Ç–æ—á–Ω–∏–∫: Ultralytics Solutions: Harness YOLO11 to Solve Real-World Problems - Ultralytics YOLO Docs]\n",
      "üöÄ: Estimate object speed using YOLO11 and object tracking techniques, crucial for applications like autonomous vehicles and traffic monitoring.\n",
      "üöÄ: Calculate distances between objects using\n",
      "centroids in YOLO11, essential for spatial analysis.\n",
      "üöÄ: Implement efficient queue management systems to minimize wait times and improve productivity using YOLO11.\n",
      "üöÄ: Organize and direct vehicle flow in parking areas with YOLO11, optimizing space utilization and user experience.\n",
      "üìä: Conduct comprehensive data analysis to discover patterns and make informed decisions, leveraging YOLO11 for descriptive, predictive, and prescriptive analytics.\n",
      "Live Inference with Streamlit\n",
      "üöÄ: Leverage the power of YOLO11 for real-time\n",
      "directly through your web browser with a user-friendly Streamlit interface.\n",
      "Track Objects in Zone\n",
      "üéØ NEW: Learn how to track objects within specific zones of video frames using YOLO11 for precise and efficient monitoring.\n",
      "yolo SOLUTIONS SOLUTION_NAME ARGS\n",
      "is a required keyword.\n",
      "(optional) is one of:\n",
      "['count', 'heatmap', 'queue', 'speed', 'workout', 'analytics', 'trackzone']\n",
      "(optional) are custom\n",
      "pairs, such as\n",
      ", to override default settings.\n",
      "\n",
      "–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å: —á—Ç–æ —Ç–∞–∫–æ–µ YOLO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 18:03:42,876 - httpx - INFO - HTTP Request: POST http://localhost:8071/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "res = ask.process(query, message_number, collection_db_name, previous_messages, show_data_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO (You Only Look Once) ‚Äî —ç—Ç–æ —Å–µ—Ä–∏—è –º–æ–¥–µ–ª–µ–π –¥–ª—è –∑–∞–¥–∞—á –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è, —Ç–∞–∫–∏—Ö –∫–∞–∫ –æ–±—ä–µ–∫—Ç–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ, —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤, –ø–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è. YOLO11 ‚Äî —ç—Ç–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è –≤–µ—Ä—Å–∏—è –º–æ–¥–µ–ª–∏ YOLO, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è –∫–æ–º–ø–∞–Ω–∏–µ–π Ultralytics. –í–æ—Ç –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∫–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ YOLO11:\n",
      "\n",
      "1. **–û–±—ä–µ–∫—Ç–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ**: YOLO11 –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö –∏ –≤–∏–¥–µ–æ. –î–ª—è —ç—Ç–æ–≥–æ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏–ª–∏ –æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ —Å–≤–æ–∏—Ö –¥–∞–Ω–Ω—ã—Ö.\n",
      "\n",
      "2. **–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤**: YOLO11 —Ç–∞–∫–∂–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é –æ–±—ä–µ–∫—Ç–æ–≤, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –Ω–µ —Ç–æ–ª—å–∫–æ –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤, –Ω–æ –∏ –∏—Ö —Ç–æ—á–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã.\n",
      "\n",
      "3. **–ü–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤**: YOLO11 –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ–∑—ã –æ–±—ä–µ–∫—Ç–æ–≤, —á—Ç–æ –ø–æ–ª–µ–∑–Ω–æ –≤ –∑–∞–¥–∞—á–∞—Ö, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –¥–≤–∏–∂–µ–Ω–∏–µ–º –∏ –ø–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ–º.\n",
      "\n",
      "4. **–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è**: YOLO11 —Ç–∞–∫–∂–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.\n",
      "\n",
      "5. **–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞**:\n",
      "   - **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏**: YOLO11 –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –µ–≥–æ –ø–æ–¥—Ö–æ–¥—è—â–∏–º –¥–ª—è –∑–∞–¥–∞—á —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏.\n",
      "   - **–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ—Å—Ç—å**: YOLO11 –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è, –≤–∫–ª—é—á–∞—è –æ–±—ä–µ–∫—Ç–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ, —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é.\n",
      "   - **–ü—Ä–æ—Å—Ç–æ—Ç–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è**: YOLO11 –ª–µ–≥–∫–æ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç—Å—è –≤ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è, –≤–∫–ª—é—á–∞—è –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã –∏ —Ä–µ–∞–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã.\n",
      "\n",
      "6. **–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏**:\n",
      "   - **GitHub**: YOLO11 –¥–æ—Å—Ç—É–ø–µ–Ω –≤ –≤–∏–¥–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –Ω–∞ GitHub, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.\n",
      "   - **Google Colab**: –ï—Å—Ç—å –ø—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è YOLO11 –≤ Google Colab, —á—Ç–æ –æ–±–ª–µ–≥—á–∞–µ—Ç –Ω–∞—á–∞–ª—å–Ω—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ.\n",
      "   - **Streamlit**: YOLO11 –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö –≤–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ Streamlit.\n",
      "\n",
      "7. **–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è**:\n",
      "   - **–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è**: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ YOLO11 –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∑–∞–¥–∞—á, —Ç–∞–∫–∏—Ö –∫–∞–∫ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—á–µ—Ä–µ–¥—è–º–∏ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞.\n",
      "   - **–ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö**: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ YOLO11 –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –≤—ã—è–≤–ª–µ–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤.\n",
      "   - **–û–±—ä–µ–∫—Ç–Ω—ã–π —Ç—Ä–µ–∫–∏–Ω–≥**: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ YOLO11 –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ.\n",
      "\n",
      "YOLO11 –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –º–æ—â–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —à–∏—Ä–æ–∫–æ–≥–æ —Å–ø–µ–∫—Ç—Ä–∞ –∑–∞–¥–∞—á –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è, –¥–µ–ª–∞—è –µ–≥–æ –ø–æ–ø—É–ª—è—Ä–Ω—ã–º –≤—ã–±–æ—Ä–æ–º –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤ –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–µ–π.\n"
     ]
    }
   ],
   "source": [
    "print(res.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –†–∞–±–æ—Ç–∞ —Å –∏—Å—Ç–æ—Ä–∏–µ–π —á–∞—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='—á—Ç–æ —Ç–∞–∫–æ–µ YOLO', additional_kwargs={}, response_metadata={}), AIMessage(content='YOLO (You Only Look Once) ‚Äî —ç—Ç–æ —Å–µ—Ä–∏—è –º–æ–¥–µ–ª–µ–π –¥–ª—è –∑–∞–¥–∞—á –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è, —Ç–∞–∫–∏—Ö –∫–∞–∫ –æ–±—ä–µ–∫—Ç–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ, —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤, –ø–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è. YOLO11 ‚Äî —ç—Ç–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è –≤–µ—Ä—Å–∏—è –º–æ–¥–µ–ª–∏ YOLO, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è –∫–æ–º–ø–∞–Ω–∏–µ–π Ultralytics. –í–æ—Ç –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∫–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ YOLO11:\\n\\n1. **–û–±—ä–µ–∫—Ç–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ**: YOLO11 –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö –∏ –≤–∏–¥–µ–æ. –î–ª—è —ç—Ç–æ–≥–æ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏–ª–∏ –æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ —Å–≤–æ–∏—Ö –¥–∞–Ω–Ω—ã—Ö.\\n\\n2. **–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤**: YOLO11 —Ç–∞–∫–∂–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é –æ–±—ä–µ–∫—Ç–æ–≤, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –Ω–µ —Ç–æ–ª—å–∫–æ –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤, –Ω–æ –∏ –∏—Ö —Ç–æ—á–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã.\\n\\n3. **–ü–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤**: YOLO11 –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ–∑—ã –æ–±—ä–µ–∫—Ç–æ–≤, —á—Ç–æ –ø–æ–ª–µ–∑–Ω–æ –≤ –∑–∞–¥–∞—á–∞—Ö, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –¥–≤–∏–∂–µ–Ω–∏–µ–º –∏ –ø–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ–º.\\n\\n4. **–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è**: YOLO11 —Ç–∞–∫–∂–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.\\n\\n5. **–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞**:\\n   - **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏**: YOLO11 –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –µ–≥–æ –ø–æ–¥—Ö–æ–¥—è—â–∏–º –¥–ª—è –∑–∞–¥–∞—á —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏.\\n   - **–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ—Å—Ç—å**: YOLO11 –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è, –≤–∫–ª—é—á–∞—è –æ–±—ä–µ–∫—Ç–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ, —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é.\\n   - **–ü—Ä–æ—Å—Ç–æ—Ç–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è**: YOLO11 –ª–µ–≥–∫–æ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç—Å—è –≤ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è, –≤–∫–ª—é—á–∞—è –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã –∏ —Ä–µ–∞–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã.\\n\\n6. **–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏**:\\n   - **GitHub**: YOLO11 –¥–æ—Å—Ç—É–ø–µ–Ω –≤ –≤–∏–¥–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –Ω–∞ GitHub, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.\\n   - **Google Colab**: –ï—Å—Ç—å –ø—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è YOLO11 –≤ Google Colab, —á—Ç–æ –æ–±–ª–µ–≥—á–∞–µ—Ç –Ω–∞—á–∞–ª—å–Ω—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ.\\n   - **Streamlit**: YOLO11 –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö –≤–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ Streamlit.\\n\\n7. **–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è**:\\n   - **–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è**: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ YOLO11 –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∑–∞–¥–∞—á, —Ç–∞–∫–∏—Ö –∫–∞–∫ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—á–µ—Ä–µ–¥—è–º–∏ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞.\\n   - **–ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö**: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ YOLO11 –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –≤—ã—è–≤–ª–µ–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤.\\n   - **–û–±—ä–µ–∫—Ç–Ω—ã–π —Ç—Ä–µ–∫–∏–Ω–≥**: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ YOLO11 –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ.\\n\\nYOLO11 –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –º–æ—â–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —à–∏—Ä–æ–∫–æ–≥–æ —Å–ø–µ–∫—Ç—Ä–∞ –∑–∞–¥–∞—á –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è, –¥–µ–ª–∞—è –µ–≥–æ –ø–æ–ø—É–ª—è—Ä–Ω—ã–º –≤—ã–±–æ—Ä–æ–º –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤ –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–µ–π.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "previous_messages = ask.get_new_history(res)\n",
    "print(previous_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "query= \"–ø—Ä–∏–º–µ—Ä –æ–±—É—á–µ–Ω–∏—è —Ö–æ—á—É\"\n",
    "message_number=1\n",
    "collection_db_name=\"data\"\n",
    "show_data_info=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 18:08:32,580 - httpx - INFO - HTTP Request: POST http://localhost:8071/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-19 18:08:32,581 - nodes.LLMNode - INFO - –ò–∑–º–µ–Ω–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–µ–¥–¥–∏–Ω–≥–æ–≤: –•–æ—Ç–∏—Ç–µ –ø—Ä–∏–º–µ—Ä—ã —Ç–æ–≥–æ, –∫–∞–∫ –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å YOLO11?\n",
      "2025-02-19 18:08:32,585 - nodes.EmbedderNode - INFO - –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ 1, —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–†–µ–∑—É–ª—å—Ç–∞—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞:\n",
      "Chunk 1: [–ò—Å—Ç–æ—á–Ω–∏–∫: Detect - Ultralytics YOLO Docs]\n",
      "yolo detect val model = yolo11n.pt # val official model yolo detect val model = path/to/best.pt # val custom model\n",
      "Use a trained YOLO11n model to run predict..., \n",
      "metadata={'source': 'chunk_0', 'distance': 0.881229, 'chunk_length': 1263}\n",
      "\n",
      "Chunk 2: [–ò—Å—Ç–æ—á–Ω–∏–∫: Segment - Ultralytics YOLO Docs]\n",
      "from ultralytics import YOLO # Load a model model = YOLO ( \"yolo11n-seg.pt\" ) # load an official model model = YOLO ( \"path/to/best.pt\" ) # load a custom mo..., \n",
      "metadata={'source': 'chunk_1', 'distance': 0.875638, 'chunk_length': 823}\n",
      "\n",
      "Chunk 3: [–ò—Å—Ç–æ—á–Ω–∏–∫: Segment - Ultralytics YOLO Docs]\n",
      "from ultralytics import YOLO # Load a model model = YOLO ( \"yolo11n-seg.yaml\" ) # build a new model from YAML model = YOLO ( \"yolo11n-seg.pt\" ) # load a pre..., \n",
      "metadata={'source': 'chunk_2', 'distance': 0.87533, 'chunk_length': 1259}\n",
      "\n",
      "Chunk 4: [–ò—Å—Ç–æ—á–Ω–∏–∫: Detect - Ultralytics YOLO Docs]\n",
      "YOLO11 Detect models are the default YOLO11 models, i.e.\n",
      "and are pretrained on\n",
      "YOLO11 pretrained Detect models are shown here. Detect, Segment and Pose model..., \n",
      "metadata={'source': 'chunk_3', 'distance': 0.87466, 'chunk_length': 1128}\n",
      "\n",
      "Chunk 5: [–ò—Å—Ç–æ—á–Ω–∏–∫: Detect - Ultralytics YOLO Docs]\n",
      "# Build a new model from YAML and start training from scratch yolo detect train data = coco8.yaml model = yolo11n.yaml epochs = 100 imgsz = 640 # Start train..., \n",
      "metadata={'source': 'chunk_4', 'distance': 0.873785, 'chunk_length': 1234}\n",
      "\n",
      "Chunk 6: [–ò—Å—Ç–æ—á–Ω–∏–∫: Detect - Ultralytics YOLO Docs]\n",
      "from ultralytics import YOLO # Load a model model = YOLO ( \"yolo11n.pt\" ) # load an official model model = YOLO ( \"path/to/best.pt\" ) # load a custom trained..., \n",
      "metadata={'source': 'chunk_5', 'distance': 0.872552, 'chunk_length': 1109}\n",
      "\n",
      "Chunk 7: [–ò—Å—Ç–æ—á–Ω–∏–∫: Segment - Ultralytics YOLO Docs]\n",
      "from ultralytics import YOLO # Load a model model = YOLO ( \"yolo11n-seg.pt\" ) # load an official model model = YOLO ( \"path/to/best.pt\" ) # load a custom mo..., \n",
      "metadata={'source': 'chunk_6', 'distance': 0.871842, 'chunk_length': 1237}\n",
      "\n",
      "Chunk 8: [–ò—Å—Ç–æ—á–Ω–∏–∫: Ultralytics YOLO11 Modes - Ultralytics YOLO Docs]\n",
      "# Export a YOLO model to ONNX format from the command line yolo export model = yolo11n.pt format = onnx\n",
      "Detailed steps for each export form..., \n",
      "metadata={'source': 'chunk_7', 'distance': 0.864856, 'chunk_length': 1168}\n",
      "\n",
      "Chunk 9: [–ò—Å—Ç–æ—á–Ω–∏–∫: Detect - Ultralytics YOLO Docs]\n",
      "from ultralytics import YOLO # Load a pretrained model model = YOLO ( \"yolo11n.pt\" ) # Train the model on your custom dataset model . train ( data = \"my_cust..., \n",
      "metadata={'source': 'chunk_8', 'distance': 0.859553, 'chunk_length': 1260}\n",
      "\n",
      "Chunk 10: [–ò—Å—Ç–æ—á–Ω–∏–∫: Computer Vision Tasks supported by Ultralytics YOLO11 - Ultralytics YOLO Docs]\n",
      "Train the YOLO11 model using the detection task.\n",
      "Use the model to make predictions by feeding in new images or..., \n",
      "metadata={'source': 'chunk_9', 'distance': 0.856096, 'chunk_length': 1312}\n",
      "\n",
      "Chunk 11: [–ò—Å—Ç–æ—á–Ω–∏–∫: Segment - Ultralytics YOLO Docs]\n",
      "Available YOLO11-seg export formats are in the table below. You can export to any format using the\n",
      ". You can predict or validate directly on exported models..., \n",
      "metadata={'source': 'chunk_10', 'distance': 0.853887, 'chunk_length': 1091}\n",
      "\n",
      "Chunk 12: [–ò—Å—Ç–æ—á–Ω–∏–∫: Ultralytics YOLO11 Modes - Ultralytics YOLO Docs]\n",
      "mode: Fine-tune your model on custom or preloaded datasets.\n",
      "mode: A post-training checkpoint to validate model performance.\n",
      "mode: Unleash t..., \n",
      "metadata={'source': 'chunk_11', 'distance': 0.853881, 'chunk_length': 1183}\n",
      "\n",
      "Chunk 13: [–ò—Å—Ç–æ—á–Ω–∏–∫: Ultralytics YOLO11 Modes - Ultralytics YOLO Docs]\n",
      "mAP (mean Average Precision)\n",
      ": This evaluates the accuracy of object detection.\n",
      "IOU (Intersection over Union)\n",
      ": Measures the overlap betwee..., \n",
      "metadata={'source': 'chunk_12', 'distance': 0.853613, 'chunk_length': 1238}\n",
      "\n",
      "Chunk 14: [–ò—Å—Ç–æ—á–Ω–∏–∫: Ultralytics Solutions: Harness YOLO11 to Solve Real-World Problems - Ultralytics YOLO Docs]\n",
      "üöÄ: Estimate object speed using YOLO11 and object tracking techniques, crucial for applications li..., \n",
      "metadata={'source': 'chunk_13', 'distance': 0.8534, 'chunk_length': 1255}\n",
      "\n",
      "Chunk 15: [–ò—Å—Ç–æ—á–Ω–∏–∫: Ultralytics YOLO11 Modes - Ultralytics YOLO Docs]\n",
      "Predict mode is used for making predictions using a trained YOLO11 model on new images or videos. In this mode, the model is loaded from a ..., \n",
      "metadata={'source': 'chunk_14', 'distance': 0.853058, 'chunk_length': 1184}\n",
      "\n",
      "Chunk 16: [–ò—Å—Ç–æ—á–Ω–∏–∫: Ultralytics YOLO11 Modes - Ultralytics YOLO Docs]\n",
      "metrics (for classification), and the inference time in milliseconds per image across various formats like ONNX, OpenVINO, TensorRT, and ot..., \n",
      "metadata={'source': 'chunk_15', 'distance': 0.849684, 'chunk_length': 1284}\n",
      "\n",
      "Chunk 17: [–ò—Å—Ç–æ—á–Ω–∏–∫: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      "classes_names: These are the human-readable names corresponding to ..., \n",
      "metadata={'source': 'chunk_16', 'distance': 0.849659, 'chunk_length': 1362}\n",
      "\n",
      "Chunk 18: [–ò—Å—Ç–æ—á–Ω–∏–∫: Segment - Ultralytics YOLO Docs]\n",
      "Object detection identifies and localizes objects within an image by drawing bounding boxes around them, whereas instance segmentation not only identifies t..., \n",
      "metadata={'source': 'chunk_17', 'distance': 0.847323, 'chunk_length': 1196}\n",
      "\n",
      "Chunk 19: [–ò—Å—Ç–æ—á–Ω–∏–∫: GitHub - Koldim2001/COCO_to_YOLOv8: Converting COCO annotation (CVAT) to annotation for YOLO-seg (instance segmentation) and YOLO-obb (oriented bounding box detection)]\n",
      "–ö–ª—é—á–µ–≤–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ ..., \n",
      "metadata={'source': 'chunk_18', 'distance': 0.845389, 'chunk_length': 1415}\n",
      "\n",
      "Chunk 20: [–ò—Å—Ç–æ—á–Ω–∏–∫: GitHub - Koldim2001/COCO_to_YOLOv8: Converting COCO annotation (CVAT) to annotation for YOLO-seg (instance segmentation) and YOLO-obb (oriented bounding box detection)]\n",
      "--coco_dataset TEXT ..., \n",
      "metadata={'source': 'chunk_19', 'distance': 0.842674, 'chunk_length': 1154}\n",
      "\n",
      "Chunk 21: [–ò—Å—Ç–æ—á–Ω–∏–∫: Segment - Ultralytics YOLO Docs]\n",
      "The output of an instance segmentation model is a set of masks or contours that outline each object in the image, along with class labels and confidence sco..., \n",
      "metadata={'source': 'chunk_20', 'distance': 0.841872, 'chunk_length': 1110}\n",
      "\n",
      "Chunk 22: [–ò—Å—Ç–æ—á–Ω–∏–∫: GitHub - Koldim2001/COCO_to_YOLOv8: Converting COCO annotation (CVAT) to annotation for YOLO-seg (instance segmentation) and YOLO-obb (oriented bounding box detection)]\n",
      "How to run the code:..., \n",
      "metadata={'source': 'chunk_21', 'distance': 0.84125, 'chunk_length': 1407}\n",
      "\n",
      "Chunk 23: [–ò—Å—Ç–æ—á–Ω–∏–∫: Koldim2001 (Dmitry Kolesnikov) ¬∑ GitHub]\n",
      "Koldim2001 (Dmitry Kolesnikov) ¬∑ GitHub\n",
      "Block or report Koldim2001\n",
      "Learn more about\n",
      "Add an optional note:\n",
      "Please don't include any personal informat..., \n",
      "metadata={'source': 'chunk_22', 'distance': 0.841177, 'chunk_length': 1277}\n",
      "\n",
      "Chunk 24: [–ò—Å—Ç–æ—á–Ω–∏–∫: Ultralytics Solutions: Harness YOLO11 to Solve Real-World Problems - Ultralytics YOLO Docs]\n",
      "yolo solutions count show = True # for object counting yolo solutions source = \"path/to/video/fil..., \n",
      "metadata={'source': 'chunk_23', 'distance': 0.840756, 'chunk_length': 1305}\n",
      "\n",
      "Chunk 25: [–ò—Å—Ç–æ—á–Ω–∏–∫: Detect - Ultralytics YOLO Docs]\n",
      "Ultralytics YOLO11 allows exporting models to various formats such as ONNX, TensorRT, CoreML, and more to ensure compatibility across different platforms and..., \n",
      "metadata={'source': 'chunk_24', 'distance': 0.840549, 'chunk_length': 1272}\n",
      "\n",
      "Chunk 26: [–ò—Å—Ç–æ—á–Ω–∏–∫: Ultralytics Solutions: Harness YOLO11 to Solve Real-World Problems - Ultralytics YOLO Docs]\n",
      "Ultralytics YOLO11 can be used for real-time object counting by leveraging its advanced object de..., \n",
      "metadata={'source': 'chunk_25', 'distance': 0.838589, 'chunk_length': 1213}\n",
      "\n",
      "Chunk 27: [–ò—Å—Ç–æ—á–Ω–∏–∫: Ultralytics Solutions: Harness YOLO11 to Solve Real-World Problems - Ultralytics YOLO Docs]\n",
      "Here's our curated list of Ultralytics solutions that can be used to create awesome\n",
      "üöÄ: Learn to p..., \n",
      "metadata={'source': 'chunk_26', 'distance': 0.837556, 'chunk_length': 1347}\n",
      "\n",
      "Chunk 28: [–ò—Å—Ç–æ—á–Ω–∏–∫: GitHub - Koldim2001/COCO_to_YOLOv8: Converting COCO annotation (CVAT) to annotation for YOLO-seg (instance segmentation) and YOLO-obb (oriented bounding box detection)]\n",
      "GitHub - Koldim2001/..., \n",
      "metadata={'source': 'chunk_27', 'distance': 0.837517, 'chunk_length': 1418}\n",
      "\n",
      "Chunk 29: [–ò—Å—Ç–æ—á–Ω–∏–∫: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      "from patched_yolo_infer import visualize_results # Assuming result ..., \n",
      "metadata={'source': 'chunk_28', 'distance': 0.835765, 'chunk_length': 1326}\n",
      "\n",
      "Chunk 30: [–ò—Å—Ç–æ—á–Ω–∏–∫: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      "The type of analysis to perform. Can be \"resolution_based\" for Reso..., \n",
      "metadata={'source': 'chunk_29', 'distance': 0.835657, 'chunk_length': 1082}\n",
      "\n",
      "\n",
      "=======================\n",
      "\n",
      "–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ—Å–ª–µ —Ä–µ—Ä–∞–Ω–∫–∞:\n",
      "Chunk 1: [–ò—Å—Ç–æ—á–Ω–∏–∫: Detect - Ultralytics YOLO Docs]\n",
      "from ultralytics import YOLO # Load a model model = YOLO ( \"yolo11n.pt\" ) # load an official model model = YOLO ( \"path/to/best.pt\" ) # load a custom trained..., \n",
      "metadata={'source': 'chunk_5', 'distance': 0.872552, 'chunk_length': 1109, 'score_rerank': 0.93991333}\n",
      "\n",
      "Chunk 2: [–ò—Å—Ç–æ—á–Ω–∏–∫: Ultralytics YOLO11 Modes - Ultralytics YOLO Docs]\n",
      "metrics (for classification), and the inference time in milliseconds per image across various formats like ONNX, OpenVINO, TensorRT, and ot..., \n",
      "metadata={'source': 'chunk_15', 'distance': 0.849684, 'chunk_length': 1284, 'score_rerank': 0.8206132}\n",
      "\n",
      "Chunk 3: [–ò—Å—Ç–æ—á–Ω–∏–∫: Segment - Ultralytics YOLO Docs]\n",
      "Available YOLO11-seg export formats are in the table below. You can export to any format using the\n",
      ". You can predict or validate directly on exported models..., \n",
      "metadata={'source': 'chunk_10', 'distance': 0.853887, 'chunk_length': 1091, 'score_rerank': 0.7874412}\n",
      "\n",
      "Chunk 4: [–ò—Å—Ç–æ—á–Ω–∏–∫: Computer Vision Tasks supported by Ultralytics YOLO11 - Ultralytics YOLO Docs]\n",
      "Train the YOLO11 model using the detection task.\n",
      "Use the model to make predictions by feeding in new images or..., \n",
      "metadata={'source': 'chunk_9', 'distance': 0.856096, 'chunk_length': 1312, 'score_rerank': 0.6796041}\n",
      "\n",
      "Chunk 5: [–ò—Å—Ç–æ—á–Ω–∏–∫: Segment - Ultralytics YOLO Docs]\n",
      "Object detection identifies and localizes objects within an image by drawing bounding boxes around them, whereas instance segmentation not only identifies t..., \n",
      "metadata={'source': 'chunk_17', 'distance': 0.847323, 'chunk_length': 1196, 'score_rerank': 0.6410854}\n",
      "\n",
      "\n",
      "=======================\n",
      "\n",
      "–ò—Ç–æ–≥–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç, –ø–æ–¥–∞–≤–∞–µ–º—ã–π –Ω–∞ –≤—Ö–æ–¥ –º–æ–¥–µ–ª–∏:\n",
      "\n",
      "system: –í—ã –ø–æ–ª–µ–∑–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫, –∫–æ—Ç–æ—Ä—ã–π –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –í–∞—à–∏ –æ—Ç–≤–µ—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —á–µ—Ç–∫–∏–º–∏, –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º–∏ –∏ –ø–æ–ª–µ–∑–Ω—ã–º–∏.\n",
      "human: —á—Ç–æ —Ç–∞–∫–æ–µ YOLO\n",
      "ai: YOLO (You Only Look Once) ‚Äî —ç—Ç–æ —Å–µ—Ä–∏—è –º–æ–¥–µ–ª–µ–π –¥–ª—è –∑–∞–¥–∞—á –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è, —Ç–∞–∫–∏—Ö –∫–∞–∫ –æ–±—ä–µ–∫—Ç–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ, —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤, –ø–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è. YOLO11 ‚Äî —ç—Ç–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è –≤–µ—Ä—Å–∏—è –º–æ–¥–µ–ª–∏ YOLO, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è –∫–æ–º–ø–∞–Ω–∏–µ–π Ultralytics. –í–æ—Ç –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∫–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ YOLO11:\n",
      "\n",
      "1. **–û–±—ä–µ–∫—Ç–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ**: YOLO11 –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö –∏ –≤–∏–¥–µ–æ. –î–ª—è —ç—Ç–æ–≥–æ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏–ª–∏ –æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ —Å–≤–æ–∏—Ö –¥–∞–Ω–Ω—ã—Ö.\n",
      "\n",
      "2. **–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤**: YOLO11 —Ç–∞–∫–∂–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é –æ–±—ä–µ–∫—Ç–æ–≤, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –Ω–µ —Ç–æ–ª—å–∫–æ –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤, –Ω–æ –∏ –∏—Ö —Ç–æ—á–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã.\n",
      "\n",
      "3. **–ü–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤**: YOLO11 –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ–∑—ã –æ–±—ä–µ–∫—Ç–æ–≤, —á—Ç–æ –ø–æ–ª–µ–∑–Ω–æ –≤ –∑–∞–¥–∞—á–∞—Ö, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –¥–≤–∏–∂–µ–Ω–∏–µ–º –∏ –ø–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ–º.\n",
      "\n",
      "4. **–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è**: YOLO11 —Ç–∞–∫–∂–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.\n",
      "\n",
      "5. **–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞**:\n",
      "   - **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏**: YOLO11 –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –µ–≥–æ –ø–æ–¥—Ö–æ–¥—è—â–∏–º –¥–ª—è –∑–∞–¥–∞—á —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏.\n",
      "   - **–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ—Å—Ç—å**: YOLO11 –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è, –≤–∫–ª—é—á–∞—è –æ–±—ä–µ–∫—Ç–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ, —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é.\n",
      "   - **–ü—Ä–æ—Å—Ç–æ—Ç–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è**: YOLO11 –ª–µ–≥–∫–æ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç—Å—è –≤ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è, –≤–∫–ª—é—á–∞—è –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã –∏ —Ä–µ–∞–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã.\n",
      "\n",
      "6. **–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏**:\n",
      "   - **GitHub**: YOLO11 –¥–æ—Å—Ç—É–ø–µ–Ω –≤ –≤–∏–¥–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –Ω–∞ GitHub, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.\n",
      "   - **Google Colab**: –ï—Å—Ç—å –ø—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è YOLO11 –≤ Google Colab, —á—Ç–æ –æ–±–ª–µ–≥—á–∞–µ—Ç –Ω–∞—á–∞–ª—å–Ω—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ.\n",
      "   - **Streamlit**: YOLO11 –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö –≤–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ Streamlit.\n",
      "\n",
      "7. **–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è**:\n",
      "   - **–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è**: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ YOLO11 –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∑–∞–¥–∞—á, —Ç–∞–∫–∏—Ö –∫–∞–∫ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—á–µ—Ä–µ–¥—è–º–∏ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞.\n",
      "   - **–ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö**: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ YOLO11 –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –≤—ã—è–≤–ª–µ–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤.\n",
      "   - **–û–±—ä–µ–∫—Ç–Ω—ã–π —Ç—Ä–µ–∫–∏–Ω–≥**: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ YOLO11 –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ.\n",
      "\n",
      "YOLO11 –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –º–æ—â–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —à–∏—Ä–æ–∫–æ–≥–æ —Å–ø–µ–∫—Ç—Ä–∞ –∑–∞–¥–∞—á –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è, –¥–µ–ª–∞—è –µ–≥–æ –ø–æ–ø—É–ª—è—Ä–Ω—ã–º –≤—ã–±–æ—Ä–æ–º –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤ –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–µ–π.\n",
      "human: –£—á–∏—Ç—ã–≤–∞–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —ç—Ç–∏—Ö –æ—Ç—Ä—ã–≤–∫–æ–≤ —Ç–µ–∫—Å—Ç–∞ –µ—Å–ª–∏ —Å—á–∏—Ç–∞–µ—à—å –Ω—É–∂–Ω—ã–º:\n",
      "1. [–ò—Å—Ç–æ—á–Ω–∏–∫: Detect - Ultralytics YOLO Docs]\n",
      "from ultralytics import YOLO # Load a model model = YOLO ( \"yolo11n.pt\" ) # load an official model model = YOLO ( \"path/to/best.pt\" ) # load a custom trained model # Export the model model . export ( format = \"onnx\" )\n",
      "yolo export model = yolo11n.pt format = onnx # export official model yolo export model = path/to/best.pt format = onnx # export custom trained model\n",
      "Available YOLO11 export formats are in the table below. You can export to any format using the\n",
      ". You can predict or validate directly on exported models, i.e.\n",
      "yolo predict model=yolo11n.onnx\n",
      ". Usage examples are shown for your model after export completes.\n",
      "TF Edge TPU\n",
      "details in the\n",
      "How do I train a YOLO11 model on my custom dataset?\n",
      "Training a YOLO11 model on a custom dataset involves a few steps:\n",
      "Prepare the Dataset\n",
      ": Ensure your dataset is in the YOLO format. For guidance, refer to our\n",
      "Load the Model\n",
      ": Use the Ultralytics YOLO library to load a pre-trained model or create a new model from a YAML file.\n",
      "Train the Model\n",
      ": Execute the\n",
      "method in Python or the\n",
      "yolo detect train\n",
      "command in CLI.\n",
      "2. [–ò—Å—Ç–æ—á–Ω–∏–∫: Ultralytics YOLO11 Modes - Ultralytics YOLO Docs]\n",
      "metrics (for classification), and the inference time in milliseconds per image across various formats like ONNX, OpenVINO, TensorRT, and others. This information can help users choose the optimal export format for their specific use case based on their requirements for speed and accuracy.\n",
      "How do I train a custom\n",
      "model with Ultralytics YOLO11?\n",
      "Training a custom object detection model with Ultralytics YOLO11 involves using the train mode. You need a dataset formatted in YOLO format, containing images and corresponding annotation files. Use the following command to start the training process:\n",
      "from ultralytics import YOLO # Load a pre-trained YOLO model (you can choose n, s, m, l, or x versions) model = YOLO ( \"yolo11n.pt\" ) # Start training on your custom dataset model . train ( data = \"path/to/dataset.yaml\" , epochs = 100 , imgsz = 640 )\n",
      "# Train a YOLO model from the command line yolo train data = path/to/dataset.yaml epochs = 100 imgsz = 640\n",
      "For more detailed instructions, you can refer to the\n",
      "Ultralytics Train Guide\n",
      "What metrics does Ultralytics YOLO11 use to validate the model's performance?\n",
      "Ultralytics YOLO11 uses various metrics during the validation process to assess model performance. These include:\n",
      "3. [–ò—Å—Ç–æ—á–Ω–∏–∫: Segment - Ultralytics YOLO Docs]\n",
      "Available YOLO11-seg export formats are in the table below. You can export to any format using the\n",
      ". You can predict or validate directly on exported models, i.e.\n",
      "yolo predict model=yolo11n-seg.onnx\n",
      ". Usage examples are shown for your model after export completes.\n",
      "TF Edge TPU\n",
      "details in the\n",
      "How do I train a YOLO11 segmentation model on a custom dataset?\n",
      "To train a YOLO11 segmentation model on a custom dataset, you first need to prepare your dataset in the YOLO segmentation format. You can use tools like\n",
      "to convert datasets from other formats. Once your dataset is ready, you can train the model using Python or CLI commands:\n",
      "from ultralytics import YOLO # Load a pretrained YOLO11 segment model model = YOLO ( \"yolo11n-seg.pt\" ) # Train the model results = model . train ( data = \"path/to/your_dataset.yaml\" , epochs = 100 , imgsz = 640 )\n",
      "yolo segment train data = path/to/your_dataset.yaml model = yolo11n-seg.pt epochs = 100 imgsz = 640\n",
      "page for more available arguments.\n",
      "What is the difference between\n",
      "and instance segmentation in YOLO11?\n",
      "4. [–ò—Å—Ç–æ—á–Ω–∏–∫: Computer Vision Tasks supported by Ultralytics YOLO11 - Ultralytics YOLO Docs]\n",
      "Train the YOLO11 model using the detection task.\n",
      "Use the model to make predictions by feeding in new images or video frames.\n",
      "from ultralytics import YOLO # Load a pre-trained YOLO model (adjust model type as needed) model = YOLO ( \"yolo11n.pt\" ) # n, s, m, l, x versions available # Perform object detection on an image results = model . predict ( source = \"image.jpg\" ) # Can also use video, directory, URL, etc. # Display the results results [ 0 ] . show () # Show the first image results\n",
      "# Run YOLO detection from the command line yolo detect model = yolo11n.pt source = \"image.jpg\" # Adjust model and source as needed\n",
      "For more detailed instructions, check out our\n",
      "What are the benefits of using YOLO11 for segmentation tasks?\n",
      "Using YOLO11 for segmentation tasks provides several advantages:\n",
      "The segmentation task leverages a variant of the U-Net architecture to achieve precise segmentation.\n",
      "YOLO11 is optimized for real-time applications, offering quick processing even for high-resolution images.\n",
      "It is ideal for medical imaging, autonomous driving, and other applications requiring detailed image segmentation.\n",
      "Learn more about the benefits and use cases of YOLO11 for segmentation in the\n",
      "image segmentation section\n",
      "5. [–ò—Å—Ç–æ—á–Ω–∏–∫: Segment - Ultralytics YOLO Docs]\n",
      "Object detection identifies and localizes objects within an image by drawing bounding boxes around them, whereas instance segmentation not only identifies the bounding boxes but also delineates the exact shape of each object. YOLO11 instance segmentation models provide masks or contours that outline each detected object, which is particularly useful for tasks where knowing the precise shape of objects is important, such as medical imaging or autonomous driving.\n",
      "Why use YOLO11 for instance segmentation?\n",
      "Ultralytics YOLO11 is a state-of-the-art model recognized for its high accuracy and real-time performance, making it ideal for instance segmentation tasks. YOLO11 Segment models come pretrained on the\n",
      ", ensuring robust performance across a variety of objects. Additionally, YOLO supports training, validation, prediction, and export functionalities with seamless integration, making it highly versatile for both research and industry applications.\n",
      "How do I load and validate a pretrained YOLO segmentation model?\n",
      "Loading and validating a pretrained YOLO segmentation model is straightforward. Here's how you can do it using both Python and CLI:\n",
      "\n",
      "–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å: –ø—Ä–∏–º–µ—Ä –æ–±—É—á–µ–Ω–∏—è —Ö–æ—á—É\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 18:08:46,358 - httpx - INFO - HTTP Request: POST http://localhost:8071/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "res = ask.process(query, message_number, collection_db_name, previous_messages, show_data_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO11 (You Only Look Once) ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—ä–µ–∫—Ç–Ω–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è –∫–æ–º–ø–∞–Ω–∏–µ–π Ultralytics, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –±—ã—Å—Ç—Ä–æ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –≤–∏–¥–µ–æ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤.\n"
     ]
    }
   ],
   "source": [
    "print(res.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'–•–æ—Ç–∏—Ç–µ –ø—Ä–∏–º–µ—Ä—ã —Ç–æ–≥–æ, –∫–∞–∫ –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å YOLO11?'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.upgraded_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'–ø—Ä–∏–º–µ—Ä –æ–±—É—á–µ–Ω–∏—è —Ö–æ—á—É'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.query"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MAIN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
