{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing + Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Парсинг веб-страниц:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def parse_url(url):\n",
    "    \"\"\"\n",
    "    Парсит содержимое указанного URL и возвращает текстовое содержимое страницы.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL-адрес для парсинга.\n",
    "\n",
    "    Returns:\n",
    "        dict: Словарь с ключами 'text' (текстовая информация) и 'description' (краткое описание страницы).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Извлекаем краткое описание (title страницы)\n",
    "        description = soup.title.string.strip() if soup.title else url\n",
    "\n",
    "        # Обработка контента\n",
    "        soup_copy = BeautifulSoup(response.text, \"html.parser\")\n",
    "        for tag in soup_copy.find_all(['pre', 'code']):\n",
    "            tag.replace_with(tag.get_text(separator=\" \", strip=True))\n",
    "        final_text = soup_copy.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "        return {'text': final_text, 'description': description}\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Ошибка при загрузке страницы {url}: {e}\")\n",
    "        return {'text': '', 'description': url}\n",
    "\n",
    "\n",
    "def filter_text(text, min_words=3):\n",
    "    \"\"\"\n",
    "    Фильтрует текст, удаляя строки с минимальным количеством слов и стоп-фразы.\n",
    "\n",
    "    Args:\n",
    "        text (str): Исходный текст.\n",
    "        min_words (int): Минимальное количество слов в строке для сохранения.\n",
    "\n",
    "    Returns:\n",
    "        str: Отфильтрованный текст.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return ''\n",
    "\n",
    "    lines = text.split(\"\\n\")\n",
    "    filtered_text = [line.strip() for line in lines if len(line.split()) > min_words]\n",
    "\n",
    "    # Поиск стоп-фраз\n",
    "    stop_phrases = [\n",
    "        \"Политика в отношении файлов cookie\",\n",
    "        \"Мы используем cookie\",\n",
    "        \"Дата обращения:\",\n",
    "        \"Использованная литература и источники:\"\n",
    "    ]\n",
    "    cutoff_index = next((i for i, line in enumerate(filtered_text) if any(phrase in line for phrase in stop_phrases)), None)\n",
    "\n",
    "    if cutoff_index is not None:\n",
    "        filtered_text = filtered_text[:cutoff_index]\n",
    "\n",
    "    return \"\\n\".join(filtered_text)\n",
    "\n",
    "\n",
    "def save_to_file(filename, content):\n",
    "    \"\"\"\n",
    "    Сохраняет текстовое содержимое в указанный файл.\n",
    "\n",
    "    Args:\n",
    "        filename (str): Имя файла для сохранения.\n",
    "        content (str): Текстовое содержимое для записи.\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(content)\n",
    "\n",
    "\n",
    "def load_from_file(filename):\n",
    "    \"\"\"\n",
    "    Загружает текстовое содержимое из указанного файла.\n",
    "\n",
    "    Args:\n",
    "        filename (str): Имя файла для чтения.\n",
    "\n",
    "    Returns:\n",
    "        str: Текстовое содержимое файла.\n",
    "    \"\"\"\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "        return file.read()\n",
    "\n",
    "\n",
    "def generate_chunks(loaded_text, url_data, chunk_size=1500, chunk_overlap=0):\n",
    "    \"\"\"\n",
    "    Генерирует чанки из загруженного текста, добавляя описание источника в начало каждого чанка.\n",
    "\n",
    "    Args:\n",
    "        loaded_text (str): Загруженный текст из файла.\n",
    "        url_data (dict): Словарь с описаниями страниц.\n",
    "        chunk_size (int): Максимальный размер чанка.\n",
    "        chunk_overlap (int): Перекрытие между чанками.\n",
    "\n",
    "    Returns:\n",
    "        list: Список отформатированных чанков.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \".\"],\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "\n",
    "    all_chunks = []\n",
    "    chunks = text_splitter.split_text(loaded_text)\n",
    "\n",
    "    # Распределение чанков по источникам (простая эвристика)\n",
    "    source_descriptions = list(url_data.values())\n",
    "\n",
    "    for i, text in enumerate(loaded_text.split('\\n\\n\\n')):\n",
    "        chunks = text_splitter.split_text(text)\n",
    "        for chunk in chunks:\n",
    "            source_description = source_descriptions[i]['description']\n",
    "            formatted_chunk = f\"[Источник: {source_description}]\\n{chunk}\"\n",
    "            all_chunks.append(formatted_chunk)\n",
    "\n",
    "    return all_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Парсинг https://www.eurochem.ru/...\n",
      "Парсинг https://www.eurochem.ru/global-operations/...\n",
      "Парсинг https://www.eurochem.ru/about-us/komplaens/...\n",
      "Парсинг https://www.eurochem.ru/proteh-lab/...\n",
      "Парсинг https://digtp.com/...\n",
      "Парсинг https://digtp.com/projects/machine-learning-platforma...\n",
      "Парсинг https://digtp.com/projects/rekomendatelnye-modeli...\n",
      "Парсинг https://digtp.com/projects/mobilnoe-prilozenie-mineralogiia...\n",
      "Парсинг https://digtp.com/contacts...\n",
      "Парсинг https://www.eurochem-career.com/news/iskusstvennyi-intellekt-v-ximii-gpt-assistenty-v-evroxime...\n",
      "Парсинг https://otus.ru/instructors/10517...\n",
      "Парсинг https://ru.wikipedia.org/wiki/ЕвроХим...\n",
      "Парсинг https://www.eurochem.ru/usolskij-kalijnyj-kombinat/...\n",
      "Парсинг https://uralmines.ru/evrohim-usolskij-kalijnyj-kombinat/...\n",
      "Парсинг https://docs.ultralytics.com/tasks/segment...\n",
      "Парсинг https://docs.ultralytics.com/tasks/detect...\n",
      "Парсинг https://docs.ultralytics.com/tasks...\n",
      "Парсинг https://docs.ultralytics.com/modes/...\n",
      "Парсинг https://docs.ultralytics.com/solutions...\n",
      "Парсинг https://github.com/Koldim2001...\n",
      "Парсинг https://github.com/Koldim2001/YOLO-Patch-Based-Inference...\n",
      "Парсинг https://github.com/Koldim2001/TrafficAnalyzer...\n",
      "Парсинг https://github.com/Koldim2001/COCO_to_YOLOv8...\n"
     ]
    }
   ],
   "source": [
    "# Список URL-адресов\n",
    "urls = [\n",
    "    \"https://www.eurochem.ru/\",\n",
    "    \"https://www.eurochem.ru/global-operations/\",\n",
    "    \"https://www.eurochem.ru/about-us/komplaens/\",\n",
    "    \"https://www.eurochem.ru/proteh-lab/\",\n",
    "    \"https://digtp.com/\",\n",
    "    \"https://digtp.com/projects/machine-learning-platforma\",\n",
    "    \"https://digtp.com/projects/rekomendatelnye-modeli\",\n",
    "    \"https://digtp.com/projects/mobilnoe-prilozenie-mineralogiia\",\n",
    "    \"https://digtp.com/contacts\",\n",
    "    \"https://www.eurochem-career.com/news/iskusstvennyi-intellekt-v-ximii-gpt-assistenty-v-evroxime\",\n",
    "    \"https://otus.ru/instructors/10517\",\n",
    "    \"https://ru.wikipedia.org/wiki/ЕвроХим\",\n",
    "    \"https://www.eurochem.ru/usolskij-kalijnyj-kombinat/\",\n",
    "    \"https://uralmines.ru/evrohim-usolskij-kalijnyj-kombinat/\",\n",
    "    \"https://docs.ultralytics.com/tasks/segment\",\n",
    "    \"https://docs.ultralytics.com/tasks/detect\",\n",
    "    \"https://docs.ultralytics.com/tasks\",\n",
    "    \"https://docs.ultralytics.com/modes/\",\n",
    "    \"https://docs.ultralytics.com/solutions\",\n",
    "    \"https://github.com/Koldim2001\",\n",
    "    \"https://github.com/Koldim2001/YOLO-Patch-Based-Inference\",\n",
    "    \"https://github.com/Koldim2001/TrafficAnalyzer\",\n",
    "    \"https://github.com/Koldim2001/COCO_to_YOLOv8\"\n",
    "]\n",
    "\n",
    "# Словарь с описаниями и текстами\n",
    "url_data = {}\n",
    "\n",
    "# Парсинг и фильтрация\n",
    "for url in urls:\n",
    "    print(f\"Парсинг {url}...\")\n",
    "    parsed_data = parse_url(url)\n",
    "    if parsed_data['text']:\n",
    "        filtered_text = filter_text(parsed_data['text'])\n",
    "        url_data[url] = {\n",
    "            'text': filtered_text,\n",
    "            'description': parsed_data['description']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Итоговый текст сохранен в файл output_parsing.txt\n"
     ]
    }
   ],
   "source": [
    "# Сохранение объединенного текста\n",
    "combined_text = \"\\n\\n\\n\".join([data['text'] for data in url_data.values()])\n",
    "save_to_file(\"output_parsing.txt\", combined_text)\n",
    "print(\"Итоговый текст сохранен в файл output_parsing.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Чанки сохранены в файл chunks_output.txt\n"
     ]
    }
   ],
   "source": [
    "# Загрузка текста для чанкинга\n",
    "loaded_text = load_from_file(\"result_parsing.txt\")\n",
    "\n",
    "# Генерация чанков\n",
    "all_chunks = generate_chunks(loaded_text, url_data, chunk_size=1500, chunk_overlap=0)\n",
    "\n",
    "# Сохранение чанков\n",
    "with open(\"chunks_output.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for i, chunk in enumerate(all_chunks):\n",
    "        file.write(f\"Чанк {i+1} ({len(chunk)} символов):\\n{chunk}\\n{'='*50}\\n\")\n",
    "print(\"Чанки сохранены в файл chunks_output.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Производим сохранение в БД векторов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
    "import numpy as np\n",
    "from langchain.embeddings.base import Embeddings\n",
    "import requests\n",
    "from typing import List\n",
    "\n",
    "# Подключение к Milvus\n",
    "def connect_to_milvus(host=\"localhost\", port=\"19530\"):\n",
    "    \"\"\"\n",
    "    Устанавливает соединение с Milvus.\n",
    "\n",
    "    Args:\n",
    "        host (str): Хост Milvus.\n",
    "        port (str): Порт Milvus.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(f\"Connecting to Milvus at {host}:{port}...\")\n",
    "    connections.connect(\"default\", host=host, port=port)\n",
    "    print(\"Connected successfully!\")\n",
    "\n",
    "\n",
    "# Создание коллекции в Milvus (с проверкой существования)\n",
    "def create_milvus_collection(collection_name, dim):\n",
    "    \"\"\"\n",
    "    Создает новую коллекцию в Milvus, если она еще не существует.\n",
    "\n",
    "    Args:\n",
    "        collection_name (str): Имя коллекции.\n",
    "        dim (int): Размерность векторов.\n",
    "\n",
    "    Returns:\n",
    "        Collection: Экземпляр коллекции.\n",
    "    \"\"\"\n",
    "    # Проверяем, существует ли коллекция\n",
    "    if utility.has_collection(collection_name) and False:\n",
    "        print(f\"Collection '{collection_name}' already exists. Loading the collection...\")\n",
    "        collection = Collection(name=collection_name)\n",
    "    else:\n",
    "        # Определение полей коллекции\n",
    "        fields = [\n",
    "            FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "            FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=dim),\n",
    "            FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=65535),\n",
    "            FieldSchema(name=\"chunk_length\", dtype=DataType.INT64)\n",
    "        ]\n",
    "        schema = CollectionSchema(fields, description=\"Collection for text chunks\")\n",
    "        collection = Collection(name=collection_name, schema=schema)\n",
    "        print(f\"Collection '{collection_name}' created successfully.\")\n",
    "    \n",
    "    return collection\n",
    "\n",
    "\n",
    "# Генерация векторов для чанков\n",
    "def generate_embeddings(chunks, embedder):\n",
    "    \"\"\"\n",
    "    Генерирует векторные представления для списка чанков.\n",
    "\n",
    "    Args:\n",
    "        chunks (list): Список текстовых чанков.\n",
    "        embedder (callable): Функция или модель для генерации эмбеддингов.\n",
    "\n",
    "    Returns:\n",
    "        list: Список векторных представлений.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    for chunk in chunks:\n",
    "        embedding = embedder(chunk)  # Предполагается, что embedder принимает строку и возвращает вектор\n",
    "        embeddings.append(embedding)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "class CustomEmbedder(Embeddings):\n",
    "    def __init__(self, embedder_url=\"http://localhost:8080/embed\"):\n",
    "        self.embedder_url = embedder_url\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Получает эмбеддинги для списка текстов.\"\"\"\n",
    "        response = requests.post(\n",
    "            self.embedder_url,\n",
    "            json={\"inputs\": texts}\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            raise Exception(f\"Ошибка: {response.status_code}, {response.text}\")\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"Получает эмбеддинг для одного текста.\"\"\"\n",
    "        return self.embed_documents([text])[0]\n",
    "\n",
    "\n",
    "# Загрузка данных в Milvus\n",
    "def insert_data_into_milvus(collection, chunks, embedder):\n",
    "    \"\"\"\n",
    "    Вставляет чанки текста и их векторные представления в Milvus.\n",
    "\n",
    "    Args:\n",
    "        collection (Collection): Коллекция Milvus.\n",
    "        chunks (list): Список текстовых чанков.\n",
    "        embedder (callable): Функция или модель для генерации эмбеддингов.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Получаем эмбеддинги для списка текстов\n",
    "    embeddings = embedder.embed_documents(chunks)\n",
    "    print(np.array(embeddings).shape)\n",
    "    texts = [chunk for chunk in chunks]\n",
    "    lengths = [len(chunk) for chunk in chunks]\n",
    "\n",
    "    # Преобразование данных в формат, приемлемый для Milvus\n",
    "    data = [\n",
    "        embeddings,  # Векторы\n",
    "        texts,       # Тексты чанков\n",
    "        lengths,      # Длины чанков\n",
    "    ]\n",
    "    # Вставка данных в коллекцию\n",
    "    collection.insert(data)\n",
    "    \n",
    "    print(\"Data inserted into Milvus successfully.\")\n",
    "\n",
    "\n",
    "def create_index(collection_name, field_name=\"embedding\", index_params=None):\n",
    "    \"\"\"\n",
    "    Создает индекс на указанном поле в коллекции Milvus.\n",
    "\n",
    "    Args:\n",
    "        collection_name (str): Имя коллекции.\n",
    "        field_name (str): Поле, на котором создается индекс (по умолчанию \"embedding\").\n",
    "        index_params (dict): Параметры индекса (по умолчанию IVF_FLAT).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Проверяем, существует ли коллекция\n",
    "    if not utility.has_collection(collection_name):\n",
    "        print(f\"Collection '{collection_name}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Загружаем коллекцию\n",
    "    collection = Collection(name=collection_name)\n",
    "\n",
    "    # Проверяем, создан ли уже индекс\n",
    "    if collection.has_index():\n",
    "        print(f\"Index already exists for collection '{collection_name}'.\")\n",
    "        return\n",
    "\n",
    "    # Устанавливаем параметры индекса (по умолчанию используем IVF_FLAT)\n",
    "    if index_params is None:\n",
    "        index_params = {\n",
    "            \"index_type\": \"IVF_FLAT\",  # Тип индекса\n",
    "            \"params\": {\"nlist\": 128},  # Количество кластеров\n",
    "            \"metric_type\": \"IP\"        # Метрика (внутреннее произведение для косинусной близости)\n",
    "        }\n",
    "\n",
    "    # Создаем индекс\n",
    "    collection.create_index(field_name=field_name, index_params=index_params)\n",
    "    print(f\"Index created on field '{field_name}' for collection '{collection_name}'.\")\n",
    "\n",
    "    # Выгружаем и заново загружаем коллекцию для применения индекса\n",
    "    collection.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_milvus_collection(collection_name, limit=10):\n",
    "    \"\"\"\n",
    "    Выводит содержимое указанной коллекции Milvus.\n",
    "\n",
    "    Args:\n",
    "        collection_name (str): Имя коллекции.\n",
    "        limit (int): Количество записей для вывода (по умолчанию 10).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Проверяем, существует ли коллекция\n",
    "    if not utility.has_collection(collection_name):\n",
    "        print(f\"Collection '{collection_name}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Загружаем коллекцию\n",
    "    collection = Collection(name=collection_name)\n",
    "\n",
    "    # Проверяем, есть ли данные в коллекции\n",
    "    if collection.num_entities == 0:\n",
    "        print(f\"Collection '{collection_name}' is empty.\")\n",
    "        return\n",
    "\n",
    "    # Выполняем запрос для получения данных\n",
    "    collection.load()  # Загружаем данные в память (если они ещё не загружены)\n",
    "    results = collection.query(expr=\"id >= 0\", output_fields=[\"id\", \"text\", \"chunk_length\"], limit=limit)\n",
    "\n",
    "    # Выводим результаты\n",
    "    print(f\"Displaying {len(results)} records from collection '{collection_name}':\")\n",
    "    for record in results:\n",
    "        print(f\"ID: {record['id']}, Text: {record['text'][:50]}..., Length: {record['chunk_length']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Milvus at localhost:19530...\n",
      "Connected successfully!\n",
      "Collection 'text_chunks' created successfully.\n",
      "(123, 1024)\n",
      "Data inserted into Milvus successfully.\n"
     ]
    }
   ],
   "source": [
    "# Создаем экземпляр эмбеддера\n",
    "embedder = CustomEmbedder(embedder_url=\"http://localhost:8080/embed\")\n",
    "\n",
    "# Подключение к Milvus\n",
    "connect_to_milvus(host=\"localhost\", port=\"19530\")\n",
    "\n",
    "# Создание коллекции (если она еще не существует)\n",
    "collection_name = \"text_chunks\"\n",
    "dim = 1024  # Размерность векторов (замените на реальную размерность вашего эмбеддера)\n",
    "\n",
    "collection = create_milvus_collection(collection_name, dim)\n",
    "insert_data_into_milvus(collection, all_chunks, embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 10 records from collection 'text_chunks':\n",
      "ID: 456094952320354851, Text: [Источник: АО «Минерально-химическая компания Евро..., Length: 770\n",
      "ID: 456094952320354852, Text: [Источник: Наши активы - добыча, производство, про..., Length: 1412\n",
      "ID: 456094952320354853, Text: [Источник: Наши активы - добыча, производство, про..., Length: 1491\n",
      "ID: 456094952320354854, Text: [Источник: Комплаенс]\n",
      "Удобрения и кормовые продукт..., Length: 1482\n",
      "ID: 456094952320354855, Text: [Источник: Комплаенс]\n",
      "Основная роль в противодейст..., Length: 1417\n",
      "ID: 456094952320354856, Text: [Источник: Комплаенс]\n",
      "создана в ЕвроХим как один и..., Length: 1410\n",
      "ID: 456094952320354857, Text: [Источник: Комплаенс]\n",
      "как часть системы комплаенс-..., Length: 948\n",
      "ID: 456094952320354858, Text: [Источник: ПроТех Лаб]\n",
      "Удобрения и кормовые продук..., Length: 1503\n",
      "ID: 456094952320354859, Text: [Источник: ПроТех Лаб]\n",
      "НИЦ ПроТехИнжиниринг (г. Са..., Length: 1460\n",
      "ID: 456094952320354860, Text: [Источник: ПроТех Лаб]\n",
      "4. Внедрение инновационных ..., Length: 1362\n"
     ]
    }
   ],
   "source": [
    "display_milvus_collection(collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar_chunks(collection_name, query_embedding, top_k=15):\n",
    "    \"\"\"\n",
    "    Выполняет поиск top_k самых ближайших чанков к заданному запросу в коллекции Milvus.\n",
    "\n",
    "    Args:\n",
    "        collection_name (str): Имя коллекции.\n",
    "        query_embedding (list): Векторный запрос (эмбеддинг).\n",
    "        top_k (int): Количество ближайших чанков для поиска.\n",
    "\n",
    "    Returns:\n",
    "        list: Список кортежей (текст чанка, расстояние до запроса).\n",
    "    \"\"\"\n",
    "    # Проверяем, существует ли коллекция\n",
    "    if not utility.has_collection(collection_name):\n",
    "        print(f\"Collection '{collection_name}' does not exist.\")\n",
    "        return []\n",
    "\n",
    "    # Загружаем коллекцию\n",
    "    collection = Collection(name=collection_name)\n",
    "\n",
    "    # Если индекс не создан, создаём его\n",
    "    if not collection.has_index():\n",
    "        create_index(collection_name, field_name=\"embedding\")\n",
    "\n",
    "    # Если данные еще не загружены, выполняем загрузку\n",
    "    collection.load()\n",
    "\n",
    "    # Выполняем поиск\n",
    "    search_params = {\n",
    "        \"metric_type\": \"IP\",  # Используем внутреннее произведение для косинусной близости\n",
    "        \"params\": {\"nprobe\": 16}  # Параметр для оптимизации поиска\n",
    "    }\n",
    "    results = collection.search(\n",
    "        data=[query_embedding],  # Список запросов (векторов)\n",
    "        anns_field=\"embedding\",  # Поле для поиска (векторное представление)\n",
    "        param=search_params,\n",
    "        limit=top_k,  # Количество результатов\n",
    "        output_fields=[\"text\", \"chunk_length\"]  # Дополнительные поля для вывода\n",
    "    )\n",
    "\n",
    "\n",
    "    # Обработка результатов\n",
    "    similar_chunks = []\n",
    "    seen_text = set()  # Множество для отслеживания уникальных текстов\n",
    "\n",
    "    for result in results[0]:  # results[0] содержит результаты для первого запроса\n",
    "        entity = result.entity\n",
    "        distance = result.distance\n",
    "        text = entity.get(\"text\")\n",
    "        chunk_length = entity.get(\"chunk_length\")\n",
    "\n",
    "        # Проверяем, не встречался ли этот вектор ранее\n",
    "        if text not in seen_text:\n",
    "            seen_text.add(text)  # Добавляем вектор в множество\n",
    "            similar_chunks.append((text, distance, chunk_length))\n",
    "\n",
    "    return similar_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эмбеддинг для запроса: [0.011806826, -0.0011667218, -0.004861634, -0.018714476, 0.012379335, 0.0031112581, 0.009422936, 0.09062537, 0.052070167, -0.01226671, 0.029151034, 0.036527954, -0.031009343, -0.024871295, -0.007710101, -0.004516721, -0.055861864, -7.933591e-05, -0.01607718, 0.0068231816, 0.038273636, -0.011243702, -0.044111352, -0.028925784, 0.0007989317, -0.0047959364, -0.026260333, -0.018752018, -0.024082921, -0.027067477, -0.01858308, 0.01917436, 0.004033373, -0.03166632, -0.03162878, 0.04208411, 0.03450071, 0.033956356, -0.026729602, 0.034725957, -0.004171808, 0.06843829, 0.0041225343, -0.040357195, -0.03048376, 0.012088387, 0.028268807, -0.03352463, 0.007949429, 0.027630601, 0.01416256, 0.01880833, -0.010061143, -0.052670833, -0.041032944, 0.032285757, -0.0068137962, 0.03258609, -0.03619008, 0.037278786, -0.015082329, -0.021079596, 0.03168509, -0.043285437, -0.0066120103, 0.03707231, 0.024946379, 0.04899176, -0.056725323, -0.014491049, -0.02168026, 0.02950768, -0.0523705, -0.002632603, -0.016105337, -0.009526175, 0.035214, 0.005176045, 0.035814665, -0.033768646, 0.05792665, 0.020666638, 0.020760491, -0.0041154954, 0.021417469, -0.0040779538, 0.032210674, 0.038480114, 0.041558526, 0.0036274549, -0.0009942652, 0.034669645, 0.03070901, -0.05574924, -0.045763183, -0.012463803, 0.037316326, 0.042834938, -0.030183427, -0.03448194, -0.018151352, -0.0037846602, 0.05113163, -0.017409906, -0.027424121, 0.002719418, 0.032511007, 0.039155863, -0.0021562944, 0.026072625, 0.06288214, 0.052257873, 0.023144381, -0.015636066, -0.04696451, 0.023425944, -0.0017832249, -0.020441389, -0.05071867, 0.024570962, 0.026823457, 0.05308379, 0.00028654782, -0.0039090165, 0.0012341794, -0.0130456975, 0.027198872, 0.03352463, -0.02427063, 0.018911568, 0.02883193, -0.0034327079, -0.052032623, -0.032923963, -0.038236097, -0.026598208, -0.009957903, 0.0008945454, -0.005021186, 0.00049801246, -0.033017818, 0.026091395, 0.034669645, -0.03949374, -0.04016949, -0.039681446, -0.010230079, -0.007217368, -0.047490094, -0.024627274, 0.0034960592, -0.040582445, 0.0033810881, 0.028907014, 0.04016949, -0.010408402, 0.017250355, 0.039343573, 0.0074097686, 0.009512097, -0.051807377, -0.07444495, -0.0067293276, -0.02308807, 0.025640897, -0.008653333, 0.015110484, 0.0030877946, -0.00081770244, -0.04167115, -0.035157688, -0.04242198, -0.009742039, -0.037015993, -0.01605841, 0.028306348, 0.032923963, 0.022431092, 0.040432278, -0.036283933, -0.004584765, 0.04504989, 0.043961186, -0.020328764, -0.016621534, 0.01202269, 0.010361475, 0.03384373, 0.01320525, 0.034969978, -0.016349357, -0.008883276, 0.02855037, 0.038930614, -0.03237961, 0.028662995, 0.033412002, 0.046889428, -0.021004513, -0.04640139, -0.035814665, 0.026504353, -0.03885553, 0.023726277, 0.011534649, -0.046063516, -0.03478227, -0.035420477, 0.021924281, -0.044073813, -0.109696485, -0.018123196, 0.021943051, -0.011534649, -0.03478227, -0.051356878, -0.032022964, 0.015504671, -0.00025839164, -0.0048850975, 0.0038222019, 0.047752887, 0.004422867, 0.017794708, 0.057438612, 0.028081099, 0.07669744, 0.030765321, 0.013834071, 0.012304252, 0.02832512, 0.018395372, -0.039343573, -0.012360564, 0.03926849, -0.028907014, -0.034331772, -0.03161001, 0.00624598, 0.025284251, -0.03498875, 0.016349357, -0.031816486, -0.007170441, -0.01811381, -0.029470138, 0.005143196, -0.056349907, 0.0121447, -0.00038010845, 0.034219146, 0.036790743, -0.02121099, -0.0012670282, 0.05758878, 0.029151034, 0.035176456, -0.009305619, 0.020966971, -0.0042985105, -0.012463803, 0.0048850975, -0.023031758, 0.053534288, 0.03288642, 0.02781831, -0.0076303254, -0.041032944, -0.02027245, -0.0226188, -0.061267853, -0.03450071, -0.020779263, -0.011356327, -0.01583316, 0.038161013, -0.027029935, -0.013974852, 0.026466811, 0.0035664497, 0.03191034, -0.03003326, 0.029226117, 0.009366623, -0.010643037, 0.031816486, 0.025340565, -0.03707231, 0.028494056, -0.031572465, 0.023726277, -0.030765321, 0.08326722, 0.01369329, -0.0014125018, 0.0020178598, -0.02571598, 0.014885236, -0.025415648, 0.016593376, -0.030352365, -0.026016312, 0.023388403, 0.010361475, -0.023519797, -0.030727781, -0.040432278, 0.03192911, -0.048240926, 0.023181923, -0.012961229, 0.024514649, 0.019915806, -0.051769834, -0.041746233, -0.032811336, -0.026091395, 0.025584584, -0.014613058, 0.029319972, -0.045763183, -0.0054435288, -0.021717802, 0.020610325, 0.029451367, -0.0073816124, 0.00024739312, -0.026110167, 0.03384373, -0.020629097, 0.038968157, -0.012285481, -0.013195864, 0.02455219, 0.004230466, 0.02883193, -0.034331772, 0.030521302, 0.016189804, 0.015504671, 0.023275778, -0.01835783, -0.023613652, -0.04501235, 0.02832512, -0.06551005, 0.033956356, -0.048391093, 0.029432597, 0.006349219, 0.015889471, 0.016921865, 0.023557339, 0.049367175, 0.004134266, 0.06945191, -0.004974259, -0.047752887, 0.025828606, -0.014631829, 0.039418656, 0.03166632, 0.03885553, -0.022055676, -0.023857672, -0.032980274, 0.031872798, -0.019024193, -0.022468634, -0.029395055, 0.039343573, 0.03971899, -0.026485583, 0.002280651, 0.029094722, -0.048691425, -0.036978453, -0.011440796, 0.006306985, -0.0034561714, -0.012210398, -0.025171626, 0.042234275, -0.014509819, -0.0326424, -0.040319655, 0.15692379, -0.0053684455, 0.02973293, -0.042985104, -0.041933943, -0.014988475, 0.01833906, 0.021717802, 0.038667824, 0.022487404, -0.02047893, -0.0033271222, 0.012567042, -0.032417152, 0.032417152, 0.0577014, 0.03048376, 0.026860999, 0.05541137, -0.017428678, 0.04662664, -0.022825278, -0.0039676754, 0.03806716, -0.03761666, -0.012567042, 0.01261397, 0.035120144, -0.04403627, 0.00045841784, -0.05713828, 0.009075676, -0.032773796, -0.021623949, 0.013787144, 0.03192911, 0.014415965, -0.0359836, 0.041483443, -0.02952645, -0.0030854484, 0.00053144793, -0.004347784, -0.013064468, 0.017062647, 0.030840404, 0.020798033, -0.032041736, -0.04095786, -0.0024542806, 0.004483872, 0.034725957, -0.030296052, -0.04591335, -0.028475286, -0.051319335, -0.026504353, -0.02811864, 0.025134085, -0.00068748015, 0.04737747, 0.010389632, -0.056612696, -0.036471643, 0.020985741, 0.002611486, -0.023501027, -0.04805322, -0.024233088, -0.03908078, -0.035214, -0.008775343, -0.0122573245, 0.021342386, 0.024158005, -0.01628366, -0.030877946, 0.049254548, 0.03600237, -0.004704429, 0.010933984, -0.031028112, -0.016508909, 0.019277599, 0.07369412, -0.01724097, 0.028662995, 0.0447871, -0.00619436, -0.001965067, 0.03161001, 0.030333593, 0.015805004, 0.06385822, 0.028494056, -0.04452431, 0.018507997, 0.011994534, -0.025547042, -0.025359334, -0.008057361, 0.018536154, -0.01273598, -0.00030018596, 0.0057391687, -0.05304625, 0.039906695, 0.036359016, 0.0067903325, -0.00440175, 0.01024885, 0.007874345, -0.007142285, -0.0226188, 0.028719306, 0.03904324, 0.04050736, -0.050530963, 0.012463803, 0.02667329, -0.039155863, 0.021924281, -0.004650463, 0.062131308, -0.029676616, 0.025678439, -0.03545802, 0.014575517, 0.02235601, -0.00059655914, 0.008906739, -0.021699032, -0.04403627, 0.037992075, 0.03493244, -0.0059315693, 0.055824324, -0.0264105, 0.006133355, 0.0595034, -0.009305619, 0.024345713, -0.025171626, 0.031872798, 0.005377831, 0.045650557, -0.015363891, -0.04268477, -0.023726277, -0.018451685, 0.012435648, -0.05574924, 0.033674795, 0.011234317, -0.033412002, 0.027442893, 0.015457744, 0.060817353, 0.02121099, -0.05113163, -0.039681446, -0.016593376, -0.021135908, 0.07283066, 0.027105018, 0.06333264, 0.021755343, -0.0031487998, 0.018752018, 0.014096862, -0.010004831, -0.024683587, -0.0019040619, 0.034444395, 0.01788856, -0.034913667, -0.037935764, 0.014003008, -0.026992394, 0.01644321, 0.0028226573, 0.044336602, 0.031009343, -0.014913391, -0.033412002, -0.005420065, 0.0030408676, -0.018507997, -0.024814982, 0.021830427, -0.036603037, 0.12110913, -0.054848243, 0.031816486, -0.002268919, 0.00649, -0.011384483, 0.019915806, -0.037992075, -0.03832995, -0.0069921184, -0.061305396, -0.0024472415, -0.0226188, -0.021980593, -0.0035594108, 0.0025833298, 0.0044533694, 0.007442618, 0.03836749, -0.0041694613, -0.025809834, 0.019333912, 0.020554014, -0.0026842228, 0.05116917, 0.04906684, 0.012445033, -0.0005839475, -0.021417469, 0.002363946, 0.02736781, 0.03172263, -0.042534605, -0.025547042, -0.021999365, 0.01501663, -0.010098684, -0.023782589, -0.03493244, -0.0039019776, -0.039643906, 0.022975445, -0.077936314, -0.025753522, -0.012369949, 0.04426152, 0.019258829, 0.027273955, -0.015316963, 0.026804686, -0.017635155, 0.056349907, -0.02856914, 0.00020809179, 0.058302067, -0.06119277, -0.025828606, 0.013561894, 0.04456185, 0.013590051, -0.017090803, 0.022055676, 0.0072736805, -0.029695388, -0.020366305, 0.0036274549, -0.01811381, -0.043886103, -0.049855214, 0.028062329, -0.008291996, -0.023144381, -0.019878265, -0.029094722, 0.070202745, 0.0059081055, -0.007405076, -0.046814345, -0.04482464, 0.05214525, -0.024176775, -0.0581519, -0.014415965, 0.058302067, 0.0076913303, -0.009835893, -0.033186752, -0.06614826, -0.004516721, -0.00958718, 0.050155547, 0.021980593, -0.01905235, 0.033055358, 0.02310684, -0.035589416, 0.03162878, 0.030540073, -0.06006652, -0.03144107, 0.015335734, -0.00929154, 0.04268477, -0.016405668, -0.0045824186, -0.028250037, -0.0016447903, -0.0029305893, -0.01929637, 0.01858308, -0.0359836, -0.05049342, -0.021417469, -0.03881799, 0.00803859, 0.00094440527, 0.03192911, 0.06333264, 0.008395235, -0.036359016, -0.040657528, -0.01835783, 0.03493244, -0.02832512, 0.004852249, 0.03095303, -0.011703586, -0.01691248, 0.045312684, -0.03859274, 0.0015849584, -0.007395691, 0.02573475, 0.038968157, 0.005147889, 0.030070802, 0.008568865, 0.024139233, -0.018432913, 0.022919133, 0.0036462257, -0.044111352, -0.009685727, -0.052670833, -0.002321712, -0.070653245, -0.03095303, -0.029920636, -0.028494056, 0.006626088, 0.012989385, 0.06562267, -0.03716616, 0.032698713, 0.047339927, -0.011666045, 0.029582763, -0.024721129, -0.023519797, -0.020629097, 0.016377512, -0.023519797, 0.0038996313, -0.013880998, 0.04617614, -0.042046566, 0.020873116, -0.03425669, -0.017700853, 0.018282747, -0.018451685, 0.03684706, -0.016734159, -0.02883193, 0.014716298, -0.0071328995, -0.003507791, -0.035101373, 0.033918813, -0.03695968, -0.037316326, -0.035214, -0.008681489, -0.022018135, 0.029920636, -0.022318467, 0.0049789515, 0.032698713, 0.016349357, -0.0075833984, -0.023726277, -0.010464715, -0.034725957, 0.03763543, -0.025565814, -0.0409954, 0.023219464, -0.0051854304, 0.05409741, 0.015532827, 0.04095786, -0.04077015, 0.04016949, -0.06626088, -0.0074566957, 0.027536746, -0.043510687, -0.010483486, -0.015420202, -0.061530642, 0.03480104, -0.004779512, -0.025565814, 0.032698713, -0.009554331, 0.00702966, -0.010736891, -0.03527031, 0.019615473, 0.0120602315, -0.026241561, 0.03665935, -0.023669964, 0.0423469, -0.0033153906, -0.025077773, -0.015532827, 0.07403199, -0.016762314, 0.060216688, 0.010483486, -0.027217643, -0.06659876, -0.005781403, -0.05349675, 0.018536154, 0.024326941, -0.017494375, 0.04257215, -0.040056862, -0.02404538, -0.039794073, -0.02263757, -0.034575794, 0.0012717209, -0.024120463, 0.029695388, -0.03172263, 0.050606046, 0.015682993, -0.025828606, 0.020122286, -0.041070484, -0.035326622, -0.013289718, 0.033956356, 0.04887913, 0.05026817, 0.03767297, 0.0058424077, -0.022449862, 0.038930614, 0.044899724, 0.031328447, 0.00803859, -0.021042055, -0.009455784, -0.027668143, -0.009192994, -0.013862227, -0.019803181, -0.010230079, 0.0069029573, 0.013224021, -0.009056905, 0.05192, 0.015908243, 0.046251222, -0.018461071, -0.021079596, 0.042159192, 0.048691425, 0.03737264, -0.030596385, -0.046138596, 0.01238872, -0.02787462, -0.011215546, 0.0048381705, 0.018235821, -0.03237961, -0.035326622, -0.014725683, -0.056762863, -0.0014207141, 0.032529775, -0.025772292, 0.0029728236, -0.03502629, -0.04910438, 0.022018135, 0.0020366306, 0.055336285, -0.027255185, 0.007888424, 0.020854346, 0.029470138, -0.021924281, -0.008568865, 0.04448677, -0.005157274, 0.052032623, -0.015063558, -0.027536746, 0.010952755, -0.0102113085, -0.00060887745, -0.04550039, -0.00312299, 0.020197367, 0.036490414, -0.031178279, -0.00612397, -0.0029845554, -0.020178597, 0.06250673, -0.04456185, -0.019136818, -0.036133766, 0.013017542, -0.048428632, 0.004052144, -0.013317875, 0.058076818, -0.058302067, 0.0010728679, -0.0005613639, -0.02308807, -0.015063558, -0.0025997541, 0.07174195, -0.016583992, 0.041596066, -0.022675112, -0.013590051, -0.038161013, -0.031816486, -0.02213076, 0.01298, -0.02025368, -0.07144162, -0.055899408, -0.029639075, 0.029376283, -0.061080147, -0.027180102, 0.03211682, -0.0050305715, 0.01012684, 0.0036133768, -0.02213076, 0.022881592, -0.024082921, -0.020366305, 0.018489227, 0.0049179466, 0.019671787, -0.008475011, -0.058752567, 0.005373138, 0.018385988, -0.035326622, -0.00998606, 0.022374779, 0.02667329, -0.049404714, 0.04786551, 0.030183427, -0.001736298, 0.028531598, -0.004066222, 0.022825278, 0.034012668, -0.03737264, -0.041370817, -0.0010364995, 0.018123196, 0.027180102, 0.030896718, 0.014190716, 0.01976564, -0.0046809656, 0.039155863, -0.010633651, 0.023501027, -0.021586407, 0.007813341, 0.005171352, -0.018780174, -0.022675112, 0.04257215, -0.027912162, -0.014453507, 0.021642718, -0.034913667, 0.038780447, -0.009526175, -0.023538569, 0.023181923, -0.047715344, 0.014509819, -0.008911432, 0.013383572, 0.037823137, -0.030296052, 0.0032848879, 0.048203383, 0.04381102, 0.02475867, 0.048841592, 0.011731743, 0.026016312, 0.018301519, -0.024383254, 0.029676616, -0.040845234, -0.035514332, -0.01772901, 0.019240057, -0.03161001, 0.03930603, -0.019221287, -0.028418973, 0.033900045, 0.0050681126, 0.009920361, 0.018639393, -0.009605951, -0.041145567, -0.0005390736, -0.01524188, 0.015354505, 0.0024777441, 0.022374779, -0.030840404, 0.009835893, 0.041108027]\n",
      "\n",
      "Найдено 10 похожих чанков:\n",
      "1. Расстояние: 0.8094, Длина: 627, Текст: [Источник: GitHub - Koldim2001/TrafficAnalyzer: Анализ трафика на круговом движении с использованием компьютерного зрения]\n",
      "По сути это та же ветка mul...\n",
      "2. Расстояние: 0.8050, Длина: 1623, Текст: [Источник: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      "Pre-initialized m...\n",
      "3. Расстояние: 0.8018, Длина: 1348, Текст: [Источник: GitHub - Koldim2001/TrafficAnalyzer: Анализ трафика на круговом движении с использованием компьютерного зрения]\n",
      ". В ней реализовано всё то ...\n",
      "4. Расстояние: 0.8009, Длина: 1587, Текст: [Источник: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      ": This advanced m...\n",
      "5. Расстояние: 0.8006, Длина: 1567, Текст: [Источник: GitHub - Koldim2001/TrafficAnalyzer: Анализ трафика на круговом движении с использованием компьютерного зрения]\n",
      "GitHub - Koldim2001/Traffic...\n",
      "6. Расстояние: 0.7997, Длина: 1597, Текст: [Источник: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      "GitHub - Koldim20...\n",
      "7. Расстояние: 0.7987, Длина: 421, Текст: [Источник: ПроТех Лаб]\n",
      "Примеры цитируемости по профессиональной деятельности\n",
      "Референсы по работе ментором/экспертом\n",
      "Отметки о государственных или иных...\n",
      "8. Расстояние: 0.7977, Длина: 1613, Текст: [Источник: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      "polygons: If avai...\n",
      "9. Расстояние: 0.7968, Длина: 226, Текст: [Источник: Ultralytics YOLO11 Modes - Ultralytics YOLO Docs]\n",
      "# Perform object tracking on a video from the command line # You can specify different so...\n",
      "10. Расстояние: 0.7963, Длина: 1471, Текст: [Источник: GitHub - Koldim2001/TrafficAnalyzer: Анализ трафика на круговом движении с использованием компьютерного зрения]\n",
      "Проект представляет собой с...\n"
     ]
    }
   ],
   "source": [
    "# Имя коллекции\n",
    "collection_name = \"text_chunks\"\n",
    "\n",
    "# Создание эмбеддинга для запроса\n",
    "query = \"что такое патчевый инференс?\"\n",
    "query_embedding = embedder.embed_query(query)  # Предполагается, что embedder уже определен\n",
    "print(\"Эмбеддинг для запроса:\", query_embedding)\n",
    "\n",
    "# Поиск похожих чанков\n",
    "similar_chunks = search_similar_chunks(collection_name, query_embedding, top_k=10)\n",
    "\n",
    "# Вывод результатов\n",
    "print(f\"\\nНайдено {len(similar_chunks)} похожих чанков:\")\n",
    "for i, (text, distance, length) in enumerate(similar_chunks, start=1):\n",
    "    print(f\"{i}. Расстояние: {distance:.4f}, Длина: {length}, Текст: {text[:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_milvus_collection(collection_name):\n",
    "    \"\"\"\n",
    "    Очищает все данные из указанной коллекции Milvus, оставляя саму коллекцию intact.\n",
    "\n",
    "    Args:\n",
    "        collection_name (str): Имя коллекции.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Проверяем, существует ли коллекция\n",
    "    if not utility.has_collection(collection_name):\n",
    "        print(f\"Collection '{collection_name}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Загружаем коллекцию\n",
    "    collection = Collection(name=collection_name)\n",
    "\n",
    "    # Проверяем, есть ли данные в коллекции\n",
    "    if collection.num_entities == 0:\n",
    "        print(f\"Collection '{collection_name}' is already empty.\")\n",
    "        return\n",
    "\n",
    "    # Очищаем данные\n",
    "    collection.delete(expr=\"id > 0\")  # Удаляем все записи\n",
    "    print(f\"All data from collection '{collection_name}' has been cleared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data from collection 'text_chunks' has been cleared.\n"
     ]
    }
   ],
   "source": [
    "# Очистка коллекции\n",
    "clear_milvus_collection(collection_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Реализация реранка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import requests\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Определение класса CustomReranker (оставляем без изменений)\n",
    "class CustomReranker:\n",
    "    def __init__(self, reranker_url: str):\n",
    "        self.reranker_url = reranker_url\n",
    "\n",
    "    def rerank(self, query: str, documents: List[Document]) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Пересчитывает релевантность документов на основе запроса.\n",
    "        \"\"\"\n",
    "        # Преобразуем документы в текстовый формат\n",
    "        texts = [doc.page_content for doc in documents]\n",
    "        # Отправляем запрос к реранкеру\n",
    "        response = requests.post(\n",
    "            self.reranker_url,\n",
    "            json={\"query\": query, \"texts\": texts}\n",
    "        )\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"Ошибка: {response.status_code}, {response.text}\")\n",
    "        # Получаем результаты реранкинга\n",
    "        results = response.json()\n",
    "        reranked_docs = []\n",
    "        # Сортируем результаты по убыванию оценки\n",
    "        results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "        # Сопоставляем результаты с документами\n",
    "        for result in results:\n",
    "            index = result[\"index\"]\n",
    "            score = result[\"score\"]\n",
    "            doc = documents[index]\n",
    "            doc.metadata[\"score\"] = score\n",
    "            reranked_docs.append(doc)\n",
    "        return reranked_docs\n",
    "\n",
    "\n",
    "# Функция для преобразования чанков в документы\n",
    "def chunks_to_documents(similar_chunks):\n",
    "    \"\"\"\n",
    "    Преобразует список чанков в список объектов Document.\n",
    "\n",
    "    Args:\n",
    "        similar_chunks (list): Список кортежей (текст чанка, расстояние до запроса).\n",
    "\n",
    "    Returns:\n",
    "        list: Список объектов Document.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    for i, (text, distance, chunk_length) in enumerate(similar_chunks):\n",
    "        metadata = {\n",
    "            \"source\": f\"chunk_{i}\",  # Источник чанка\n",
    "            \"distance\": distance,   # Расстояние до запроса\n",
    "            \"chunk_length\": chunk_length  # Длина чанка\n",
    "        }\n",
    "        documents.append(Document(page_content=text, metadata=metadata))\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Топ-3 самых релевантных чанка:\n",
      "1. Документ: [Источник: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object dete... | Релевантность: 0.2520 | Источник: chunk_5\n",
      "2. Документ: [Источник: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object dete... | Релевантность: 0.1458 | Источник: chunk_3\n",
      "3. Документ: [Источник: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object dete... | Релевантность: 0.1009 | Источник: chunk_1\n"
     ]
    }
   ],
   "source": [
    "# Преобразование чанков в документы\n",
    "documents = chunks_to_documents(similar_chunks)\n",
    "\n",
    "# Указываем URL реранкера\n",
    "RERANKER_URL = \"http://localhost:8081/rerank\"\n",
    "custom_reranker = CustomReranker(reranker_url=RERANKER_URL)\n",
    "\n",
    "# Применяем реранкер\n",
    "reranked_docs = custom_reranker.rerank(query, documents)\n",
    "\n",
    "# Выбираем топ-3 самых релевантных чанка\n",
    "top_3_docs = reranked_docs[:3]\n",
    "\n",
    "# Выводим результаты\n",
    "print(\"\\nТоп-3 самых релевантных чанка:\")\n",
    "for i, doc in enumerate(top_3_docs, start=1):\n",
    "    print(f\"{i}. Документ: {doc.page_content[:100]}... | Релевантность: {doc.metadata['score']:.4f} | Источник: {doc.metadata['source']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Соберем воедино RAG и зададим вопрос"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Хочу конвертировать разметку из COCO в YOLO формат. У меня разметка лежит в папке data и еще мне нужно автоматическое сплитование. Пусть на трейне будет 60% данных. Напиши cli команду что мне нужна\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"text_chunks\"\n",
    "\n",
    "embedder = CustomEmbedder(embedder_url=\"http://localhost:8080/embed\")\n",
    "query_embedding = embedder.embed_query(question)  # Предполагается, что embedder уже определен\n",
    "\n",
    "# Поиск похожих чанков\n",
    "similar_chunks = search_similar_chunks(collection_name, query_embedding, top_k=15)\n",
    "documents = chunks_to_documents(similar_chunks)\n",
    "\n",
    "custom_reranker = CustomReranker(reranker_url=\"http://localhost:8081/rerank\")\n",
    "reranked_docs = custom_reranker.rerank(question, documents)\n",
    "\n",
    "# Фильтрация топ-документов по условию score > 0.0025\n",
    "top_docs = [doc for doc in reranked_docs if doc.metadata.get(\"score\", 0) > 0.005]\n",
    "top_docs = top_docs[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'chunk_1', 'distance': 0.9030083417892456, 'chunk_length': 1674, 'score': 0.96989965}, page_content='[Источник: GitHub - Koldim2001/COCO_to_YOLOv8: Converting COCO annotation (CVAT) to annotation for YOLO-seg (instance segmentation) and YOLO-obb (oriented bounding box detection)]\\n--yolo_dataset TEXT   Folder with the resulting YOLOv8 format dataset.\\n--print_info BOOLEAN  Enable/Disable processing log output mode. Default is\\n--autosplit BOOLEAN   Enable/Disable automatic split into train/val. Default\\nis disabled (uses the CVAT annotations)\\n--percent_val FLOAT   Percentage of data for validation when using\\nautosplit=True. Default is 25%\\n--help                Show existing options for parsing arguments in the CLI\\nRussian Version of README:\\nРепозиторий позволяет преобразовать разметку формата COCO в формат, поддерживаемый для обучения моделей YOLOv8-seg (инстанс сегментация) и YOLOv8-obb (детекция повернутых боксов).\\nКлючевое применение репозитория -> работа с выгруженной разметкой\\nв случае с YOLOv8-obb) из приложения CVAT в формате COCO 1.0 (с указанием режима save images = True).\\nЕсли же используете без CVAT, то убедитесь перед запуском, что ваша папка с COCO датасетом имеет такую структуру:\\nPS: Для задчи инстанс сегментации имеется также поддержка моделей Ultralytics YOLO11-seg, YOLOv9-seg и YOLOv5-seg (так как у них аналогичная разметка с версией v8)\\nПример использования репозитория для задачи\\nпредставлен в видео на YouTube -\\nПример использования репозитория для задачи\\nпредставлен в видео на YouTube -\\npip install -r requirements.txt\\nКлассический подход c предустановленным в CVAT разделением на train/val/test (у тасок определен Subset):\\npython coco_to_yolo.py --coco_dataset=\"dataset_folder\" --lang_ru=True\\nВариант с авторазделением на train и val:'), Document(metadata={'source': 'chunk_4', 'distance': 0.8902299404144287, 'chunk_length': 1622, 'score': 0.95695716}, page_content='[Источник: GitHub - Koldim2001/COCO_to_YOLOv8: Converting COCO annotation (CVAT) to annotation for YOLO-seg (instance segmentation) and YOLO-obb (oriented bounding box detection)]\\nGitHub - Koldim2001/COCO_to_YOLOv8: Converting COCO annotation (CVAT) to annotation for YOLO-seg (instance segmentation) and YOLO-obb (oriented bounding box detection)\\nCOCO to YOLO converter for instance segmentation (YOLOv8-seg) and oriented bounding box detection (YOLOv8-obb)\\nThe repository allows converting annotations in COCO format to a format compatible with training YOLOv8-seg models (instance segmentation) and YOLOv8-obb models (rotated bounding box detection).\\nKey usage of the repository -> handling annotated\\nin the case of YOLOv8-obb) exported from the CVAT application in COCO 1.0 format (with the save images mode set to True).\\nIf you use it without CVAT, make sure that your COCO dataset folder has the following structure:\\nPS: For the instance segmentation task, Ultralytics YOLO11-seg, YOLOv9-seg and YOLOv5-seg models are also supported (since they have similar annotation format to v8).\\npip install -r requirements.txt\\nHow to run the code:\\nClassic approach with pre-defined train/val/test split from CVAT (Tasks have a defined Subset in CVAT):\\nOption with automatic split into train and val:\\npython coco_to_yolo.py --coco_dataset=\"dataset_folder\" --autosplit=True --percent_val=30\\nList of parameters with explanations that can be passed to the program before running it in the command line interface (CLI):\\n--coco_dataset TEXT   Folder with COCO 1.0 format dataset (can be exported\\nfrom CVAT). Default is \"COCO_dataset\"'), Document(metadata={'source': 'chunk_0', 'distance': 0.9058507680892944, 'chunk_length': 1077, 'score': 0.9302051}, page_content='[Источник: GitHub - Koldim2001/COCO_to_YOLOv8: Converting COCO annotation (CVAT) to annotation for YOLO-seg (instance segmentation) and YOLO-obb (oriented bounding box detection)]\\npython coco_to_yolo.py --coco_dataset=\"dataset_folder\" --autosplit=True --percent_val=30 --lang_ru=True\\nСписок параметров с пояснениями, которые можно передать на вход программы перед ее запуском в cli:\\n--coco_dataset TEXT   Папка с датасетом формата COCO 1.0 (можно выгрузить из\\nCVAT). По умолчанию \" COCO_dataset \" --yolo_dataset TEXT   Папка с итоговым датасетом формата YOLOv8. По\\nумолчанию \" YOLO_dataset \" --print_info BOOLEAN  Вкл/Выкл режима вывода логов обработки. По умолчанию\\n--autosplit BOOLEAN   Вкл/Выкл режима автоматического разделения на\\ntrain/val. По умолчанию отключен (берет согласно\\n--percent_val FLOAT   Процент данных на валидацию при выборе режима\\nautosplit=True. По умолчанию 25%\\n--lang_ru BOOLEAN     Устанавливает русский язык комментариев, если выбрано\\nзначение True. По умолчанию английский\\n--help                Покажет существующие варианты парсинга аргументов в CLI'), Document(metadata={'source': 'chunk_3', 'distance': 0.8920798301696777, 'chunk_length': 1511, 'score': 0.67308426}, page_content='[Источник: Segment - Ultralytics YOLO Docs]\\nvalues are for single-model single-scale on\\nyolo val segment data=coco.yaml device=0\\naveraged over COCO val images using an\\nyolo val segment data=coco.yaml batch=1 device=0|cpu\\nTrain YOLO11n-seg on the COCO8-seg dataset for 100\\nat image size 640. For a full list of available arguments see the\\nfrom ultralytics import YOLO # Load a model model = YOLO ( \"yolo11n-seg.yaml\" ) # build a new model from YAML model = YOLO ( \"yolo11n-seg.pt\" ) # load a pretrained model (recommended for training) model = YOLO ( \"yolo11n-seg.yaml\" ) . load ( \"yolo11n.pt\" ) # build from YAML and transfer weights # Train the model results = model . train ( data = \"coco8-seg.yaml\" , epochs = 100 , imgsz = 640 )\\n# Build a new model from YAML and start training from scratch yolo segment train data = coco8-seg.yaml model = yolo11n-seg.yaml epochs = 100 imgsz = 640 # Start training from a pretrained *.pt model yolo segment train data = coco8-seg.yaml model = yolo11n-seg.pt epochs = 100 imgsz = 640 # Build a new model from YAML, transfer pretrained weights to it and start training yolo segment train data = coco8-seg.yaml model = yolo11n-seg.yaml pretrained = yolo11n-seg.pt epochs = 100 imgsz = 640\\nYOLO segmentation dataset format can be found in detail in the\\n. To convert your existing dataset from other formats (like COCO etc.) to YOLO format, please use\\nValidate trained YOLO11n-seg model\\non the COCO8-seg dataset. No arguments are needed as the\\nand arguments as model attributes.'), Document(metadata={'source': 'chunk_2', 'distance': 0.8970799446105957, 'chunk_length': 1319, 'score': 0.6399611}, page_content='[Источник: Detect - Ultralytics YOLO Docs]\\nyolo val detect data=coco.yaml batch=1 device=0|cpu\\nTrain YOLO11n on the COCO8 dataset for 100\\nat image size 640. For a full list of available arguments see the\\nfrom ultralytics import YOLO # Load a model model = YOLO ( \"yolo11n.yaml\" ) # build a new model from YAML model = YOLO ( \"yolo11n.pt\" ) # load a pretrained model (recommended for training) model = YOLO ( \"yolo11n.yaml\" ) . load ( \"yolo11n.pt\" ) # build from YAML and transfer weights # Train the model results = model . train ( data = \"coco8.yaml\" , epochs = 100 , imgsz = 640 )\\n# Build a new model from YAML and start training from scratch yolo detect train data = coco8.yaml model = yolo11n.yaml epochs = 100 imgsz = 640 # Start training from a pretrained *.pt model yolo detect train data = coco8.yaml model = yolo11n.pt epochs = 100 imgsz = 640 # Build a new model from YAML, transfer pretrained weights to it and start training yolo detect train data = coco8.yaml model = yolo11n.yaml pretrained = yolo11n.pt epochs = 100 imgsz = 640\\nYOLO detection dataset format can be found in detail in the\\n. To convert your existing dataset from other formats (like COCO etc.) to YOLO format, please use\\nValidate trained YOLO11n model\\non the COCO8 dataset. No arguments are needed as the\\nand arguments as model attributes.')]\n"
     ]
    }
   ],
   "source": [
    "print(top_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk: [Источник: GitHub - Koldim2001/COCO_to_YOLOv8: Converting COCO annotation (CVAT) to annotation for YOLO-seg (instance segmentation) and YOLO-obb (oriented bounding box detection)]\n",
      "--yolo_dataset TEXT   Folder with the resulting YOLOv8 format dataset.\n",
      "--print_info BOOLEAN  Enable/Disable processing log output mode. Default is\n",
      "--autosplit BOOLEAN   Enable/Disable automatic split into train/val. Default\n",
      "is disabled (uses the CVAT annotations)\n",
      "--percent_val FLOAT   Percentage of data for validation when using\n",
      "autosplit=True. Default is 25%\n",
      "--help                Show existing options for parsing arguments in the CLI\n",
      "Russian Version of README:\n",
      "Репозиторий позволяет преобразовать разметку формата COCO в формат, поддерживаемый для обучения моделей YOLOv8-seg (инстанс сегментация) и YOLOv8-obb (детекция повернутых боксов).\n",
      "Ключевое применение репозитория -> работа с выгруженной разметкой\n",
      "в случае с YOLOv8-obb) из приложения CVAT в формате COCO 1.0 (с указанием режима save images = True).\n",
      "Если же используете без CVAT, то убедитесь перед запуском, что ваша папка с COCO датасетом имеет такую структуру:\n",
      "PS: Для задчи инстанс сегментации имеется также поддержка моделей Ultralytics YOLO11-seg, YOLOv9-seg и YOLOv5-seg (так как у них аналогичная разметка с версией v8)\n",
      "Пример использования репозитория для задачи\n",
      "представлен в видео на YouTube -\n",
      "Пример использования репозитория для задачи\n",
      "представлен в видео на YouTube -\n",
      "pip install -r requirements.txt\n",
      "Классический подход c предустановленным в CVAT разделением на train/val/test (у тасок определен Subset):\n",
      "python coco_to_yolo.py --coco_dataset=\"dataset_folder\" --lang_ru=True\n",
      "Вариант с авторазделением на train и val:..., \n",
      "metadata={'source': 'chunk_1', 'distance': 0.9030083417892456, 'chunk_length': 1674, 'score': 0.96989965}\n",
      "\n",
      "Chunk: [Источник: GitHub - Koldim2001/COCO_to_YOLOv8: Converting COCO annotation (CVAT) to annotation for YOLO-seg (instance segmentation) and YOLO-obb (oriented bounding box detection)]\n",
      "GitHub - Koldim2001/COCO_to_YOLOv8: Converting COCO annotation (CVAT) to annotation for YOLO-seg (instance segmentation) and YOLO-obb (oriented bounding box detection)\n",
      "COCO to YOLO converter for instance segmentation (YOLOv8-seg) and oriented bounding box detection (YOLOv8-obb)\n",
      "The repository allows converting annotations in COCO format to a format compatible with training YOLOv8-seg models (instance segmentation) and YOLOv8-obb models (rotated bounding box detection).\n",
      "Key usage of the repository -> handling annotated\n",
      "in the case of YOLOv8-obb) exported from the CVAT application in COCO 1.0 format (with the save images mode set to True).\n",
      "If you use it without CVAT, make sure that your COCO dataset folder has the following structure:\n",
      "PS: For the instance segmentation task, Ultralytics YOLO11-seg, YOLOv9-seg and YOLOv5-seg models are also supported (since they have similar annotation format to v8).\n",
      "pip install -r requirements.txt\n",
      "How to run the code:\n",
      "Classic approach with pre-defined train/val/test split from CVAT (Tasks have a defined Subset in CVAT):\n",
      "Option with automatic split into train and val:\n",
      "python coco_to_yolo.py --coco_dataset=\"dataset_folder\" --autosplit=True --percent_val=30\n",
      "List of parameters with explanations that can be passed to the program before running it in the command line interface (CLI):\n",
      "--coco_dataset TEXT   Folder with COCO 1.0 format dataset (can be exported\n",
      "from CVAT). Default is \"COCO_dataset\"..., \n",
      "metadata={'source': 'chunk_4', 'distance': 0.8902299404144287, 'chunk_length': 1622, 'score': 0.95695716}\n",
      "\n",
      "Chunk: [Источник: GitHub - Koldim2001/COCO_to_YOLOv8: Converting COCO annotation (CVAT) to annotation for YOLO-seg (instance segmentation) and YOLO-obb (oriented bounding box detection)]\n",
      "python coco_to_yolo.py --coco_dataset=\"dataset_folder\" --autosplit=True --percent_val=30 --lang_ru=True\n",
      "Список параметров с пояснениями, которые можно передать на вход программы перед ее запуском в cli:\n",
      "--coco_dataset TEXT   Папка с датасетом формата COCO 1.0 (можно выгрузить из\n",
      "CVAT). По умолчанию \" COCO_dataset \" --yolo_dataset TEXT   Папка с итоговым датасетом формата YOLOv8. По\n",
      "умолчанию \" YOLO_dataset \" --print_info BOOLEAN  Вкл/Выкл режима вывода логов обработки. По умолчанию\n",
      "--autosplit BOOLEAN   Вкл/Выкл режима автоматического разделения на\n",
      "train/val. По умолчанию отключен (берет согласно\n",
      "--percent_val FLOAT   Процент данных на валидацию при выборе режима\n",
      "autosplit=True. По умолчанию 25%\n",
      "--lang_ru BOOLEAN     Устанавливает русский язык комментариев, если выбрано\n",
      "значение True. По умолчанию английский\n",
      "--help                Покажет существующие варианты парсинга аргументов в CLI..., \n",
      "metadata={'source': 'chunk_0', 'distance': 0.9058507680892944, 'chunk_length': 1077, 'score': 0.9302051}\n",
      "\n",
      "Chunk: [Источник: Segment - Ultralytics YOLO Docs]\n",
      "values are for single-model single-scale on\n",
      "yolo val segment data=coco.yaml device=0\n",
      "averaged over COCO val images using an\n",
      "yolo val segment data=coco.yaml batch=1 device=0|cpu\n",
      "Train YOLO11n-seg on the COCO8-seg dataset for 100\n",
      "at image size 640. For a full list of available arguments see the\n",
      "from ultralytics import YOLO # Load a model model = YOLO ( \"yolo11n-seg.yaml\" ) # build a new model from YAML model = YOLO ( \"yolo11n-seg.pt\" ) # load a pretrained model (recommended for training) model = YOLO ( \"yolo11n-seg.yaml\" ) . load ( \"yolo11n.pt\" ) # build from YAML and transfer weights # Train the model results = model . train ( data = \"coco8-seg.yaml\" , epochs = 100 , imgsz = 640 )\n",
      "# Build a new model from YAML and start training from scratch yolo segment train data = coco8-seg.yaml model = yolo11n-seg.yaml epochs = 100 imgsz = 640 # Start training from a pretrained *.pt model yolo segment train data = coco8-seg.yaml model = yolo11n-seg.pt epochs = 100 imgsz = 640 # Build a new model from YAML, transfer pretrained weights to it and start training yolo segment train data = coco8-seg.yaml model = yolo11n-seg.yaml pretrained = yolo11n-seg.pt epochs = 100 imgsz = 640\n",
      "YOLO segmentation dataset format can be found in detail in the\n",
      ". To convert your existing dataset from other formats (like COCO etc.) to YOLO format, please use\n",
      "Validate trained YOLO11n-seg model\n",
      "on the COCO8-seg dataset. No arguments are needed as the\n",
      "and arguments as model attributes...., \n",
      "metadata={'source': 'chunk_3', 'distance': 0.8920798301696777, 'chunk_length': 1511, 'score': 0.67308426}\n",
      "\n",
      "Chunk: [Источник: Detect - Ultralytics YOLO Docs]\n",
      "yolo val detect data=coco.yaml batch=1 device=0|cpu\n",
      "Train YOLO11n on the COCO8 dataset for 100\n",
      "at image size 640. For a full list of available arguments see the\n",
      "from ultralytics import YOLO # Load a model model = YOLO ( \"yolo11n.yaml\" ) # build a new model from YAML model = YOLO ( \"yolo11n.pt\" ) # load a pretrained model (recommended for training) model = YOLO ( \"yolo11n.yaml\" ) . load ( \"yolo11n.pt\" ) # build from YAML and transfer weights # Train the model results = model . train ( data = \"coco8.yaml\" , epochs = 100 , imgsz = 640 )\n",
      "# Build a new model from YAML and start training from scratch yolo detect train data = coco8.yaml model = yolo11n.yaml epochs = 100 imgsz = 640 # Start training from a pretrained *.pt model yolo detect train data = coco8.yaml model = yolo11n.pt epochs = 100 imgsz = 640 # Build a new model from YAML, transfer pretrained weights to it and start training yolo detect train data = coco8.yaml model = yolo11n.yaml pretrained = yolo11n.pt epochs = 100 imgsz = 640\n",
      "YOLO detection dataset format can be found in detail in the\n",
      ". To convert your existing dataset from other formats (like COCO etc.) to YOLO format, please use\n",
      "Validate trained YOLO11n model\n",
      "on the COCO8 dataset. No arguments are needed as the\n",
      "and arguments as model attributes...., \n",
      "metadata={'source': 'chunk_2', 'distance': 0.8970799446105957, 'chunk_length': 1319, 'score': 0.6399611}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "limit_size = 200\n",
    "for i, doc in enumerate(top_docs):\n",
    "    print(f\"Chunk: {doc.page_content[:limit_size]}..., \\nmetadata={doc.metadata}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'chunk_1', 'distance': 0.9030083417892456, 'chunk_length': 1674, 'score': 0.96989965}, page_content='[Источник: GitHub - Koldim2001/COCO_to_YOLOv8: Converting COCO annotation (CVAT) to annotation for YOLO-seg (instance segmentation) and YOLO-obb (oriented bounding box detection)]\\n--yolo_dataset TEXT   Folder with the resulting YOLOv8 format dataset.\\n--print_info BOOLEAN  Enable/Disable processing log output mode. Default is\\n--autosplit BOOLEAN   Enable/Disable automatic split into train/val. Default\\nis disabled (uses the CVAT annotations)\\n--percent_val FLOAT   Percentage of data for validation when using\\nautosplit=True. Default is 25%\\n--help                Show existing options for parsing arguments in the CLI\\nRussian Version of README:\\nРепозиторий позволяет преобразовать разметку формата COCO в формат, поддерживаемый для обучения моделей YOLOv8-seg (инстанс сегментация) и YOLOv8-obb (детекция повернутых боксов).\\nКлючевое применение репозитория -> работа с выгруженной разметкой\\nв случае с YOLOv8-obb) из приложения CVAT в формате COCO 1.0 (с указанием режима save images = True).\\nЕсли же используете без CVAT, то убедитесь перед запуском, что ваша папка с COCO датасетом имеет такую структуру:\\nPS: Для задчи инстанс сегментации имеется также поддержка моделей Ultralytics YOLO11-seg, YOLOv9-seg и YOLOv5-seg (так как у них аналогичная разметка с версией v8)\\nПример использования репозитория для задачи\\nпредставлен в видео на YouTube -\\nПример использования репозитория для задачи\\nпредставлен в видео на YouTube -\\npip install -r requirements.txt\\nКлассический подход c предустановленным в CVAT разделением на train/val/test (у тасок определен Subset):\\npython coco_to_yolo.py --coco_dataset=\"dataset_folder\" --lang_ru=True\\nВариант с авторазделением на train и val:'),\n",
       " Document(metadata={'source': 'chunk_4', 'distance': 0.8902299404144287, 'chunk_length': 1622, 'score': 0.95695716}, page_content='[Источник: GitHub - Koldim2001/COCO_to_YOLOv8: Converting COCO annotation (CVAT) to annotation for YOLO-seg (instance segmentation) and YOLO-obb (oriented bounding box detection)]\\nGitHub - Koldim2001/COCO_to_YOLOv8: Converting COCO annotation (CVAT) to annotation for YOLO-seg (instance segmentation) and YOLO-obb (oriented bounding box detection)\\nCOCO to YOLO converter for instance segmentation (YOLOv8-seg) and oriented bounding box detection (YOLOv8-obb)\\nThe repository allows converting annotations in COCO format to a format compatible with training YOLOv8-seg models (instance segmentation) and YOLOv8-obb models (rotated bounding box detection).\\nKey usage of the repository -> handling annotated\\nin the case of YOLOv8-obb) exported from the CVAT application in COCO 1.0 format (with the save images mode set to True).\\nIf you use it without CVAT, make sure that your COCO dataset folder has the following structure:\\nPS: For the instance segmentation task, Ultralytics YOLO11-seg, YOLOv9-seg and YOLOv5-seg models are also supported (since they have similar annotation format to v8).\\npip install -r requirements.txt\\nHow to run the code:\\nClassic approach with pre-defined train/val/test split from CVAT (Tasks have a defined Subset in CVAT):\\nOption with automatic split into train and val:\\npython coco_to_yolo.py --coco_dataset=\"dataset_folder\" --autosplit=True --percent_val=30\\nList of parameters with explanations that can be passed to the program before running it in the command line interface (CLI):\\n--coco_dataset TEXT   Folder with COCO 1.0 format dataset (can be exported\\nfrom CVAT). Default is \"COCO_dataset\"'),\n",
       " Document(metadata={'source': 'chunk_0', 'distance': 0.9058507680892944, 'chunk_length': 1077, 'score': 0.9302051}, page_content='[Источник: GitHub - Koldim2001/COCO_to_YOLOv8: Converting COCO annotation (CVAT) to annotation for YOLO-seg (instance segmentation) and YOLO-obb (oriented bounding box detection)]\\npython coco_to_yolo.py --coco_dataset=\"dataset_folder\" --autosplit=True --percent_val=30 --lang_ru=True\\nСписок параметров с пояснениями, которые можно передать на вход программы перед ее запуском в cli:\\n--coco_dataset TEXT   Папка с датасетом формата COCO 1.0 (можно выгрузить из\\nCVAT). По умолчанию \" COCO_dataset \" --yolo_dataset TEXT   Папка с итоговым датасетом формата YOLOv8. По\\nумолчанию \" YOLO_dataset \" --print_info BOOLEAN  Вкл/Выкл режима вывода логов обработки. По умолчанию\\n--autosplit BOOLEAN   Вкл/Выкл режима автоматического разделения на\\ntrain/val. По умолчанию отключен (берет согласно\\n--percent_val FLOAT   Процент данных на валидацию при выборе режима\\nautosplit=True. По умолчанию 25%\\n--lang_ru BOOLEAN     Устанавливает русский язык комментариев, если выбрано\\nзначение True. По умолчанию английский\\n--help                Покажет существующие варианты парсинга аргументов в CLI'),\n",
       " Document(metadata={'source': 'chunk_3', 'distance': 0.8920798301696777, 'chunk_length': 1511, 'score': 0.67308426}, page_content='[Источник: Segment - Ultralytics YOLO Docs]\\nvalues are for single-model single-scale on\\nyolo val segment data=coco.yaml device=0\\naveraged over COCO val images using an\\nyolo val segment data=coco.yaml batch=1 device=0|cpu\\nTrain YOLO11n-seg on the COCO8-seg dataset for 100\\nat image size 640. For a full list of available arguments see the\\nfrom ultralytics import YOLO # Load a model model = YOLO ( \"yolo11n-seg.yaml\" ) # build a new model from YAML model = YOLO ( \"yolo11n-seg.pt\" ) # load a pretrained model (recommended for training) model = YOLO ( \"yolo11n-seg.yaml\" ) . load ( \"yolo11n.pt\" ) # build from YAML and transfer weights # Train the model results = model . train ( data = \"coco8-seg.yaml\" , epochs = 100 , imgsz = 640 )\\n# Build a new model from YAML and start training from scratch yolo segment train data = coco8-seg.yaml model = yolo11n-seg.yaml epochs = 100 imgsz = 640 # Start training from a pretrained *.pt model yolo segment train data = coco8-seg.yaml model = yolo11n-seg.pt epochs = 100 imgsz = 640 # Build a new model from YAML, transfer pretrained weights to it and start training yolo segment train data = coco8-seg.yaml model = yolo11n-seg.yaml pretrained = yolo11n-seg.pt epochs = 100 imgsz = 640\\nYOLO segmentation dataset format can be found in detail in the\\n. To convert your existing dataset from other formats (like COCO etc.) to YOLO format, please use\\nValidate trained YOLO11n-seg model\\non the COCO8-seg dataset. No arguments are needed as the\\nand arguments as model attributes.'),\n",
       " Document(metadata={'source': 'chunk_2', 'distance': 0.8970799446105957, 'chunk_length': 1319, 'score': 0.6399611}, page_content='[Источник: Detect - Ultralytics YOLO Docs]\\nyolo val detect data=coco.yaml batch=1 device=0|cpu\\nTrain YOLO11n on the COCO8 dataset for 100\\nat image size 640. For a full list of available arguments see the\\nfrom ultralytics import YOLO # Load a model model = YOLO ( \"yolo11n.yaml\" ) # build a new model from YAML model = YOLO ( \"yolo11n.pt\" ) # load a pretrained model (recommended for training) model = YOLO ( \"yolo11n.yaml\" ) . load ( \"yolo11n.pt\" ) # build from YAML and transfer weights # Train the model results = model . train ( data = \"coco8.yaml\" , epochs = 100 , imgsz = 640 )\\n# Build a new model from YAML and start training from scratch yolo detect train data = coco8.yaml model = yolo11n.yaml epochs = 100 imgsz = 640 # Start training from a pretrained *.pt model yolo detect train data = coco8.yaml model = yolo11n.pt epochs = 100 imgsz = 640 # Build a new model from YAML, transfer pretrained weights to it and start training yolo detect train data = coco8.yaml model = yolo11n.yaml pretrained = yolo11n.pt epochs = 100 imgsz = 640\\nYOLO detection dataset format can be found in detail in the\\n. To convert your existing dataset from other formats (like COCO etc.) to YOLO format, please use\\nValidate trained YOLO11n model\\non the COCO8 dataset. No arguments are needed as the\\nand arguments as model attributes.')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Дмитрий\\AppData\\Local\\Temp\\ipykernel_22556\\2768345714.py:9: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  chat = ChatOpenAI(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Итоговый промпт, подаваемый на вход модели:\n",
      "system: Вы полезный помощник, который отвечает на вопросы на русском языке. Ваши ответы должны быть четкими, информативными и полезными.\n",
      "human: Учитывай информацию из этих отрывков текста если считаешь нужным:\n",
      "1. [Источник: GitHub - Koldim2001/COCO_to_YOLOv8: Converting COCO annotation (CVAT) to annotation for YOLO-seg (instance segmentation) and YOLO-obb (oriented bounding box detection)]\n",
      "--yolo_dataset TEXT   Folder with the resulting YOLOv8 format dataset.\n",
      "--print_info BOOLEAN  Enable/Disable processing log output mode. Default is\n",
      "--autosplit BOOLEAN   Enable/Disable automatic split into train/val. Default\n",
      "is disabled (uses the CVAT annotations)\n",
      "--percent_val FLOAT   Percentage of data for validation when using\n",
      "autosplit=True. Default is 25%\n",
      "--help                Show existing options for parsing arguments in the CLI\n",
      "Russian Version of README:\n",
      "Репозиторий позволяет преобразовать разметку формата COCO в формат, поддерживаемый для обучения моделей YOLOv8-seg (инстанс сегментация) и YOLOv8-obb (детекция повернутых боксов).\n",
      "Ключевое применение репозитория -> работа с выгруженной разметкой\n",
      "в случае с YOLOv8-obb) из приложения CVAT в формате COCO 1.0 (с указанием режима save images = True).\n",
      "Если же используете без CVAT, то убедитесь перед запуском, что ваша папка с COCO датасетом имеет такую структуру:\n",
      "PS: Для задчи инстанс сегментации имеется также поддержка моделей Ultralytics YOLO11-seg, YOLOv9-seg и YOLOv5-seg (так как у них аналогичная разметка с версией v8)\n",
      "Пример использования репозитория для задачи\n",
      "представлен в видео на YouTube -\n",
      "Пример использования репозитория для задачи\n",
      "представлен в видео на YouTube -\n",
      "pip install -r requirements.txt\n",
      "Классический подход c предустановленным в CVAT разделением на train/val/test (у тасок определен Subset):\n",
      "python coco_to_yolo.py --coco_dataset=\"dataset_folder\" --lang_ru=True\n",
      "Вариант с авторазделением на train и val:\n",
      "2. [Источник: GitHub - Koldim2001/COCO_to_YOLOv8: Converting COCO annotation (CVAT) to annotation for YOLO-seg (instance segmentation) and YOLO-obb (oriented bounding box detection)]\n",
      "GitHub - Koldim2001/COCO_to_YOLOv8: Converting COCO annotation (CVAT) to annotation for YOLO-seg (instance segmentation) and YOLO-obb (oriented bounding box detection)\n",
      "COCO to YOLO converter for instance segmentation (YOLOv8-seg) and oriented bounding box detection (YOLOv8-obb)\n",
      "The repository allows converting annotations in COCO format to a format compatible with training YOLOv8-seg models (instance segmentation) and YOLOv8-obb models (rotated bounding box detection).\n",
      "Key usage of the repository -> handling annotated\n",
      "in the case of YOLOv8-obb) exported from the CVAT application in COCO 1.0 format (with the save images mode set to True).\n",
      "If you use it without CVAT, make sure that your COCO dataset folder has the following structure:\n",
      "PS: For the instance segmentation task, Ultralytics YOLO11-seg, YOLOv9-seg and YOLOv5-seg models are also supported (since they have similar annotation format to v8).\n",
      "pip install -r requirements.txt\n",
      "How to run the code:\n",
      "Classic approach with pre-defined train/val/test split from CVAT (Tasks have a defined Subset in CVAT):\n",
      "Option with automatic split into train and val:\n",
      "python coco_to_yolo.py --coco_dataset=\"dataset_folder\" --autosplit=True --percent_val=30\n",
      "List of parameters with explanations that can be passed to the program before running it in the command line interface (CLI):\n",
      "--coco_dataset TEXT   Folder with COCO 1.0 format dataset (can be exported\n",
      "from CVAT). Default is \"COCO_dataset\"\n",
      "3. [Источник: GitHub - Koldim2001/COCO_to_YOLOv8: Converting COCO annotation (CVAT) to annotation for YOLO-seg (instance segmentation) and YOLO-obb (oriented bounding box detection)]\n",
      "python coco_to_yolo.py --coco_dataset=\"dataset_folder\" --autosplit=True --percent_val=30 --lang_ru=True\n",
      "Список параметров с пояснениями, которые можно передать на вход программы перед ее запуском в cli:\n",
      "--coco_dataset TEXT   Папка с датасетом формата COCO 1.0 (можно выгрузить из\n",
      "CVAT). По умолчанию \" COCO_dataset \" --yolo_dataset TEXT   Папка с итоговым датасетом формата YOLOv8. По\n",
      "умолчанию \" YOLO_dataset \" --print_info BOOLEAN  Вкл/Выкл режима вывода логов обработки. По умолчанию\n",
      "--autosplit BOOLEAN   Вкл/Выкл режима автоматического разделения на\n",
      "train/val. По умолчанию отключен (берет согласно\n",
      "--percent_val FLOAT   Процент данных на валидацию при выборе режима\n",
      "autosplit=True. По умолчанию 25%\n",
      "--lang_ru BOOLEAN     Устанавливает русский язык комментариев, если выбрано\n",
      "значение True. По умолчанию английский\n",
      "--help                Покажет существующие варианты парсинга аргументов в CLI\n",
      "4. [Источник: Segment - Ultralytics YOLO Docs]\n",
      "values are for single-model single-scale on\n",
      "yolo val segment data=coco.yaml device=0\n",
      "averaged over COCO val images using an\n",
      "yolo val segment data=coco.yaml batch=1 device=0|cpu\n",
      "Train YOLO11n-seg on the COCO8-seg dataset for 100\n",
      "at image size 640. For a full list of available arguments see the\n",
      "from ultralytics import YOLO # Load a model model = YOLO ( \"yolo11n-seg.yaml\" ) # build a new model from YAML model = YOLO ( \"yolo11n-seg.pt\" ) # load a pretrained model (recommended for training) model = YOLO ( \"yolo11n-seg.yaml\" ) . load ( \"yolo11n.pt\" ) # build from YAML and transfer weights # Train the model results = model . train ( data = \"coco8-seg.yaml\" , epochs = 100 , imgsz = 640 )\n",
      "# Build a new model from YAML and start training from scratch yolo segment train data = coco8-seg.yaml model = yolo11n-seg.yaml epochs = 100 imgsz = 640 # Start training from a pretrained *.pt model yolo segment train data = coco8-seg.yaml model = yolo11n-seg.pt epochs = 100 imgsz = 640 # Build a new model from YAML, transfer pretrained weights to it and start training yolo segment train data = coco8-seg.yaml model = yolo11n-seg.yaml pretrained = yolo11n-seg.pt epochs = 100 imgsz = 640\n",
      "YOLO segmentation dataset format can be found in detail in the\n",
      ". To convert your existing dataset from other formats (like COCO etc.) to YOLO format, please use\n",
      "Validate trained YOLO11n-seg model\n",
      "on the COCO8-seg dataset. No arguments are needed as the\n",
      "and arguments as model attributes.\n",
      "5. [Источник: Detect - Ultralytics YOLO Docs]\n",
      "yolo val detect data=coco.yaml batch=1 device=0|cpu\n",
      "Train YOLO11n on the COCO8 dataset for 100\n",
      "at image size 640. For a full list of available arguments see the\n",
      "from ultralytics import YOLO # Load a model model = YOLO ( \"yolo11n.yaml\" ) # build a new model from YAML model = YOLO ( \"yolo11n.pt\" ) # load a pretrained model (recommended for training) model = YOLO ( \"yolo11n.yaml\" ) . load ( \"yolo11n.pt\" ) # build from YAML and transfer weights # Train the model results = model . train ( data = \"coco8.yaml\" , epochs = 100 , imgsz = 640 )\n",
      "# Build a new model from YAML and start training from scratch yolo detect train data = coco8.yaml model = yolo11n.yaml epochs = 100 imgsz = 640 # Start training from a pretrained *.pt model yolo detect train data = coco8.yaml model = yolo11n.pt epochs = 100 imgsz = 640 # Build a new model from YAML, transfer pretrained weights to it and start training yolo detect train data = coco8.yaml model = yolo11n.yaml pretrained = yolo11n.pt epochs = 100 imgsz = 640\n",
      "YOLO detection dataset format can be found in detail in the\n",
      ". To convert your existing dataset from other formats (like COCO etc.) to YOLO format, please use\n",
      "Validate trained YOLO11n model\n",
      "on the COCO8 dataset. No arguments are needed as the\n",
      "and arguments as model attributes.\n",
      "\n",
      "Ответь на вопрос: Хочу конвертировать разметку из COCO в YOLO формат. У меня разметка лежит в папке data и еще мне нужно автоматическое сплитование. Пусть на трейне будет 60% данных. Напиши cli команду что мне нужна\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "# Устанавливаем API ключ и базовый URL\n",
    "openai_api_key = \"EMPTY\"  # Ключ не требуется, так как используется локальный сервер\n",
    "openai_api_base = \"http://localhost:8071/v1\"  # Адрес вашего локального API сервера\n",
    "\n",
    "# Создаем экземпляр ChatOpenAI с модифицированными параметрами\n",
    "chat = ChatOpenAI(\n",
    "    openai_api_key=openai_api_key,\n",
    "    openai_api_base=openai_api_base,\n",
    "    model_name=\"Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4\",  # Указываем имя модели\n",
    "    max_tokens=5000,  # Ограничиваем количество токенов в ответе\n",
    "    temperature=0  # Устанавливаем температуру (от 0 до 1, где 0 - детерминированный ответ, 1 - более случайный)\n",
    ")\n",
    "\n",
    "# Системный промпт, который задает контекст для модели\n",
    "system_prompt = SystemMessage(content=\"Вы полезный помощник, который отвечает на вопросы на русском языке. Ваши ответы должны быть четкими, информативными и полезными.\")\n",
    "\n",
    "# Добавляем три документа из top_3_docs как примеры\n",
    "examples = \"\\n\".join([f\"{i}. {doc.page_content}\" for i, doc in enumerate(top_docs, start=1)])\n",
    "\n",
    "if len(top_docs) > 0:\n",
    "    # Формируем итоговый промпт\n",
    "    human_message_content = (\n",
    "        f\"Учитывай информацию из этих отрывков текста если считаешь нужным:\\n\"\n",
    "        f\"{examples}\\n\\n\"\n",
    "        f\"Ответь на вопрос: {question}\"\n",
    "    )\n",
    "else:\n",
    "    # Формируем итоговый промпт\n",
    "    human_message_content = (\n",
    "        f\"Ответь на вопрос: {question}\\n\"\n",
    "        f\"Обязательно укажи, что отвечаешь без учета контекса с представленных сайтов, так как не нашел ничего релевантного\"\n",
    "    )\n",
    "    \n",
    "messages = [\n",
    "    system_prompt,  # Системный промпт\n",
    "    HumanMessage(content=human_message_content)  # Промпт пользователя с примерами\n",
    "]\n",
    "\n",
    "# Выводим итоговый промпт\n",
    "print(\"Итоговый промпт, подаваемый на вход модели:\")\n",
    "for message in messages:\n",
    "    print(f\"{message.type}: {message.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Вы полезный помощник, который отвечает на вопросы на русском языке. Ваши ответы должны быть четкими, информативными и полезными.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Учитывай информацию из этих отрывков текста если считаешь нужным:\\n1. [Источник: GitHub - Koldim2001/COCO_to_YOLOv8: Converting COCO annotation (CVAT) to annotation for YOLO-seg (instance segmentation) and YOLO-obb (oriented bounding box detection)]\\n--yolo_dataset TEXT   Folder with the resulting YOLOv8 format dataset.\\n--print_info BOOLEAN  Enable/Disable processing log output mode. Default is\\n--autosplit BOOLEAN   Enable/Disable automatic split into train/val. Default\\nis disabled (uses the CVAT annotations)\\n--percent_val FLOAT   Percentage of data for validation when using\\nautosplit=True. Default is 25%\\n--help                Show existing options for parsing arguments in the CLI\\nRussian Version of README:\\nРепозиторий позволяет преобразовать разметку формата COCO в формат, поддерживаемый для обучения моделей YOLOv8-seg (инстанс сегментация) и YOLOv8-obb (детекция повернутых боксов).\\nКлючевое применение репозитория -> работа с выгруженной разметкой\\nв случае с YOLOv8-obb) из приложения CVAT в формате COCO 1.0 (с указанием режима save images = True).\\nЕсли же используете без CVAT, то убедитесь перед запуском, что ваша папка с COCO датасетом имеет такую структуру:\\nPS: Для задчи инстанс сегментации имеется также поддержка моделей Ultralytics YOLO11-seg, YOLOv9-seg и YOLOv5-seg (так как у них аналогичная разметка с версией v8)\\nПример использования репозитория для задачи\\nпредставлен в видео на YouTube -\\nПример использования репозитория для задачи\\nпредставлен в видео на YouTube -\\npip install -r requirements.txt\\nКлассический подход c предустановленным в CVAT разделением на train/val/test (у тасок определен Subset):\\npython coco_to_yolo.py --coco_dataset=\"dataset_folder\" --lang_ru=True\\nВариант с авторазделением на train и val:\\n2. [Источник: GitHub - Koldim2001/COCO_to_YOLOv8: Converting COCO annotation (CVAT) to annotation for YOLO-seg (instance segmentation) and YOLO-obb (oriented bounding box detection)]\\nGitHub - Koldim2001/COCO_to_YOLOv8: Converting COCO annotation (CVAT) to annotation for YOLO-seg (instance segmentation) and YOLO-obb (oriented bounding box detection)\\nCOCO to YOLO converter for instance segmentation (YOLOv8-seg) and oriented bounding box detection (YOLOv8-obb)\\nThe repository allows converting annotations in COCO format to a format compatible with training YOLOv8-seg models (instance segmentation) and YOLOv8-obb models (rotated bounding box detection).\\nKey usage of the repository -> handling annotated\\nin the case of YOLOv8-obb) exported from the CVAT application in COCO 1.0 format (with the save images mode set to True).\\nIf you use it without CVAT, make sure that your COCO dataset folder has the following structure:\\nPS: For the instance segmentation task, Ultralytics YOLO11-seg, YOLOv9-seg and YOLOv5-seg models are also supported (since they have similar annotation format to v8).\\npip install -r requirements.txt\\nHow to run the code:\\nClassic approach with pre-defined train/val/test split from CVAT (Tasks have a defined Subset in CVAT):\\nOption with automatic split into train and val:\\npython coco_to_yolo.py --coco_dataset=\"dataset_folder\" --autosplit=True --percent_val=30\\nList of parameters with explanations that can be passed to the program before running it in the command line interface (CLI):\\n--coco_dataset TEXT   Folder with COCO 1.0 format dataset (can be exported\\nfrom CVAT). Default is \"COCO_dataset\"\\n3. [Источник: GitHub - Koldim2001/COCO_to_YOLOv8: Converting COCO annotation (CVAT) to annotation for YOLO-seg (instance segmentation) and YOLO-obb (oriented bounding box detection)]\\npython coco_to_yolo.py --coco_dataset=\"dataset_folder\" --autosplit=True --percent_val=30 --lang_ru=True\\nСписок параметров с пояснениями, которые можно передать на вход программы перед ее запуском в cli:\\n--coco_dataset TEXT   Папка с датасетом формата COCO 1.0 (можно выгрузить из\\nCVAT). По умолчанию \" COCO_dataset \" --yolo_dataset TEXT   Папка с итоговым датасетом формата YOLOv8. По\\nумолчанию \" YOLO_dataset \" --print_info BOOLEAN  Вкл/Выкл режима вывода логов обработки. По умолчанию\\n--autosplit BOOLEAN   Вкл/Выкл режима автоматического разделения на\\ntrain/val. По умолчанию отключен (берет согласно\\n--percent_val FLOAT   Процент данных на валидацию при выборе режима\\nautosplit=True. По умолчанию 25%\\n--lang_ru BOOLEAN     Устанавливает русский язык комментариев, если выбрано\\nзначение True. По умолчанию английский\\n--help                Покажет существующие варианты парсинга аргументов в CLI\\n4. [Источник: Segment - Ultralytics YOLO Docs]\\nvalues are for single-model single-scale on\\nyolo val segment data=coco.yaml device=0\\naveraged over COCO val images using an\\nyolo val segment data=coco.yaml batch=1 device=0|cpu\\nTrain YOLO11n-seg on the COCO8-seg dataset for 100\\nat image size 640. For a full list of available arguments see the\\nfrom ultralytics import YOLO # Load a model model = YOLO ( \"yolo11n-seg.yaml\" ) # build a new model from YAML model = YOLO ( \"yolo11n-seg.pt\" ) # load a pretrained model (recommended for training) model = YOLO ( \"yolo11n-seg.yaml\" ) . load ( \"yolo11n.pt\" ) # build from YAML and transfer weights # Train the model results = model . train ( data = \"coco8-seg.yaml\" , epochs = 100 , imgsz = 640 )\\n# Build a new model from YAML and start training from scratch yolo segment train data = coco8-seg.yaml model = yolo11n-seg.yaml epochs = 100 imgsz = 640 # Start training from a pretrained *.pt model yolo segment train data = coco8-seg.yaml model = yolo11n-seg.pt epochs = 100 imgsz = 640 # Build a new model from YAML, transfer pretrained weights to it and start training yolo segment train data = coco8-seg.yaml model = yolo11n-seg.yaml pretrained = yolo11n-seg.pt epochs = 100 imgsz = 640\\nYOLO segmentation dataset format can be found in detail in the\\n. To convert your existing dataset from other formats (like COCO etc.) to YOLO format, please use\\nValidate trained YOLO11n-seg model\\non the COCO8-seg dataset. No arguments are needed as the\\nand arguments as model attributes.\\n5. [Источник: Detect - Ultralytics YOLO Docs]\\nyolo val detect data=coco.yaml batch=1 device=0|cpu\\nTrain YOLO11n on the COCO8 dataset for 100\\nat image size 640. For a full list of available arguments see the\\nfrom ultralytics import YOLO # Load a model model = YOLO ( \"yolo11n.yaml\" ) # build a new model from YAML model = YOLO ( \"yolo11n.pt\" ) # load a pretrained model (recommended for training) model = YOLO ( \"yolo11n.yaml\" ) . load ( \"yolo11n.pt\" ) # build from YAML and transfer weights # Train the model results = model . train ( data = \"coco8.yaml\" , epochs = 100 , imgsz = 640 )\\n# Build a new model from YAML and start training from scratch yolo detect train data = coco8.yaml model = yolo11n.yaml epochs = 100 imgsz = 640 # Start training from a pretrained *.pt model yolo detect train data = coco8.yaml model = yolo11n.pt epochs = 100 imgsz = 640 # Build a new model from YAML, transfer pretrained weights to it and start training yolo detect train data = coco8.yaml model = yolo11n.yaml pretrained = yolo11n.pt epochs = 100 imgsz = 640\\nYOLO detection dataset format can be found in detail in the\\n. To convert your existing dataset from other formats (like COCO etc.) to YOLO format, please use\\nValidate trained YOLO11n model\\non the COCO8 dataset. No arguments are needed as the\\nand arguments as model attributes.\\n\\nОтветь на вопрос: Хочу конвертировать разметку из COCO в YOLO формат. У меня разметка лежит в папке data и еще мне нужно автоматическое сплитование. Пусть на трейне будет 60% данных. Напиши cli команду что мне нужна', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Дмитрий\\AppData\\Local\\Temp\\ipykernel_22556\\2283460295.py:2: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = chat(messages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ответ модели:\n",
      "Для вашего случая, когда у вас есть разметка в формате COCO, которая находится в папке `data`, и вы хотите автоматически разделить данные на тренировочную и валидационную выборки с соотношением 60% на трейн и 40% на валидацию, вы можете использовать следующую команду:\n",
      "\n",
      "```bash\n",
      "python coco_to_yolo.py --coco_dataset=\"data\" --autosplit=True --percent_val=40 --lang_ru=True\n",
      "```\n",
      "\n",
      "Объяснение параметров:\n",
      "- `--coco_dataset=\"data\"`: Указывает путь к папке с вашим датасетом COCO.\n",
      "- `--autosplit=True`: Включает автоматическое разделение данных на тренировочную и валидационную выборки.\n",
      "- `--percent_val=40`: Указывает, что 40% данных будут использованы для валидации.\n",
      "- `--lang_ru=True`: Устанавливает русский язык для вывода сообщений.\n",
      "\n",
      "Эта команда преобразует вашу разметку из COCO в формат YOLOv8 и автоматически создаст тренировочную и валидационную выборки с указанным соотношением.\n"
     ]
    }
   ],
   "source": [
    "# Получаем ответ от модели\n",
    "response = chat(messages)\n",
    "\n",
    "# Выводим ответ\n",
    "print(\"\\nОтвет модели:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 19:44:26,830 - nodes.VectorDBNode - INFO - Connected to Milvus at localhost:19530 successfully!\n",
      "2025-02-18 19:44:26,830 - nodes.DataParsingNode - INFO - Парсинг https://www.eurochem.ru/...\n",
      "2025-02-18 19:44:26,974 - nodes.DataParsingNode - INFO - Парсинг https://www.eurochem.ru/global-operations/...\n",
      "2025-02-18 19:44:27,128 - nodes.DataParsingNode - INFO - Парсинг https://www.eurochem.ru/about-us/komplaens/...\n",
      "2025-02-18 19:44:27,250 - nodes.DataParsingNode - INFO - Парсинг https://www.eurochem.ru/proteh-lab/...\n",
      "2025-02-18 19:44:27,480 - nodes.DataParsingNode - INFO - Парсинг https://digtp.com/...\n",
      "2025-02-18 19:44:27,596 - nodes.DataParsingNode - INFO - Парсинг https://digtp.com/projects/machine-learning-platforma...\n",
      "2025-02-18 19:44:27,710 - nodes.DataParsingNode - INFO - Парсинг https://digtp.com/projects/rekomendatelnye-modeli...\n",
      "2025-02-18 19:44:27,799 - nodes.DataParsingNode - INFO - Парсинг https://digtp.com/projects/mobilnoe-prilozenie-mineralogiia...\n",
      "2025-02-18 19:44:27,888 - nodes.DataParsingNode - INFO - Парсинг https://digtp.com/contacts...\n",
      "2025-02-18 19:44:28,007 - nodes.DataParsingNode - INFO - Парсинг https://www.eurochem-career.com/news/iskusstvennyi-intellekt-v-ximii-gpt-assistenty-v-evroxime...\n",
      "2025-02-18 19:44:28,179 - nodes.DataParsingNode - INFO - Парсинг https://otus.ru/instructors/10517...\n",
      "2025-02-18 19:44:31,070 - nodes.DataParsingNode - INFO - Парсинг https://ru.wikipedia.org/wiki/ЕвроХим...\n",
      "2025-02-18 19:44:31,341 - nodes.DataParsingNode - INFO - Парсинг https://www.eurochem.ru/usolskij-kalijnyj-kombinat/...\n",
      "2025-02-18 19:44:31,482 - nodes.DataParsingNode - INFO - Парсинг https://uralmines.ru/evrohim-usolskij-kalijnyj-kombinat/...\n",
      "2025-02-18 19:44:32,242 - nodes.DataParsingNode - INFO - Парсинг https://docs.ultralytics.com/tasks/segment...\n",
      "2025-02-18 19:44:33,056 - nodes.DataParsingNode - INFO - Парсинг https://docs.ultralytics.com/tasks/detect...\n",
      "2025-02-18 19:44:33,538 - nodes.DataParsingNode - INFO - Парсинг https://docs.ultralytics.com/tasks...\n",
      "2025-02-18 19:44:34,162 - nodes.DataParsingNode - INFO - Парсинг https://docs.ultralytics.com/modes/...\n",
      "2025-02-18 19:44:34,620 - nodes.DataParsingNode - INFO - Парсинг https://docs.ultralytics.com/solutions...\n",
      "2025-02-18 19:44:35,098 - nodes.DataParsingNode - INFO - Парсинг https://github.com/Koldim2001...\n",
      "2025-02-18 19:44:35,746 - nodes.DataParsingNode - INFO - Парсинг https://github.com/Koldim2001/YOLO-Patch-Based-Inference...\n",
      "2025-02-18 19:44:36,559 - nodes.DataParsingNode - INFO - Парсинг https://github.com/Koldim2001/TrafficAnalyzer...\n",
      "2025-02-18 19:44:37,424 - nodes.DataParsingNode - INFO - Парсинг https://github.com/Koldim2001/COCO_to_YOLOv8...\n",
      "2025-02-18 19:44:38,222 - nodes.EmbedderNode - INFO - Обработка батча 1, размер батча: 156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Чанки сохранены в файл results/chunks_output.txt\n",
      "Итоговый текст сохранен в файл results/output_parsing.txt\n",
      "DataElement(\n",
      "  Сollection db name: dta\n",
      "  URLs: https://www.eurochem.ru/, https://www.eurochem.ru/global-operations/, https://www.eurochem.ru/about-us/komplaens/, https://www.eurochem.ru/proteh-lab/, https://digtp.com/, https://digtp.com/projects/machine-learning-platforma, https://digtp.com/projects/rekomendatelnye-modeli, https://digtp.com/projects/mobilnoe-prilozenie-mineralogiia, https://digtp.com/contacts, https://www.eurochem-career.com/news/iskusstvennyi-intellekt-v-ximii-gpt-assistenty-v-evroxime, https://otus.ru/instructors/10517, https://ru.wikipedia.org/wiki/ЕвроХим, https://www.eurochem.ru/usolskij-kalijnyj-kombinat/, https://uralmines.ru/evrohim-usolskij-kalijnyj-kombinat/, https://docs.ultralytics.com/tasks/segment, https://docs.ultralytics.com/tasks/detect, https://docs.ultralytics.com/tasks, https://docs.ultralytics.com/modes/, https://docs.ultralytics.com/solutions, https://github.com/Koldim2001, https://github.com/Koldim2001/YOLO-Patch-Based-Inference, https://github.com/Koldim2001/TrafficAnalyzer, https://github.com/Koldim2001/COCO_to_YOLOv8\n",
      "  URL Data: 23 entries\n",
      "  Chunks: 156 chunks\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 19:44:39,688 - nodes.VectorDBNode - INFO - Collection 'dta' deleted successfully.\n",
      "2025-02-18 19:44:39,722 - nodes.VectorDBNode - INFO - Collection 'dta' created successfully.\n",
      "2025-02-18 19:44:40,242 - nodes.VectorDBNode - INFO - Index created on field 'embedding' for collection 'dta'.\n",
      "2025-02-18 19:44:40,880 - nodes.VectorDBNode - INFO - Collection 'dta' loaded successfully.\n",
      "2025-02-18 19:44:41,041 - nodes.VectorDBNode - INFO - Inserted 156 records into collection 'dta'.\n"
     ]
    }
   ],
   "source": [
    "from services.MakeDatasetRAG import MakeDatasetRAG\n",
    "\n",
    "mk = MakeDatasetRAG()\n",
    "\n",
    "url_list = [\n",
    "    \"https://www.eurochem.ru/\",\n",
    "    \"https://www.eurochem.ru/global-operations/\",\n",
    "    \"https://www.eurochem.ru/about-us/komplaens/\",\n",
    "    \"https://www.eurochem.ru/proteh-lab/\",\n",
    "    \"https://digtp.com/\",\n",
    "    \"https://digtp.com/projects/machine-learning-platforma\",\n",
    "    \"https://digtp.com/projects/rekomendatelnye-modeli\",\n",
    "    \"https://digtp.com/projects/mobilnoe-prilozenie-mineralogiia\",\n",
    "    \"https://digtp.com/contacts\",\n",
    "    \"https://www.eurochem-career.com/news/iskusstvennyi-intellekt-v-ximii-gpt-assistenty-v-evroxime\",\n",
    "    \"https://otus.ru/instructors/10517\",\n",
    "    \"https://ru.wikipedia.org/wiki/ЕвроХим\",\n",
    "    \"https://www.eurochem.ru/usolskij-kalijnyj-kombinat/\",\n",
    "    \"https://uralmines.ru/evrohim-usolskij-kalijnyj-kombinat/\",\n",
    "    \"https://docs.ultralytics.com/tasks/segment\",\n",
    "    \"https://docs.ultralytics.com/tasks/detect\",\n",
    "    \"https://docs.ultralytics.com/tasks\",\n",
    "    \"https://docs.ultralytics.com/modes/\",\n",
    "    \"https://docs.ultralytics.com/solutions\",\n",
    "    \"https://github.com/Koldim2001\",\n",
    "    \"https://github.com/Koldim2001/YOLO-Patch-Based-Inference\",\n",
    "    \"https://github.com/Koldim2001/TrafficAnalyzer\",\n",
    "    \"https://github.com/Koldim2001/COCO_to_YOLOv8\"\n",
    "]\n",
    "\n",
    "collection_db_name = \"dta\"\n",
    "\n",
    "mk.process(url_list, collection_db_name, show_data_info=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 19:44:58,213 - nodes.VectorDBNode - INFO - First 5 records in collection 'dta':\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 19:44:58,214 - nodes.VectorDBNode - INFO - ID: 456101818772499929, Text: [Источник: АО «Минерально-химическая компания Евро..., Length: 1023, Timestamp: 1739897080\n",
      "2025-02-18 19:44:58,214 - nodes.VectorDBNode - INFO - ID: 456101818772499930, Text: [Источник: Наши активы - добыча, производство, про..., Length: 1229, Timestamp: 1739897080\n",
      "2025-02-18 19:44:58,215 - nodes.VectorDBNode - INFO - ID: 456101818772499931, Text: [Источник: Наши активы - добыча, производство, про..., Length: 1226, Timestamp: 1739897080\n",
      "2025-02-18 19:44:58,215 - nodes.VectorDBNode - INFO - ID: 456101818772499932, Text: [Источник: Наши активы - добыча, производство, про..., Length: 678, Timestamp: 1739897080\n",
      "2025-02-18 19:44:58,215 - nodes.VectorDBNode - INFO - ID: 456101818772499933, Text: [Источник: Комплаенс]\n",
      "Свяжитесь с нами\n",
      "Удобрения и..., Length: 1204, Timestamp: 1739897080\n"
     ]
    }
   ],
   "source": [
    "mk.vector_db_node.display_first_n_records(collection_db_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 19:45:00,175 - nodes.VectorDBNode - INFO - Collection 'dta' already exists.\n"
     ]
    }
   ],
   "source": [
    "mk.vector_db_node.create_milvus_collection(collection_db_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mk.vector_db_node.get_total_records(collection_db_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 14:16:57,654 - nodes.VectorDBNode - INFO - Connected to Milvus at localhost:19530 successfully!\n",
      "c:\\МГТУ НАУКА\\ПРОЕКТЫ GIT для ДУШИ\\RAG_LLM\\nodes\\LLMNode.py:24: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  self.chat = ChatOpenAI(\n"
     ]
    }
   ],
   "source": [
    "from services.AskLLM import AskLLM\n",
    "ask = AskLLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "query= \"что такое ультралитикс\"\n",
    "message_number=0\n",
    "collection_db_name='datya'\n",
    "previous_messages=[]\n",
    "show_data_info=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\МГТУ НАУКА\\ПРОЕКТЫ GIT для ДУШИ\\RAG_LLM\\nodes\\LLMNode.py:79: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = self.chat(messages)\n",
      "2025-02-19 14:17:02,420 - httpx - INFO - HTTP Request: POST http://localhost:8071/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "ask.process(query, message_number, collection_db_name, previous_messages, show_data_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MAIN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
