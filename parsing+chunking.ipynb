{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing + Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Парсинг веб-страниц:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def parse_url(url):\n",
    "    \"\"\"\n",
    "    Парсит содержимое указанного URL и возвращает текстовое содержимое страницы.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL-адрес для парсинга.\n",
    "\n",
    "    Returns:\n",
    "        dict: Словарь с ключами 'text' (текстовая информация) и 'description' (краткое описание страницы).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Извлекаем краткое описание (title страницы)\n",
    "        description = soup.title.string.strip() if soup.title else url\n",
    "\n",
    "        # Обработка контента\n",
    "        soup_copy = BeautifulSoup(response.text, \"html.parser\")\n",
    "        for tag in soup_copy.find_all(['pre', 'code']):\n",
    "            tag.replace_with(tag.get_text(separator=\" \", strip=True))\n",
    "        final_text = soup_copy.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "        return {'text': final_text, 'description': description}\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Ошибка при загрузке страницы {url}: {e}\")\n",
    "        return {'text': '', 'description': url}\n",
    "\n",
    "\n",
    "def filter_text(text, min_words=3):\n",
    "    \"\"\"\n",
    "    Фильтрует текст, удаляя строки с минимальным количеством слов и стоп-фразы.\n",
    "\n",
    "    Args:\n",
    "        text (str): Исходный текст.\n",
    "        min_words (int): Минимальное количество слов в строке для сохранения.\n",
    "\n",
    "    Returns:\n",
    "        str: Отфильтрованный текст.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return ''\n",
    "\n",
    "    lines = text.split(\"\\n\")\n",
    "    filtered_text = [line.strip() for line in lines if len(line.split()) > min_words]\n",
    "\n",
    "    # Поиск стоп-фраз\n",
    "    stop_phrases = [\n",
    "        \"Политика в отношении файлов cookie\",\n",
    "        \"Мы используем cookie\",\n",
    "        \"Дата обращения:\",\n",
    "        \"Использованная литература и источники:\"\n",
    "    ]\n",
    "    cutoff_index = next((i for i, line in enumerate(filtered_text) if any(phrase in line for phrase in stop_phrases)), None)\n",
    "\n",
    "    if cutoff_index is not None:\n",
    "        filtered_text = filtered_text[:cutoff_index]\n",
    "\n",
    "    return \"\\n\".join(filtered_text)\n",
    "\n",
    "\n",
    "def save_to_file(filename, content):\n",
    "    \"\"\"\n",
    "    Сохраняет текстовое содержимое в указанный файл.\n",
    "\n",
    "    Args:\n",
    "        filename (str): Имя файла для сохранения.\n",
    "        content (str): Текстовое содержимое для записи.\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(content)\n",
    "\n",
    "\n",
    "def load_from_file(filename):\n",
    "    \"\"\"\n",
    "    Загружает текстовое содержимое из указанного файла.\n",
    "\n",
    "    Args:\n",
    "        filename (str): Имя файла для чтения.\n",
    "\n",
    "    Returns:\n",
    "        str: Текстовое содержимое файла.\n",
    "    \"\"\"\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "        return file.read()\n",
    "\n",
    "\n",
    "def generate_chunks(loaded_text, url_data, chunk_size=1500, chunk_overlap=0):\n",
    "    \"\"\"\n",
    "    Генерирует чанки из загруженного текста, добавляя описание источника в начало каждого чанка.\n",
    "\n",
    "    Args:\n",
    "        loaded_text (str): Загруженный текст из файла.\n",
    "        url_data (dict): Словарь с описаниями страниц.\n",
    "        chunk_size (int): Максимальный размер чанка.\n",
    "        chunk_overlap (int): Перекрытие между чанками.\n",
    "\n",
    "    Returns:\n",
    "        list: Список отформатированных чанков.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \".\"],\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "\n",
    "    all_chunks = []\n",
    "    chunks = text_splitter.split_text(loaded_text)\n",
    "\n",
    "    # Распределение чанков по источникам (простая эвристика)\n",
    "    source_descriptions = list(url_data.values())\n",
    "\n",
    "    for i, text in enumerate(loaded_text.split('\\n\\n\\n')):\n",
    "        chunks = text_splitter.split_text(text)\n",
    "        for chunk in chunks:\n",
    "            source_description = source_descriptions[i]['description']\n",
    "            formatted_chunk = f\"[Источник: {source_description}]\\n{chunk}\"\n",
    "            all_chunks.append(formatted_chunk)\n",
    "\n",
    "    return all_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Парсинг https://www.eurochem.ru/...\n",
      "Парсинг https://www.eurochem.ru/global-operations/...\n",
      "Парсинг https://www.eurochem.ru/about-us/komplaens/...\n",
      "Парсинг https://www.eurochem.ru/proteh-lab/...\n",
      "Парсинг https://digtp.com/...\n",
      "Парсинг https://digtp.com/projects/machine-learning-platforma...\n",
      "Парсинг https://digtp.com/projects/rekomendatelnye-modeli...\n",
      "Парсинг https://digtp.com/projects/mobilnoe-prilozenie-mineralogiia...\n",
      "Парсинг https://digtp.com/contacts...\n",
      "Парсинг https://www.eurochem-career.com/news/iskusstvennyi-intellekt-v-ximii-gpt-assistenty-v-evroxime...\n",
      "Парсинг https://otus.ru/instructors/10517...\n",
      "Парсинг https://ru.wikipedia.org/wiki/ЕвроХим...\n",
      "Парсинг https://www.eurochem.ru/usolskij-kalijnyj-kombinat/...\n",
      "Парсинг https://uralmines.ru/evrohim-usolskij-kalijnyj-kombinat/...\n",
      "Парсинг https://docs.ultralytics.com/tasks/segment...\n",
      "Парсинг https://docs.ultralytics.com/tasks/detect...\n",
      "Парсинг https://docs.ultralytics.com/tasks...\n",
      "Парсинг https://docs.ultralytics.com/modes/...\n",
      "Парсинг https://docs.ultralytics.com/solutions...\n",
      "Парсинг https://github.com/Koldim2001...\n",
      "Парсинг https://github.com/Koldim2001/YOLO-Patch-Based-Inference...\n",
      "Парсинг https://github.com/Koldim2001/TrafficAnalyzer...\n",
      "Парсинг https://github.com/Koldim2001/COCO_to_YOLOv8...\n"
     ]
    }
   ],
   "source": [
    "# Список URL-адресов\n",
    "urls = [\n",
    "    \"https://www.eurochem.ru/\",\n",
    "    \"https://www.eurochem.ru/global-operations/\",\n",
    "    \"https://www.eurochem.ru/about-us/komplaens/\",\n",
    "    \"https://www.eurochem.ru/proteh-lab/\",\n",
    "    \"https://digtp.com/\",\n",
    "    \"https://digtp.com/projects/machine-learning-platforma\",\n",
    "    \"https://digtp.com/projects/rekomendatelnye-modeli\",\n",
    "    \"https://digtp.com/projects/mobilnoe-prilozenie-mineralogiia\",\n",
    "    \"https://digtp.com/contacts\",\n",
    "    \"https://www.eurochem-career.com/news/iskusstvennyi-intellekt-v-ximii-gpt-assistenty-v-evroxime\",\n",
    "    \"https://otus.ru/instructors/10517\",\n",
    "    \"https://ru.wikipedia.org/wiki/ЕвроХим\",\n",
    "    \"https://www.eurochem.ru/usolskij-kalijnyj-kombinat/\",\n",
    "    \"https://uralmines.ru/evrohim-usolskij-kalijnyj-kombinat/\",\n",
    "    \"https://docs.ultralytics.com/tasks/segment\",\n",
    "    \"https://docs.ultralytics.com/tasks/detect\",\n",
    "    \"https://docs.ultralytics.com/tasks\",\n",
    "    \"https://docs.ultralytics.com/modes/\",\n",
    "    \"https://docs.ultralytics.com/solutions\",\n",
    "    \"https://github.com/Koldim2001\",\n",
    "    \"https://github.com/Koldim2001/YOLO-Patch-Based-Inference\",\n",
    "    \"https://github.com/Koldim2001/TrafficAnalyzer\",\n",
    "    \"https://github.com/Koldim2001/COCO_to_YOLOv8\"\n",
    "]\n",
    "\n",
    "# Словарь с описаниями и текстами\n",
    "url_data = {}\n",
    "\n",
    "# Парсинг и фильтрация\n",
    "for url in urls:\n",
    "    print(f\"Парсинг {url}...\")\n",
    "    parsed_data = parse_url(url)\n",
    "    if parsed_data['text']:\n",
    "        filtered_text = filter_text(parsed_data['text'])\n",
    "        url_data[url] = {\n",
    "            'text': filtered_text,\n",
    "            'description': parsed_data['description']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Итоговый текст сохранен в файл output_parsing.txt\n"
     ]
    }
   ],
   "source": [
    "# Сохранение объединенного текста\n",
    "combined_text = \"\\n\\n\\n\".join([data['text'] for data in url_data.values()])\n",
    "save_to_file(\"output_parsing.txt\", combined_text)\n",
    "print(\"Итоговый текст сохранен в файл output_parsing.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Чанки сохранены в файл chunks_output.txt\n"
     ]
    }
   ],
   "source": [
    "# Загрузка текста для чанкинга\n",
    "loaded_text = load_from_file(\"result_parsing.txt\")\n",
    "\n",
    "# Генерация чанков\n",
    "all_chunks = generate_chunks(loaded_text, url_data, chunk_size=1500, chunk_overlap=0)\n",
    "\n",
    "# Сохранение чанков\n",
    "with open(\"chunks_output.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for i, chunk in enumerate(all_chunks):\n",
    "        file.write(f\"Чанк {i+1} ({len(chunk)} символов):\\n{chunk}\\n{'='*50}\\n\")\n",
    "print(\"Чанки сохранены в файл chunks_output.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Производим сохранение в БД векторов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
    "import numpy as np\n",
    "from langchain.embeddings.base import Embeddings\n",
    "import requests\n",
    "from typing import List\n",
    "\n",
    "# Подключение к Milvus\n",
    "def connect_to_milvus(host=\"localhost\", port=\"19530\"):\n",
    "    \"\"\"\n",
    "    Устанавливает соединение с Milvus.\n",
    "\n",
    "    Args:\n",
    "        host (str): Хост Milvus.\n",
    "        port (str): Порт Milvus.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(f\"Connecting to Milvus at {host}:{port}...\")\n",
    "    connections.connect(\"default\", host=host, port=port)\n",
    "    print(\"Connected successfully!\")\n",
    "\n",
    "\n",
    "# Создание коллекции в Milvus (с проверкой существования)\n",
    "def create_milvus_collection(collection_name, dim):\n",
    "    \"\"\"\n",
    "    Создает новую коллекцию в Milvus, если она еще не существует.\n",
    "\n",
    "    Args:\n",
    "        collection_name (str): Имя коллекции.\n",
    "        dim (int): Размерность векторов.\n",
    "\n",
    "    Returns:\n",
    "        Collection: Экземпляр коллекции.\n",
    "    \"\"\"\n",
    "    # Проверяем, существует ли коллекция\n",
    "    if utility.has_collection(collection_name) and False:\n",
    "        print(f\"Collection '{collection_name}' already exists. Loading the collection...\")\n",
    "        collection = Collection(name=collection_name)\n",
    "    else:\n",
    "        # Определение полей коллекции\n",
    "        fields = [\n",
    "            FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "            FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=dim),\n",
    "            FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=65535),\n",
    "            FieldSchema(name=\"chunk_length\", dtype=DataType.INT64)\n",
    "        ]\n",
    "        schema = CollectionSchema(fields, description=\"Collection for text chunks\")\n",
    "        collection = Collection(name=collection_name, schema=schema)\n",
    "        print(f\"Collection '{collection_name}' created successfully.\")\n",
    "    \n",
    "    return collection\n",
    "\n",
    "\n",
    "# Генерация векторов для чанков\n",
    "def generate_embeddings(chunks, embedder):\n",
    "    \"\"\"\n",
    "    Генерирует векторные представления для списка чанков.\n",
    "\n",
    "    Args:\n",
    "        chunks (list): Список текстовых чанков.\n",
    "        embedder (callable): Функция или модель для генерации эмбеддингов.\n",
    "\n",
    "    Returns:\n",
    "        list: Список векторных представлений.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    for chunk in chunks:\n",
    "        embedding = embedder(chunk)  # Предполагается, что embedder принимает строку и возвращает вектор\n",
    "        embeddings.append(embedding)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "class CustomEmbedder(Embeddings):\n",
    "    def __init__(self, embedder_url=\"http://localhost:8080/embed\"):\n",
    "        self.embedder_url = embedder_url\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Получает эмбеддинги для списка текстов.\"\"\"\n",
    "        response = requests.post(\n",
    "            self.embedder_url,\n",
    "            json={\"inputs\": texts}\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            raise Exception(f\"Ошибка: {response.status_code}, {response.text}\")\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"Получает эмбеддинг для одного текста.\"\"\"\n",
    "        return self.embed_documents([text])[0]\n",
    "\n",
    "\n",
    "# Загрузка данных в Milvus\n",
    "def insert_data_into_milvus(collection, chunks, embedder):\n",
    "    \"\"\"\n",
    "    Вставляет чанки текста и их векторные представления в Milvus.\n",
    "\n",
    "    Args:\n",
    "        collection (Collection): Коллекция Milvus.\n",
    "        chunks (list): Список текстовых чанков.\n",
    "        embedder (callable): Функция или модель для генерации эмбеддингов.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Получаем эмбеддинги для списка текстов\n",
    "    embeddings = embedder.embed_documents(chunks)\n",
    "    print(np.array(embeddings).shape)\n",
    "    texts = [chunk for chunk in chunks]\n",
    "    lengths = [len(chunk) for chunk in chunks]\n",
    "\n",
    "    # Преобразование данных в формат, приемлемый для Milvus\n",
    "    data = [\n",
    "        embeddings,  # Векторы\n",
    "        texts,       # Тексты чанков\n",
    "        lengths      # Длины чанков\n",
    "    ]\n",
    "    # Вставка данных в коллекцию\n",
    "    collection.insert(data)\n",
    "    \n",
    "    print(\"Data inserted into Milvus successfully.\")\n",
    "\n",
    "\n",
    "def create_index(collection_name, field_name=\"embedding\", index_params=None):\n",
    "    \"\"\"\n",
    "    Создает индекс на указанном поле в коллекции Milvus.\n",
    "\n",
    "    Args:\n",
    "        collection_name (str): Имя коллекции.\n",
    "        field_name (str): Поле, на котором создается индекс (по умолчанию \"embedding\").\n",
    "        index_params (dict): Параметры индекса (по умолчанию IVF_FLAT).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Проверяем, существует ли коллекция\n",
    "    if not utility.has_collection(collection_name):\n",
    "        print(f\"Collection '{collection_name}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Загружаем коллекцию\n",
    "    collection = Collection(name=collection_name)\n",
    "\n",
    "    # Проверяем, создан ли уже индекс\n",
    "    if collection.has_index():\n",
    "        print(f\"Index already exists for collection '{collection_name}'.\")\n",
    "        return\n",
    "\n",
    "    # Устанавливаем параметры индекса (по умолчанию используем IVF_FLAT)\n",
    "    if index_params is None:\n",
    "        index_params = {\n",
    "            \"index_type\": \"IVF_FLAT\",  # Тип индекса\n",
    "            \"params\": {\"nlist\": 128},  # Количество кластеров\n",
    "            \"metric_type\": \"IP\"        # Метрика (внутреннее произведение для косинусной близости)\n",
    "        }\n",
    "\n",
    "    # Создаем индекс\n",
    "    collection.create_index(field_name=field_name, index_params=index_params)\n",
    "    print(f\"Index created on field '{field_name}' for collection '{collection_name}'.\")\n",
    "\n",
    "    # Выгружаем и заново загружаем коллекцию для применения индекса\n",
    "    collection.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_milvus_collection(collection_name, limit=10):\n",
    "    \"\"\"\n",
    "    Выводит содержимое указанной коллекции Milvus.\n",
    "\n",
    "    Args:\n",
    "        collection_name (str): Имя коллекции.\n",
    "        limit (int): Количество записей для вывода (по умолчанию 10).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Проверяем, существует ли коллекция\n",
    "    if not utility.has_collection(collection_name):\n",
    "        print(f\"Collection '{collection_name}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Загружаем коллекцию\n",
    "    collection = Collection(name=collection_name)\n",
    "\n",
    "    # Проверяем, есть ли данные в коллекции\n",
    "    if collection.num_entities == 0:\n",
    "        print(f\"Collection '{collection_name}' is empty.\")\n",
    "        return\n",
    "\n",
    "    # Выполняем запрос для получения данных\n",
    "    collection.load()  # Загружаем данные в память (если они ещё не загружены)\n",
    "    results = collection.query(expr=\"id >= 0\", output_fields=[\"id\", \"text\", \"chunk_length\"], limit=limit)\n",
    "\n",
    "    # Выводим результаты\n",
    "    print(f\"Displaying {len(results)} records from collection '{collection_name}':\")\n",
    "    for record in results:\n",
    "        print(f\"ID: {record['id']}, Text: {record['text'][:50]}..., Length: {record['chunk_length']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Milvus at localhost:19530...\n",
      "Connected successfully!\n",
      "Collection 'text_chunks' created successfully.\n",
      "(123, 1024)\n",
      "Data inserted into Milvus successfully.\n"
     ]
    }
   ],
   "source": [
    "# Создаем экземпляр эмбеддера\n",
    "embedder = CustomEmbedder(embedder_url=\"http://localhost:8080/embed\")\n",
    "\n",
    "# Подключение к Milvus\n",
    "connect_to_milvus(host=\"localhost\", port=\"19530\")\n",
    "\n",
    "# Создание коллекции (если она еще не существует)\n",
    "collection_name = \"text_chunks\"\n",
    "dim = 1024  # Размерность векторов (замените на реальную размерность вашего эмбеддера)\n",
    "\n",
    "collection = create_milvus_collection(collection_name, dim)\n",
    "insert_data_into_milvus(collection, all_chunks, embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 10 records from collection 'text_chunks':\n",
      "ID: 456077840787179177, Text: [Источник: АО «Минерально-химическая компания Евро..., Length: 770\n",
      "ID: 456077840787179178, Text: [Источник: Наши активы - добыча, производство, про..., Length: 1412\n",
      "ID: 456077840787179179, Text: [Источник: Наши активы - добыча, производство, про..., Length: 1491\n",
      "ID: 456077840787179180, Text: [Источник: Комплаенс]\n",
      "Удобрения и кормовые продукт..., Length: 1482\n",
      "ID: 456077840787179181, Text: [Источник: Комплаенс]\n",
      "Основная роль в противодейст..., Length: 1417\n",
      "ID: 456077840787179182, Text: [Источник: Комплаенс]\n",
      "создана в ЕвроХим как один и..., Length: 1410\n",
      "ID: 456077840787179183, Text: [Источник: Комплаенс]\n",
      "как часть системы комплаенс-..., Length: 948\n",
      "ID: 456077840787179184, Text: [Источник: ПроТех Лаб]\n",
      "Удобрения и кормовые продук..., Length: 1503\n",
      "ID: 456077840787179185, Text: [Источник: ПроТех Лаб]\n",
      "НИЦ ПроТехИнжиниринг (г. Са..., Length: 1460\n",
      "ID: 456077840787179186, Text: [Источник: ПроТех Лаб]\n",
      "4. Внедрение инновационных ..., Length: 1362\n"
     ]
    }
   ],
   "source": [
    "display_milvus_collection(collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar_chunks(collection_name, query_embedding, top_k=15):\n",
    "    \"\"\"\n",
    "    Выполняет поиск top_k самых ближайших чанков к заданному запросу в коллекции Milvus.\n",
    "\n",
    "    Args:\n",
    "        collection_name (str): Имя коллекции.\n",
    "        query_embedding (list): Векторный запрос (эмбеддинг).\n",
    "        top_k (int): Количество ближайших чанков для поиска.\n",
    "\n",
    "    Returns:\n",
    "        list: Список кортежей (текст чанка, расстояние до запроса).\n",
    "    \"\"\"\n",
    "    # Проверяем, существует ли коллекция\n",
    "    if not utility.has_collection(collection_name):\n",
    "        print(f\"Collection '{collection_name}' does not exist.\")\n",
    "        return []\n",
    "\n",
    "    # Загружаем коллекцию\n",
    "    collection = Collection(name=collection_name)\n",
    "\n",
    "    # Если индекс не создан, создаём его\n",
    "    if not collection.has_index():\n",
    "        create_index(collection_name, field_name=\"embedding\")\n",
    "\n",
    "    # Если данные еще не загружены, выполняем загрузку\n",
    "    collection.load()\n",
    "\n",
    "    # Выполняем поиск\n",
    "    search_params = {\n",
    "        \"metric_type\": \"IP\",  # Используем внутреннее произведение для косинусной близости\n",
    "        \"params\": {\"nprobe\": 16}  # Параметр для оптимизации поиска\n",
    "    }\n",
    "    results = collection.search(\n",
    "        data=[query_embedding],  # Список запросов (векторов)\n",
    "        anns_field=\"embedding\",  # Поле для поиска (векторное представление)\n",
    "        param=search_params,\n",
    "        limit=top_k,  # Количество результатов\n",
    "        output_fields=[\"text\", \"chunk_length\"]  # Дополнительные поля для вывода\n",
    "    )\n",
    "\n",
    "\n",
    "    # Обработка результатов\n",
    "    similar_chunks = []\n",
    "    seen_text = set()  # Множество для отслеживания уникальных векторов\n",
    "\n",
    "    for result in results[0]:  # results[0] содержит результаты для первого запроса\n",
    "        entity = result.entity\n",
    "        distance = result.distance\n",
    "        text = entity.get(\"text\")\n",
    "        chunk_length = entity.get(\"chunk_length\")\n",
    "\n",
    "        # Проверяем, не встречался ли этот вектор ранее\n",
    "        if text not in seen_text:\n",
    "            seen_text.add(text)  # Добавляем вектор в множество\n",
    "            similar_chunks.append((text, distance, chunk_length))\n",
    "\n",
    "    return similar_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эмбеддинг для запроса: [0.011806826, -0.0011667218, -0.004861634, -0.018714476, 0.012379335, 0.0031112581, 0.009422936, 0.09062537, 0.052070167, -0.01226671, 0.029151034, 0.036527954, -0.031009343, -0.024871295, -0.007710101, -0.004516721, -0.055861864, -7.933591e-05, -0.01607718, 0.0068231816, 0.038273636, -0.011243702, -0.044111352, -0.028925784, 0.0007989317, -0.0047959364, -0.026260333, -0.018752018, -0.024082921, -0.027067477, -0.01858308, 0.01917436, 0.004033373, -0.03166632, -0.03162878, 0.04208411, 0.03450071, 0.033956356, -0.026729602, 0.034725957, -0.004171808, 0.06843829, 0.0041225343, -0.040357195, -0.03048376, 0.012088387, 0.028268807, -0.03352463, 0.007949429, 0.027630601, 0.01416256, 0.01880833, -0.010061143, -0.052670833, -0.041032944, 0.032285757, -0.0068137962, 0.03258609, -0.03619008, 0.037278786, -0.015082329, -0.021079596, 0.03168509, -0.043285437, -0.0066120103, 0.03707231, 0.024946379, 0.04899176, -0.056725323, -0.014491049, -0.02168026, 0.02950768, -0.0523705, -0.002632603, -0.016105337, -0.009526175, 0.035214, 0.005176045, 0.035814665, -0.033768646, 0.05792665, 0.020666638, 0.020760491, -0.0041154954, 0.021417469, -0.0040779538, 0.032210674, 0.038480114, 0.041558526, 0.0036274549, -0.0009942652, 0.034669645, 0.03070901, -0.05574924, -0.045763183, -0.012463803, 0.037316326, 0.042834938, -0.030183427, -0.03448194, -0.018151352, -0.0037846602, 0.05113163, -0.017409906, -0.027424121, 0.002719418, 0.032511007, 0.039155863, -0.0021562944, 0.026072625, 0.06288214, 0.052257873, 0.023144381, -0.015636066, -0.04696451, 0.023425944, -0.0017832249, -0.020441389, -0.05071867, 0.024570962, 0.026823457, 0.05308379, 0.00028654782, -0.0039090165, 0.0012341794, -0.0130456975, 0.027198872, 0.03352463, -0.02427063, 0.018911568, 0.02883193, -0.0034327079, -0.052032623, -0.032923963, -0.038236097, -0.026598208, -0.009957903, 0.0008945454, -0.005021186, 0.00049801246, -0.033017818, 0.026091395, 0.034669645, -0.03949374, -0.04016949, -0.039681446, -0.010230079, -0.007217368, -0.047490094, -0.024627274, 0.0034960592, -0.040582445, 0.0033810881, 0.028907014, 0.04016949, -0.010408402, 0.017250355, 0.039343573, 0.0074097686, 0.009512097, -0.051807377, -0.07444495, -0.0067293276, -0.02308807, 0.025640897, -0.008653333, 0.015110484, 0.0030877946, -0.00081770244, -0.04167115, -0.035157688, -0.04242198, -0.009742039, -0.037015993, -0.01605841, 0.028306348, 0.032923963, 0.022431092, 0.040432278, -0.036283933, -0.004584765, 0.04504989, 0.043961186, -0.020328764, -0.016621534, 0.01202269, 0.010361475, 0.03384373, 0.01320525, 0.034969978, -0.016349357, -0.008883276, 0.02855037, 0.038930614, -0.03237961, 0.028662995, 0.033412002, 0.046889428, -0.021004513, -0.04640139, -0.035814665, 0.026504353, -0.03885553, 0.023726277, 0.011534649, -0.046063516, -0.03478227, -0.035420477, 0.021924281, -0.044073813, -0.109696485, -0.018123196, 0.021943051, -0.011534649, -0.03478227, -0.051356878, -0.032022964, 0.015504671, -0.00025839164, -0.0048850975, 0.0038222019, 0.047752887, 0.004422867, 0.017794708, 0.057438612, 0.028081099, 0.07669744, 0.030765321, 0.013834071, 0.012304252, 0.02832512, 0.018395372, -0.039343573, -0.012360564, 0.03926849, -0.028907014, -0.034331772, -0.03161001, 0.00624598, 0.025284251, -0.03498875, 0.016349357, -0.031816486, -0.007170441, -0.01811381, -0.029470138, 0.005143196, -0.056349907, 0.0121447, -0.00038010845, 0.034219146, 0.036790743, -0.02121099, -0.0012670282, 0.05758878, 0.029151034, 0.035176456, -0.009305619, 0.020966971, -0.0042985105, -0.012463803, 0.0048850975, -0.023031758, 0.053534288, 0.03288642, 0.02781831, -0.0076303254, -0.041032944, -0.02027245, -0.0226188, -0.061267853, -0.03450071, -0.020779263, -0.011356327, -0.01583316, 0.038161013, -0.027029935, -0.013974852, 0.026466811, 0.0035664497, 0.03191034, -0.03003326, 0.029226117, 0.009366623, -0.010643037, 0.031816486, 0.025340565, -0.03707231, 0.028494056, -0.031572465, 0.023726277, -0.030765321, 0.08326722, 0.01369329, -0.0014125018, 0.0020178598, -0.02571598, 0.014885236, -0.025415648, 0.016593376, -0.030352365, -0.026016312, 0.023388403, 0.010361475, -0.023519797, -0.030727781, -0.040432278, 0.03192911, -0.048240926, 0.023181923, -0.012961229, 0.024514649, 0.019915806, -0.051769834, -0.041746233, -0.032811336, -0.026091395, 0.025584584, -0.014613058, 0.029319972, -0.045763183, -0.0054435288, -0.021717802, 0.020610325, 0.029451367, -0.0073816124, 0.00024739312, -0.026110167, 0.03384373, -0.020629097, 0.038968157, -0.012285481, -0.013195864, 0.02455219, 0.004230466, 0.02883193, -0.034331772, 0.030521302, 0.016189804, 0.015504671, 0.023275778, -0.01835783, -0.023613652, -0.04501235, 0.02832512, -0.06551005, 0.033956356, -0.048391093, 0.029432597, 0.006349219, 0.015889471, 0.016921865, 0.023557339, 0.049367175, 0.004134266, 0.06945191, -0.004974259, -0.047752887, 0.025828606, -0.014631829, 0.039418656, 0.03166632, 0.03885553, -0.022055676, -0.023857672, -0.032980274, 0.031872798, -0.019024193, -0.022468634, -0.029395055, 0.039343573, 0.03971899, -0.026485583, 0.002280651, 0.029094722, -0.048691425, -0.036978453, -0.011440796, 0.006306985, -0.0034561714, -0.012210398, -0.025171626, 0.042234275, -0.014509819, -0.0326424, -0.040319655, 0.15692379, -0.0053684455, 0.02973293, -0.042985104, -0.041933943, -0.014988475, 0.01833906, 0.021717802, 0.038667824, 0.022487404, -0.02047893, -0.0033271222, 0.012567042, -0.032417152, 0.032417152, 0.0577014, 0.03048376, 0.026860999, 0.05541137, -0.017428678, 0.04662664, -0.022825278, -0.0039676754, 0.03806716, -0.03761666, -0.012567042, 0.01261397, 0.035120144, -0.04403627, 0.00045841784, -0.05713828, 0.009075676, -0.032773796, -0.021623949, 0.013787144, 0.03192911, 0.014415965, -0.0359836, 0.041483443, -0.02952645, -0.0030854484, 0.00053144793, -0.004347784, -0.013064468, 0.017062647, 0.030840404, 0.020798033, -0.032041736, -0.04095786, -0.0024542806, 0.004483872, 0.034725957, -0.030296052, -0.04591335, -0.028475286, -0.051319335, -0.026504353, -0.02811864, 0.025134085, -0.00068748015, 0.04737747, 0.010389632, -0.056612696, -0.036471643, 0.020985741, 0.002611486, -0.023501027, -0.04805322, -0.024233088, -0.03908078, -0.035214, -0.008775343, -0.0122573245, 0.021342386, 0.024158005, -0.01628366, -0.030877946, 0.049254548, 0.03600237, -0.004704429, 0.010933984, -0.031028112, -0.016508909, 0.019277599, 0.07369412, -0.01724097, 0.028662995, 0.0447871, -0.00619436, -0.001965067, 0.03161001, 0.030333593, 0.015805004, 0.06385822, 0.028494056, -0.04452431, 0.018507997, 0.011994534, -0.025547042, -0.025359334, -0.008057361, 0.018536154, -0.01273598, -0.00030018596, 0.0057391687, -0.05304625, 0.039906695, 0.036359016, 0.0067903325, -0.00440175, 0.01024885, 0.007874345, -0.007142285, -0.0226188, 0.028719306, 0.03904324, 0.04050736, -0.050530963, 0.012463803, 0.02667329, -0.039155863, 0.021924281, -0.004650463, 0.062131308, -0.029676616, 0.025678439, -0.03545802, 0.014575517, 0.02235601, -0.00059655914, 0.008906739, -0.021699032, -0.04403627, 0.037992075, 0.03493244, -0.0059315693, 0.055824324, -0.0264105, 0.006133355, 0.0595034, -0.009305619, 0.024345713, -0.025171626, 0.031872798, 0.005377831, 0.045650557, -0.015363891, -0.04268477, -0.023726277, -0.018451685, 0.012435648, -0.05574924, 0.033674795, 0.011234317, -0.033412002, 0.027442893, 0.015457744, 0.060817353, 0.02121099, -0.05113163, -0.039681446, -0.016593376, -0.021135908, 0.07283066, 0.027105018, 0.06333264, 0.021755343, -0.0031487998, 0.018752018, 0.014096862, -0.010004831, -0.024683587, -0.0019040619, 0.034444395, 0.01788856, -0.034913667, -0.037935764, 0.014003008, -0.026992394, 0.01644321, 0.0028226573, 0.044336602, 0.031009343, -0.014913391, -0.033412002, -0.005420065, 0.0030408676, -0.018507997, -0.024814982, 0.021830427, -0.036603037, 0.12110913, -0.054848243, 0.031816486, -0.002268919, 0.00649, -0.011384483, 0.019915806, -0.037992075, -0.03832995, -0.0069921184, -0.061305396, -0.0024472415, -0.0226188, -0.021980593, -0.0035594108, 0.0025833298, 0.0044533694, 0.007442618, 0.03836749, -0.0041694613, -0.025809834, 0.019333912, 0.020554014, -0.0026842228, 0.05116917, 0.04906684, 0.012445033, -0.0005839475, -0.021417469, 0.002363946, 0.02736781, 0.03172263, -0.042534605, -0.025547042, -0.021999365, 0.01501663, -0.010098684, -0.023782589, -0.03493244, -0.0039019776, -0.039643906, 0.022975445, -0.077936314, -0.025753522, -0.012369949, 0.04426152, 0.019258829, 0.027273955, -0.015316963, 0.026804686, -0.017635155, 0.056349907, -0.02856914, 0.00020809179, 0.058302067, -0.06119277, -0.025828606, 0.013561894, 0.04456185, 0.013590051, -0.017090803, 0.022055676, 0.0072736805, -0.029695388, -0.020366305, 0.0036274549, -0.01811381, -0.043886103, -0.049855214, 0.028062329, -0.008291996, -0.023144381, -0.019878265, -0.029094722, 0.070202745, 0.0059081055, -0.007405076, -0.046814345, -0.04482464, 0.05214525, -0.024176775, -0.0581519, -0.014415965, 0.058302067, 0.0076913303, -0.009835893, -0.033186752, -0.06614826, -0.004516721, -0.00958718, 0.050155547, 0.021980593, -0.01905235, 0.033055358, 0.02310684, -0.035589416, 0.03162878, 0.030540073, -0.06006652, -0.03144107, 0.015335734, -0.00929154, 0.04268477, -0.016405668, -0.0045824186, -0.028250037, -0.0016447903, -0.0029305893, -0.01929637, 0.01858308, -0.0359836, -0.05049342, -0.021417469, -0.03881799, 0.00803859, 0.00094440527, 0.03192911, 0.06333264, 0.008395235, -0.036359016, -0.040657528, -0.01835783, 0.03493244, -0.02832512, 0.004852249, 0.03095303, -0.011703586, -0.01691248, 0.045312684, -0.03859274, 0.0015849584, -0.007395691, 0.02573475, 0.038968157, 0.005147889, 0.030070802, 0.008568865, 0.024139233, -0.018432913, 0.022919133, 0.0036462257, -0.044111352, -0.009685727, -0.052670833, -0.002321712, -0.070653245, -0.03095303, -0.029920636, -0.028494056, 0.006626088, 0.012989385, 0.06562267, -0.03716616, 0.032698713, 0.047339927, -0.011666045, 0.029582763, -0.024721129, -0.023519797, -0.020629097, 0.016377512, -0.023519797, 0.0038996313, -0.013880998, 0.04617614, -0.042046566, 0.020873116, -0.03425669, -0.017700853, 0.018282747, -0.018451685, 0.03684706, -0.016734159, -0.02883193, 0.014716298, -0.0071328995, -0.003507791, -0.035101373, 0.033918813, -0.03695968, -0.037316326, -0.035214, -0.008681489, -0.022018135, 0.029920636, -0.022318467, 0.0049789515, 0.032698713, 0.016349357, -0.0075833984, -0.023726277, -0.010464715, -0.034725957, 0.03763543, -0.025565814, -0.0409954, 0.023219464, -0.0051854304, 0.05409741, 0.015532827, 0.04095786, -0.04077015, 0.04016949, -0.06626088, -0.0074566957, 0.027536746, -0.043510687, -0.010483486, -0.015420202, -0.061530642, 0.03480104, -0.004779512, -0.025565814, 0.032698713, -0.009554331, 0.00702966, -0.010736891, -0.03527031, 0.019615473, 0.0120602315, -0.026241561, 0.03665935, -0.023669964, 0.0423469, -0.0033153906, -0.025077773, -0.015532827, 0.07403199, -0.016762314, 0.060216688, 0.010483486, -0.027217643, -0.06659876, -0.005781403, -0.05349675, 0.018536154, 0.024326941, -0.017494375, 0.04257215, -0.040056862, -0.02404538, -0.039794073, -0.02263757, -0.034575794, 0.0012717209, -0.024120463, 0.029695388, -0.03172263, 0.050606046, 0.015682993, -0.025828606, 0.020122286, -0.041070484, -0.035326622, -0.013289718, 0.033956356, 0.04887913, 0.05026817, 0.03767297, 0.0058424077, -0.022449862, 0.038930614, 0.044899724, 0.031328447, 0.00803859, -0.021042055, -0.009455784, -0.027668143, -0.009192994, -0.013862227, -0.019803181, -0.010230079, 0.0069029573, 0.013224021, -0.009056905, 0.05192, 0.015908243, 0.046251222, -0.018461071, -0.021079596, 0.042159192, 0.048691425, 0.03737264, -0.030596385, -0.046138596, 0.01238872, -0.02787462, -0.011215546, 0.0048381705, 0.018235821, -0.03237961, -0.035326622, -0.014725683, -0.056762863, -0.0014207141, 0.032529775, -0.025772292, 0.0029728236, -0.03502629, -0.04910438, 0.022018135, 0.0020366306, 0.055336285, -0.027255185, 0.007888424, 0.020854346, 0.029470138, -0.021924281, -0.008568865, 0.04448677, -0.005157274, 0.052032623, -0.015063558, -0.027536746, 0.010952755, -0.0102113085, -0.00060887745, -0.04550039, -0.00312299, 0.020197367, 0.036490414, -0.031178279, -0.00612397, -0.0029845554, -0.020178597, 0.06250673, -0.04456185, -0.019136818, -0.036133766, 0.013017542, -0.048428632, 0.004052144, -0.013317875, 0.058076818, -0.058302067, 0.0010728679, -0.0005613639, -0.02308807, -0.015063558, -0.0025997541, 0.07174195, -0.016583992, 0.041596066, -0.022675112, -0.013590051, -0.038161013, -0.031816486, -0.02213076, 0.01298, -0.02025368, -0.07144162, -0.055899408, -0.029639075, 0.029376283, -0.061080147, -0.027180102, 0.03211682, -0.0050305715, 0.01012684, 0.0036133768, -0.02213076, 0.022881592, -0.024082921, -0.020366305, 0.018489227, 0.0049179466, 0.019671787, -0.008475011, -0.058752567, 0.005373138, 0.018385988, -0.035326622, -0.00998606, 0.022374779, 0.02667329, -0.049404714, 0.04786551, 0.030183427, -0.001736298, 0.028531598, -0.004066222, 0.022825278, 0.034012668, -0.03737264, -0.041370817, -0.0010364995, 0.018123196, 0.027180102, 0.030896718, 0.014190716, 0.01976564, -0.0046809656, 0.039155863, -0.010633651, 0.023501027, -0.021586407, 0.007813341, 0.005171352, -0.018780174, -0.022675112, 0.04257215, -0.027912162, -0.014453507, 0.021642718, -0.034913667, 0.038780447, -0.009526175, -0.023538569, 0.023181923, -0.047715344, 0.014509819, -0.008911432, 0.013383572, 0.037823137, -0.030296052, 0.0032848879, 0.048203383, 0.04381102, 0.02475867, 0.048841592, 0.011731743, 0.026016312, 0.018301519, -0.024383254, 0.029676616, -0.040845234, -0.035514332, -0.01772901, 0.019240057, -0.03161001, 0.03930603, -0.019221287, -0.028418973, 0.033900045, 0.0050681126, 0.009920361, 0.018639393, -0.009605951, -0.041145567, -0.0005390736, -0.01524188, 0.015354505, 0.0024777441, 0.022374779, -0.030840404, 0.009835893, 0.041108027]\n",
      "\n",
      "Найдено 10 похожих чанков:\n",
      "1. Расстояние: 0.8094, Длина: 627, Текст: [Источник: GitHub - Koldim2001/TrafficAnalyzer: Анализ трафика на круговом движении с использованием компьютерного зрения]\n",
      "По сути это та же ветка mul...\n",
      "2. Расстояние: 0.8050, Длина: 1623, Текст: [Источник: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      "Pre-initialized m...\n",
      "3. Расстояние: 0.8018, Длина: 1348, Текст: [Источник: GitHub - Koldim2001/TrafficAnalyzer: Анализ трафика на круговом движении с использованием компьютерного зрения]\n",
      ". В ней реализовано всё то ...\n",
      "4. Расстояние: 0.8009, Длина: 1587, Текст: [Источник: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      ": This advanced m...\n",
      "5. Расстояние: 0.8006, Длина: 1567, Текст: [Источник: GitHub - Koldim2001/TrafficAnalyzer: Анализ трафика на круговом движении с использованием компьютерного зрения]\n",
      "GitHub - Koldim2001/Traffic...\n",
      "6. Расстояние: 0.7997, Длина: 1597, Текст: [Источник: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      "GitHub - Koldim20...\n",
      "7. Расстояние: 0.7987, Длина: 421, Текст: [Источник: ПроТех Лаб]\n",
      "Примеры цитируемости по профессиональной деятельности\n",
      "Референсы по работе ментором/экспертом\n",
      "Отметки о государственных или иных...\n",
      "8. Расстояние: 0.7976, Длина: 1613, Текст: [Источник: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      "polygons: If avai...\n",
      "9. Расстояние: 0.7968, Длина: 226, Текст: [Источник: Ultralytics YOLO11 Modes - Ultralytics YOLO Docs]\n",
      "# Perform object tracking on a video from the command line # You can specify different so...\n",
      "10. Расстояние: 0.7963, Длина: 1471, Текст: [Источник: GitHub - Koldim2001/TrafficAnalyzer: Анализ трафика на круговом движении с использованием компьютерного зрения]\n",
      "Проект представляет собой с...\n"
     ]
    }
   ],
   "source": [
    "# Имя коллекции\n",
    "collection_name = \"text_chunks\"\n",
    "\n",
    "# Создание эмбеддинга для запроса\n",
    "query = \"что такое патчевый инференс?\"\n",
    "query_embedding = embedder.embed_query(query)  # Предполагается, что embedder уже определен\n",
    "print(\"Эмбеддинг для запроса:\", query_embedding)\n",
    "\n",
    "# Поиск похожих чанков\n",
    "similar_chunks = search_similar_chunks(collection_name, query_embedding, top_k=10)\n",
    "\n",
    "# Вывод результатов\n",
    "print(f\"\\nНайдено {len(similar_chunks)} похожих чанков:\")\n",
    "for i, (text, distance, length) in enumerate(similar_chunks, start=1):\n",
    "    print(f\"{i}. Расстояние: {distance:.4f}, Длина: {length}, Текст: {text[:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_milvus_collection(collection_name):\n",
    "    \"\"\"\n",
    "    Очищает все данные из указанной коллекции Milvus, оставляя саму коллекцию intact.\n",
    "\n",
    "    Args:\n",
    "        collection_name (str): Имя коллекции.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Проверяем, существует ли коллекция\n",
    "    if not utility.has_collection(collection_name):\n",
    "        print(f\"Collection '{collection_name}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Загружаем коллекцию\n",
    "    collection = Collection(name=collection_name)\n",
    "\n",
    "    # Проверяем, есть ли данные в коллекции\n",
    "    if collection.num_entities == 0:\n",
    "        print(f\"Collection '{collection_name}' is already empty.\")\n",
    "        return\n",
    "\n",
    "    # Очищаем данные\n",
    "    collection.delete(expr=\"id > 0\")  # Удаляем все записи\n",
    "    print(f\"All data from collection '{collection_name}' has been cleared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data from collection 'text_chunks' has been cleared.\n"
     ]
    }
   ],
   "source": [
    "# Очистка коллекции\n",
    "clear_milvus_collection(collection_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Реализация реранка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import requests\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Определение класса CustomReranker (оставляем без изменений)\n",
    "class CustomReranker:\n",
    "    def __init__(self, reranker_url: str):\n",
    "        self.reranker_url = reranker_url\n",
    "\n",
    "    def rerank(self, query: str, documents: List[Document]) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Пересчитывает релевантность документов на основе запроса.\n",
    "        \"\"\"\n",
    "        # Преобразуем документы в текстовый формат\n",
    "        texts = [doc.page_content for doc in documents]\n",
    "        # Отправляем запрос к реранкеру\n",
    "        response = requests.post(\n",
    "            self.reranker_url,\n",
    "            json={\"query\": query, \"texts\": texts}\n",
    "        )\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"Ошибка: {response.status_code}, {response.text}\")\n",
    "        # Получаем результаты реранкинга\n",
    "        results = response.json()\n",
    "        reranked_docs = []\n",
    "        # Сортируем результаты по убыванию оценки\n",
    "        results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "        # Сопоставляем результаты с документами\n",
    "        for result in results:\n",
    "            index = result[\"index\"]\n",
    "            score = result[\"score\"]\n",
    "            doc = documents[index]\n",
    "            doc.metadata[\"score\"] = score\n",
    "            reranked_docs.append(doc)\n",
    "        return reranked_docs\n",
    "\n",
    "\n",
    "# Функция для преобразования чанков в документы\n",
    "def chunks_to_documents(similar_chunks):\n",
    "    \"\"\"\n",
    "    Преобразует список чанков в список объектов Document.\n",
    "\n",
    "    Args:\n",
    "        similar_chunks (list): Список кортежей (текст чанка, расстояние до запроса).\n",
    "\n",
    "    Returns:\n",
    "        list: Список объектов Document.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    for i, (text, distance, chunk_length) in enumerate(similar_chunks):\n",
    "        metadata = {\n",
    "            \"source\": f\"chunk_{i}\",  # Источник чанка\n",
    "            \"distance\": distance,   # Расстояние до запроса\n",
    "            \"chunk_length\": chunk_length  # Длина чанка\n",
    "        }\n",
    "        documents.append(Document(page_content=text, metadata=metadata))\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Топ-3 самых релевантных чанка:\n",
      "1. Документ: [Источник: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object dete... | Релевантность: 0.2520 | Источник: chunk_5\n",
      "2. Документ: [Источник: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object dete... | Релевантность: 0.1461 | Источник: chunk_3\n",
      "3. Документ: [Источник: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object dete... | Релевантность: 0.1009 | Источник: chunk_1\n"
     ]
    }
   ],
   "source": [
    "# Преобразование чанков в документы\n",
    "documents = chunks_to_documents(similar_chunks)\n",
    "\n",
    "# Указываем URL реранкера\n",
    "RERANKER_URL = \"http://localhost:8081/rerank\"\n",
    "custom_reranker = CustomReranker(reranker_url=RERANKER_URL)\n",
    "\n",
    "# Применяем реранкер\n",
    "reranked_docs = custom_reranker.rerank(query, documents)\n",
    "\n",
    "# Выбираем топ-3 самых релевантных чанка\n",
    "top_3_docs = reranked_docs[:3]\n",
    "\n",
    "# Выводим результаты\n",
    "print(\"\\nТоп-3 самых релевантных чанка:\")\n",
    "for i, doc in enumerate(top_3_docs, start=1):\n",
    "    print(f\"{i}. Документ: {doc.page_content[:100]}... | Релевантность: {doc.metadata['score']:.4f} | Источник: {doc.metadata['source']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Соберем воедино RAG и зададим вопрос"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"что такое патчевый инференс? Приведи пример как его в коде реализовать для сегментации\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"text_chunks\"\n",
    "\n",
    "embedder = CustomEmbedder(embedder_url=\"http://localhost:8080/embed\")\n",
    "query_embedding = embedder.embed_query(question)  # Предполагается, что embedder уже определен\n",
    "\n",
    "# Поиск похожих чанков\n",
    "similar_chunks = search_similar_chunks(collection_name, query_embedding, top_k=15)\n",
    "documents = chunks_to_documents(similar_chunks)\n",
    "\n",
    "custom_reranker = CustomReranker(reranker_url=\"http://localhost:8081/rerank\")\n",
    "reranked_docs = custom_reranker.rerank(question, documents)\n",
    "\n",
    "# Выбираем топ-3 самых релевантных чанка\n",
    "top_docs = reranked_docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'chunk_13', 'distance': 0.838281512260437, 'chunk_length': 1578, 'score': 0.5924898}, page_content=\"[Источник: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\\nExample of custom visualization of usual inference results\\nFor Russian-speaking users, there is a detailed video presentation of the project at the AiConf 2024 conference. The YouTube video is available at this\\nTo carry out patch-based inference of YOLO models using our library, you need to follow a sequential procedure. First, you create an instance of the\\nclass, providing all desired parameters related to YOLO inference and the patch segmentation principle.\\nSubsequently, you pass the obtained object of this class to\\n, which facilitates the consolidation of all predictions from each overlapping crop, followed by intelligent suppression of duplicates.\\nUpon completion, you receive the result, from which you can extract the desired outcome of frame processing.\\nThe output obtained from the process includes several attributes that can be leveraged for further analysis or visualization:\\nimg: This attribute contains the original image on which the inference was performed. It provides context for the detected objects.\\nconfidences: This attribute holds the confidence scores associated with each detected object. These scores indicate the model's confidence level in the accuracy of its predictions.\\nboxes: These bounding boxes are represented as a list of lists, where each list contains four values: [x_min, y_min, x_max, y_max]. These values correspond to the coordinates of the top-left and bottom-right corners of each bounding box.\"),\n",
       " Document(metadata={'source': 'chunk_3', 'distance': 0.8572112321853638, 'chunk_length': 1587, 'score': 0.53028214}, page_content='[Источник: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\\n: This advanced mode employs a neural network to analyze the images. The algorithm performs a standard inference of the network on the entire image and identifies the largest detected objects. Based on the sizes of these objects, the algorithm selects patch parameters to ensure that the largest objects are fully contained within a patch, and overlapping patches ensure comprehensive coverage. In this mode, it is necessary to input the model that will be used for patch-based inference in the subsequent steps.\\nPossible arguments of the\\nThe input image in BGR format.\\nThe type of analysis to perform. Can be \"resolution_based\" for Resolution-Based Analysis or \"network_based\" for Neural Network-Based Analysis.\\nPre-initialized model object for \"network_based\" mode. If not provided, the default YOLO11m model will be used.\\nA list of class indices to consider for object detection in \"network_based\" mode. If None, all classes will be considered.\\nThe confidence threshold for detection in \"network_based\" mode.\\nimport cv2 from ultralytics import YOLO from patched_yolo_infer import auto_calculate_crop_values # Load the image img_path = \"test_image.jpg\" img = cv2 . imread ( img_path ) # Calculate the optimal crop size and overlap for an image shape_x , shape_y , overlap_x , overlap_y = auto_calculate_crop_values ( image = img , mode = \"network_based\" , model = YOLO ( \"yolo11m.pt\" )\\nAn example of working with\\nis presented in Google Colab notebook -'),\n",
       " Document(metadata={'source': 'chunk_0', 'distance': 0.8625388145446777, 'chunk_length': 880, 'score': 0.44435376}, page_content='[Источник: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\\nImplementing Patching at Different Resolutions\\nThere is an opportunity to produce cropping into patches at different resolutions. This way, small objects can be detected when cropping into smaller patches, and large objects can be detected when cropping into larger patches. As a result, the algorithm will be able to detect a wider range of object sizes in the frame. To achieve this, the image needs to be processed multiple times through MakeCropsDetectThem with different patch parameters, and then pass the list of element_crops to the CombineDetections process.\\nAn example of using this approach can be seen in this Google Colab notebook -\\nPython library for YOLO small object detection and instance segmentation\\nPython library 1.3.8 version'),\n",
       " Document(metadata={'source': 'chunk_1', 'distance': 0.8581663370132446, 'chunk_length': 1597, 'score': 0.41365477}, page_content=\"[Источник: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\\nGitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation\\nThis Python library simplifies SAHI-like inference for instance segmentation tasks, enabling the detection of small objects in images. It caters to both object detection and instance segmentation tasks, supporting a wide range of Ultralytics models.\\nThe library also provides a sleek customization of the visualization of the inference results for all models, both in the standard approach (direct network run) and the unique patch-based variant.\\n: The library offers support for multiple ultralytics deep learning\\n, such as YOLOv8, YOLOv8-seg, YOLOv9, YOLOv9-seg, YOLOv10, YOLO11, YOLO11-seg, FastSAM, and RTDETR. Users can select from pre-trained options or utilize custom-trained models to best meet their task requirements.\\nExplanation of how Patch-Based-Inference works:\\nYou can install the library via pip:\\n- Click here to visit the PyPI page of\\nNote: If CUDA support is available, it's recommended to pre-install PyTorch with CUDA support before installing the library. Otherwise, the CPU version will be installed by default.\\nInteractive notebooks are provided to showcase the functionality of the library. These notebooks cover batch-inference procedures for detection, instance segmentation, inference custom visualization, and more. Each notebook is paired with a tutorial on YouTube, making it easy to learn and implement features.\"),\n",
       " Document(metadata={'source': 'chunk_5', 'distance': 0.8505747318267822, 'chunk_length': 1613, 'score': 0.35131106}, page_content='[Источник: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\\npolygons: If available, this attribute provides a list containing NumPy arrays of polygon coordinates that represent segmentation masks corresponding to the detected objects. These polygons can be utilized to accurately outline the boundaries of each object.\\nclasses_ids: This attribute contains the class IDs assigned to each detected object. These IDs correspond to specific object classes defined during the model training phase. \\nclasses_names: These are the human-readable names corresponding to the class IDs. They provide semantic labels for the detected objects, making the results easier to interpret.\\nCode example: import cv2 from patched_yolo_infer import MakeCropsDetectThem , CombineDetections # Load the image img_path = \"test_image.jpg\" img = cv2 . imread ( img_path ) element_crops = MakeCropsDetectThem ( image = img , model_path = \"yolo11m.pt\" , segment = False , shape_x = 640 , shape_y = 640 , overlap_x = 25 , overlap_y = 25 , conf = 0.5 , iou = 0.7 ,\\n) result = CombineDetections ( element_crops , nms_threshold = 0.25 ) # Final Results: img = result . image confidences = result . filtered_confidences boxes = result . filtered_boxes polygons = result . filtered_polygons classes_ids = result . filtered_classes_id classes_names = result . filtered_classes_names\\nExplanation of possible input arguments:\\nClass implementing cropping and passing crops through a neural network for detection/segmentation:\\nThe input image in BGR format.\\nPath to the YOLO model.')]"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Итоговый промпт, подаваемый на вход модели:\n",
      "system: Вы полезный помощник, который отвечает на вопросы на русском языке. Ваши ответы должны быть четкими, информативными и полезными.\n",
      "human: Учитывай информацию из этих отрывков текста если считаешь нужным:\n",
      "1. [Источник: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      "Example of custom visualization of usual inference results\n",
      "For Russian-speaking users, there is a detailed video presentation of the project at the AiConf 2024 conference. The YouTube video is available at this\n",
      "To carry out patch-based inference of YOLO models using our library, you need to follow a sequential procedure. First, you create an instance of the\n",
      "class, providing all desired parameters related to YOLO inference and the patch segmentation principle.\n",
      "Subsequently, you pass the obtained object of this class to\n",
      ", which facilitates the consolidation of all predictions from each overlapping crop, followed by intelligent suppression of duplicates.\n",
      "Upon completion, you receive the result, from which you can extract the desired outcome of frame processing.\n",
      "The output obtained from the process includes several attributes that can be leveraged for further analysis or visualization:\n",
      "img: This attribute contains the original image on which the inference was performed. It provides context for the detected objects.\n",
      "confidences: This attribute holds the confidence scores associated with each detected object. These scores indicate the model's confidence level in the accuracy of its predictions.\n",
      "boxes: These bounding boxes are represented as a list of lists, where each list contains four values: [x_min, y_min, x_max, y_max]. These values correspond to the coordinates of the top-left and bottom-right corners of each bounding box.\n",
      "2. [Источник: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      ": This advanced mode employs a neural network to analyze the images. The algorithm performs a standard inference of the network on the entire image and identifies the largest detected objects. Based on the sizes of these objects, the algorithm selects patch parameters to ensure that the largest objects are fully contained within a patch, and overlapping patches ensure comprehensive coverage. In this mode, it is necessary to input the model that will be used for patch-based inference in the subsequent steps.\n",
      "Possible arguments of the\n",
      "The input image in BGR format.\n",
      "The type of analysis to perform. Can be \"resolution_based\" for Resolution-Based Analysis or \"network_based\" for Neural Network-Based Analysis.\n",
      "Pre-initialized model object for \"network_based\" mode. If not provided, the default YOLO11m model will be used.\n",
      "A list of class indices to consider for object detection in \"network_based\" mode. If None, all classes will be considered.\n",
      "The confidence threshold for detection in \"network_based\" mode.\n",
      "import cv2 from ultralytics import YOLO from patched_yolo_infer import auto_calculate_crop_values # Load the image img_path = \"test_image.jpg\" img = cv2 . imread ( img_path ) # Calculate the optimal crop size and overlap for an image shape_x , shape_y , overlap_x , overlap_y = auto_calculate_crop_values ( image = img , mode = \"network_based\" , model = YOLO ( \"yolo11m.pt\" )\n",
      "An example of working with\n",
      "is presented in Google Colab notebook -\n",
      "3. [Источник: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      "Implementing Patching at Different Resolutions\n",
      "There is an opportunity to produce cropping into patches at different resolutions. This way, small objects can be detected when cropping into smaller patches, and large objects can be detected when cropping into larger patches. As a result, the algorithm will be able to detect a wider range of object sizes in the frame. To achieve this, the image needs to be processed multiple times through MakeCropsDetectThem with different patch parameters, and then pass the list of element_crops to the CombineDetections process.\n",
      "An example of using this approach can be seen in this Google Colab notebook -\n",
      "Python library for YOLO small object detection and instance segmentation\n",
      "Python library 1.3.8 version\n",
      "4. [Источник: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      "GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation\n",
      "This Python library simplifies SAHI-like inference for instance segmentation tasks, enabling the detection of small objects in images. It caters to both object detection and instance segmentation tasks, supporting a wide range of Ultralytics models.\n",
      "The library also provides a sleek customization of the visualization of the inference results for all models, both in the standard approach (direct network run) and the unique patch-based variant.\n",
      ": The library offers support for multiple ultralytics deep learning\n",
      ", such as YOLOv8, YOLOv8-seg, YOLOv9, YOLOv9-seg, YOLOv10, YOLO11, YOLO11-seg, FastSAM, and RTDETR. Users can select from pre-trained options or utilize custom-trained models to best meet their task requirements.\n",
      "Explanation of how Patch-Based-Inference works:\n",
      "You can install the library via pip:\n",
      "- Click here to visit the PyPI page of\n",
      "Note: If CUDA support is available, it's recommended to pre-install PyTorch with CUDA support before installing the library. Otherwise, the CPU version will be installed by default.\n",
      "Interactive notebooks are provided to showcase the functionality of the library. These notebooks cover batch-inference procedures for detection, instance segmentation, inference custom visualization, and more. Each notebook is paired with a tutorial on YouTube, making it easy to learn and implement features.\n",
      "5. [Источник: GitHub - Koldim2001/YOLO-Patch-Based-Inference: Python library for YOLO small object detection and instance segmentation]\n",
      "polygons: If available, this attribute provides a list containing NumPy arrays of polygon coordinates that represent segmentation masks corresponding to the detected objects. These polygons can be utilized to accurately outline the boundaries of each object.\n",
      "classes_ids: This attribute contains the class IDs assigned to each detected object. These IDs correspond to specific object classes defined during the model training phase. \n",
      "classes_names: These are the human-readable names corresponding to the class IDs. They provide semantic labels for the detected objects, making the results easier to interpret.\n",
      "Code example: import cv2 from patched_yolo_infer import MakeCropsDetectThem , CombineDetections # Load the image img_path = \"test_image.jpg\" img = cv2 . imread ( img_path ) element_crops = MakeCropsDetectThem ( image = img , model_path = \"yolo11m.pt\" , segment = False , shape_x = 640 , shape_y = 640 , overlap_x = 25 , overlap_y = 25 , conf = 0.5 , iou = 0.7 ,\n",
      ") result = CombineDetections ( element_crops , nms_threshold = 0.25 ) # Final Results: img = result . image confidences = result . filtered_confidences boxes = result . filtered_boxes polygons = result . filtered_polygons classes_ids = result . filtered_classes_id classes_names = result . filtered_classes_names\n",
      "Explanation of possible input arguments:\n",
      "Class implementing cropping and passing crops through a neural network for detection/segmentation:\n",
      "The input image in BGR format.\n",
      "Path to the YOLO model.\n",
      "\n",
      "Ответь на вопрос: что такое патчевый инференс? Приведи пример как его в коде реализовать для сегментации\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "# Устанавливаем API ключ и базовый URL\n",
    "openai_api_key = \"EMPTY\"  # Ключ не требуется, так как используется локальный сервер\n",
    "openai_api_base = \"http://localhost:8071/v1\"  # Адрес вашего локального API сервера\n",
    "\n",
    "# Создаем экземпляр ChatOpenAI с модифицированными параметрами\n",
    "chat = ChatOpenAI(\n",
    "    openai_api_key=openai_api_key,\n",
    "    openai_api_base=openai_api_base,\n",
    "    model_name=\"Qwen/Qwen2.5-3B-Instruct-GPTQ-Int4\",  # Указываем имя модели\n",
    "    max_tokens=5000,  # Ограничиваем количество токенов в ответе\n",
    "    temperature=0  # Устанавливаем температуру (от 0 до 1, где 0 - детерминированный ответ, 1 - более случайный)\n",
    ")\n",
    "\n",
    "# Системный промпт, который задает контекст для модели\n",
    "system_prompt = SystemMessage(content=\"Вы полезный помощник, который отвечает на вопросы на русском языке. Ваши ответы должны быть четкими, информативными и полезными.\")\n",
    "\n",
    "# Добавляем три документа из top_3_docs как примеры\n",
    "examples = \"\\n\".join([f\"{i}. {doc.page_content}\" for i, doc in enumerate(top_docs, start=1)])\n",
    "\n",
    "# Формируем итоговый промпт\n",
    "human_message_content = (\n",
    "    f\"Учитывай информацию из этих отрывков текста если считаешь нужным:\\n\"\n",
    "    f\"{examples}\\n\\n\"\n",
    "    f\"Ответь на вопрос: {question}\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    system_prompt,  # Системный промпт\n",
    "    HumanMessage(content=human_message_content)  # Промпт пользователя с примерами\n",
    "]\n",
    "\n",
    "# Выводим итоговый промпт\n",
    "print(\"Итоговый промпт, подаваемый на вход модели:\")\n",
    "for message in messages:\n",
    "    print(f\"{message.type}: {message.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ответ модели:\n",
      "Патчевый инференс (patch-based inference) - это метод, при котором изображение разделяют на мелкие патчи (или куски), затем применяют модель на каждом патче отдельно, и результаты объединяют. Этот подход позволяет эффективно обрабатывать объекты разной размерности, так как мелкие объекты могут быть лучше обнаружены при использовании мелких патчей, а крупные объекты - при использовании крупных патчей.\n",
      "\n",
      "Вот пример реализации патчевого инференса для сегментации с использованием Python-библиотеки, описанной в предоставленных отрывках текста:\n",
      "\n",
      "```python\n",
      "import cv2\n",
      "from patched_yolo_infer import MakeCropsDetectThem, CombineDetections\n",
      "from ultralytics import YOLO\n",
      "\n",
      "# Загрузка изображения\n",
      "img_path = \"test_image.jpg\"\n",
      "img = cv2.imread(img_path)\n",
      "\n",
      "# Создание объекта для создания патчей и их обработки\n",
      "element_crops = MakeCropsDetectThem(\n",
      "    image=img,\n",
      "    model_path=\"yolo11m.pt\",\n",
      "    segment=True,  # Для сегментации\n",
      "    shape_x=640,  # Ширина патча\n",
      "    shape_y=640,  # Высота патча\n",
      "    overlap_x=25,  # Ширина перекрытия патчей\n",
      "    overlap_y=25,  # Высота перекрытия патчей\n",
      "    conf=0.5,  # Порог уверенности\n",
      "    iou=0.7  # Порог перекрытия\n",
      ")\n",
      "\n",
      "# Объединение результатов\n",
      "result = CombineDetections(element_crops, nms_threshold=0.25)\n",
      "\n",
      "# Получение результата\n",
      "img = result.image\n",
      "confidences = result.filtered_confidences\n",
      "boxes = result.filtered_boxes\n",
      "polygons = result.filtered_polygons\n",
      "classes_ids = result.filtered_classes_id\n",
      "classes_names = result.filtered_classes_names\n",
      "```\n",
      "\n",
      "В этом примере:\n",
      "- `MakeCropsDetectThem` создает патчи изображения и применяет модель на каждом патче.\n",
      "- `CombineDetections` объединяет результаты всех патчей.\n",
      "- `segment=True` указывает, что мы работаем с сегментацией.\n",
      "- `shape_x` и `shape_y` задают размер патчей.\n",
      "- `overlap_x` и `overlap_y` задают перекрытие патчей.\n",
      "- `conf` и `iou` задают порог уверенности и порог перекрытия, соответственно.\n",
      "\n",
      "Этот подход позволяет эффективно обрабатывать объекты разной размерности, обеспечивая точное обнаружение и сегментацию малых и крупных объектов.\n"
     ]
    }
   ],
   "source": [
    "# Получаем ответ от модели\n",
    "response = chat(messages)\n",
    "\n",
    "# Выводим ответ\n",
    "print(\"\\nОтвет модели:\")\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MAIN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
