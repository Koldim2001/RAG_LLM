version: '2.3'

x-environment: &environment
  DEBUG: 1

services:
  emb:
    container_name: embedder
    image: ghcr.io/huggingface/text-embeddings-inference:1.5.1
    restart: always
    ports:
      - 8080:80
    volumes:
      - ./models/nlp/embedders:/data
    pull_policy: never
    environment:
      <<: *environment
    command: --model-id intfloat/multilingual-e5-large-instruct --max-client-batch-size 512 --max-concurrent-requests 512
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            device_ids: [ '0' ]
            capabilities: [gpu]
    shm_size: 64g
    ulimits:
      memlock: -1
      stack: 67108864
      
  reranker:
    container_name: reranker
    image: ghcr.io/huggingface/text-embeddings-inference:1.5.1
    restart: always
    ports:
      - 8081:80
    volumes:
      - ./models/nlp/rerankers:/data
    pull_policy: never
    environment:
      <<: *environment
    command: --model-id BAAI/bge-reranker-v2-m3 --max-client-batch-size 128 
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            device_ids: [ '0' ]
            capabilities: [gpu]
    shm_size: 64g
    ulimits:
      memlock: -1
      stack: 67108864
      
  vllm:
    container_name: vllm
    image: vllm/vllm-openai:latest
    restart: always
    ports:
      - 8071:8000
    volumes:
      - ./models/nlp/llm:/models
    environment:
      <<: *environment
    command: --model /models/Qwen2.5-3B-Instruct-GPTQ-Int4 --port 8000 --served-model-name Qwen/Qwen2.5-3B-Instruct-GPTQ-Int4 --enable-auto-tool-choice --tool-call-parser hermes --gpu-memory-utilization 0.60
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            device_ids: [ '0' ]
            capabilities: [gpu]
    shm_size: 64g
    ulimits:
      memlock: -1
      stack: 67108864

  vllm-visual:
    container_name: vllm-visual
    image: vllm/vllm-openai:latest
    restart: always
    ports:
      - 8089:8000
    volumes:
      - ./models/nlp/vqa:/models
    environment:
      <<: *environment
    command: --model /models/Qwen2-VL-7B-Instruct-GPTQ-Int4 --port 8000 --gpu-memory-utilization 0.95 --served-model-name Qwen/Qwen2-VL-7B-Instruct-GPTQ-Int4 --limit-mm-per-prompt='{"image":1}'
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            device_ids: [ '0' ]
            capabilities: [gpu]
